{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "018a74ce",
   "metadata": {},
   "source": [
    "**Table of contents**<a id='toc0_'></a>    \n",
    "- [Importation des bibliothèques](#toc1_)    \n",
    "  - [Importation des paquets ou modules de la bibliothèque OpenFL](#toc1_1_)    \n",
    "  - [Importation des paquets ou modules de la bibliothèque PyTorch](#toc1_2_)    \n",
    "  - [Importation d’autres paquets ou modules requis](#toc1_3_)    \n",
    "- [Définition du modèle d‘entraînement](#toc2_)    \n",
    "  - [Définition des chargeurs de données](#toc2_1_)    \n",
    "  - [Définition du modèle de réseau CNN](#toc2_2_)    \n",
    "  - [Définition de la fonction d'inférence utilisée dans le test](#toc2_3_)    \n",
    "- [Définition des règles de l'apprentissage fédéré](#toc3_)    \n",
    "  - [Méthode de calcul de la moyenne des poids d'apprentissage fédéré](#toc3_1_)    \n",
    "\n",
    "<!-- vscode-jupyter-toc-config\n",
    "\tnumbering=false\n",
    "\tanchor=true\n",
    "\tflat=false\n",
    "\tminLevel=1\n",
    "\tmaxLevel=6\n",
    "\t/vscode-jupyter-toc-config -->\n",
    "<!-- THIS CELL WILL BE REPLACED ON TOC UPDATE. DO NOT WRITE YOUR TEXT IN THIS CELL -->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18f3172d",
   "metadata": {},
   "source": [
    "# <a id='toc1_'></a>[Importation des bibliothèques](#toc0_)\n",
    "\n",
    "## <a id='toc1_1_'></a>[Importation des paquets ou modules de la bibliothèque OpenFL](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4f63aa4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# La classe `FLSpec` définit la spécification du flux.\n",
    "\n",
    "# Les flux définis par l'utilisateur sont des sous-classes de cette classe.\n",
    "from openfl.experimental.workflow.interface import Aggregator, Collaborator, FLSpec\n",
    "# La fonction `aggregator/collaborator` est un décorateur de placement qui définit l'endroit où la \n",
    "# tâche sera assignée.\n",
    "from openfl.experimental.workflow.placement import aggregator, collaborator\n",
    "from openfl.experimental.workflow.runtime import LocalRuntime"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ef6e368",
   "metadata": {},
   "source": [
    "## <a id='toc1_2_'></a>[Importation des paquets ou modules de la bibliothèque PyTorch](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2b89c419",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from torchsummary import summary\n",
    "\n",
    "from torchvision import datasets, transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94a15a0d",
   "metadata": {},
   "source": [
    "## <a id='toc1_3_'></a>[Importation d’autres paquets ou modules requis](#toc0_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3d76255d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from termcolor import cprint"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a940bb46",
   "metadata": {},
   "source": [
    "# <a id='toc2_'></a>[Définition du modèle d‘entraînement](#toc0_)\n",
    "\n",
    "## <a id='toc2_1_'></a>[Définition des chargeurs de données](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a662af95",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 28, 28, 60000])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_path = \"/tmp/files/\"\n",
    "\n",
    "tensor_mnist = datasets.MNIST(\n",
    "    data_path, train=True, download=True, transform=transforms.ToTensor()\n",
    ")\n",
    "\n",
    "tensor_images = torch.stack([tensor_image for tensor_image, _ in tensor_mnist], dim=3)\n",
    "\n",
    "tensor_images.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0ac187a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.1307])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor_mean = tensor_images.view(1, -1).mean(dim=1)\n",
    "tensor_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "889a543d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.3081])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor_std = tensor_images.view(1, -1).std(dim=1)\n",
    "tensor_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "67ffab47",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "01. torchvision.transforms.Compose(transforms)\n",
    "    - Composes several transforms together.\n",
    "\n",
    "02. torchvision.transforms.Normalize(mean, std, inplace=False)\n",
    "    - Normalize a tensor image with mean and standard deviation.\n",
    "    - output[channel] = (input[channel] - mean[channel]) / std[channel]\n",
    "\"\"\"\n",
    "\n",
    "mnist_train = datasets.MNIST(\n",
    "    \"/tmp/files/\",\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=transforms.Compose(\n",
    "        [\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(tensor_mean, tensor_std),\n",
    "        ]\n",
    "    ),\n",
    ")\n",
    "\n",
    "mnist_test = datasets.MNIST(\n",
    "    \"/tmp/files/\",\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=transforms.Compose(\n",
    "        [\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(tensor_mean, tensor_std),\n",
    "        ]\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e947be44",
   "metadata": {},
   "source": [
    "## <a id='toc2_2_'></a>[Définition du modèle de réseau CNN](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32mMPS is available\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "if torch.backends.mps.is_available():\n",
    "    cprint(\"MPS is available\", \"green\")\n",
    "    device = torch.device(\"mps:0\")\n",
    "elif torch.backends.cuda.is_available():\n",
    "    cprint(\"CUDA is available\", \"green\")\n",
    "    device = torch.device(\"cuda:0\")\n",
    "elif torch.backends.cudnn.is_built():\n",
    "    cprint(\"CUDNN is available\", \"green\")\n",
    "    device = torch.device(\"cuda:0\")\n",
    "else:\n",
    "    cprint(\"CUDA and MPS are not available\", \"red\")\n",
    "    cprint(\"Using CPU\", \"red\")\n",
    "    device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cba3007c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "03. torch.nn.Conv2d(in_channels, out_channels, kernel_size, stride=1, padding=0, dilation=1,\n",
    "    groups=1, bias=True, padding_mode='zeros', device=None, dtype=None)\n",
    "    - Applies a 2D convolution over an input signal composed of several input planes.\n",
    "\n",
    "04. torch.nn.Dropout2d(p=0.5, inplace=False)\n",
    "    - Randomly zero out entire channels.\n",
    "    - Each channel will be zeroed out independently on every forward call with probability p using\n",
    "    samples from a Bernoulli distribution.\n",
    "\n",
    "05. torch.nn.functional.max_pool2d(input, kernel_size, stride=None, padding=0,\n",
    "    dilation=1, ceil_mode=False, return_indices=False)\n",
    "    - Applies a 2D max pooling over an input signal composed of several input planes.\n",
    "\n",
    "06. torch.nn.functional.dropout(input, p=0.5, training=True, inplace=False)\n",
    "    - During training, randomly zeroes some elements of the input tensor with probability p.\n",
    "    - Uses samples from a Bernoulli distribution.\n",
    "\n",
    "07. torch.nn.functional.log_softmax(input, dim=None, _stacklevel=3, dtype=None)\n",
    "    - Apply a softmax followed by a logarithm.\n",
    "    - While mathematically equivalent to log(softmax(x)), doing these two operations separately is\n",
    "    slower and numerically unstable. This function uses an alternative formulation to compute the\n",
    "    output and gradient correctly.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        # La première couche convolutionnelle : le nombre de canaux d’entrée est de 1, c’est-à-dire\n",
    "        # une image en niveaux de gris, le nombre de canaux de sortie est de 10, la taille du filtre\n",
    "        # convolutif est de 5x5, le stride est de 1 et le padding est de 0.\n",
    "\n",
    "        # Par conséquent, après que l'image d'entrée (1x28x28) a été convoluée, la taille de la\n",
    "        # carte de caractéristiques de sortie est de 10x24x24.\n",
    "        self.conv1 = nn.Conv2d(1, 10, kernel_size=5)\n",
    "        # La deuxième couche convolutionnelle : le nombre de canaux d’entrée est de 10, le nombre de\n",
    "        # canaux de sortie est de 20, la taille du filtre convolutif est de 5x5, le stride est de 1\n",
    "        # et le padding est de 0.\n",
    "\n",
    "        # Par conséquent, après que l'image d'entrée (10x12x12) a été convoluée, la taille de la\n",
    "        # carte de caractéristiques de sortie est de 20x8x8.\n",
    "        self.conv2 = nn.Conv2d(10, 20, kernel_size=5)\n",
    "        # Une couche d'abandon.\n",
    "        self.conv2_drop = nn.Dropout2d()\n",
    "        # La première couche de fully connected : le nombre de canaux d’entrée est de 20, chaque\n",
    "        # canal a une taille de 4x4, soit un total de 20x4x4 = 320 nœuds, tandis que la sortie est\n",
    "        # fixée à 50 nœuds.\n",
    "        self.fc1 = nn.Linear(320, 50)\n",
    "        # La deuxième couche de fully connected : l'entrée a 50 nœuds et la sortie a 10 nœuds\n",
    "        # (correspondant à 10 catégories).\n",
    "        self.fc2 = nn.Linear(50, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # La première couche convolutive est suivie d'une couche de pooling de type max pooling avec\n",
    "        # un filtre convolutif de taille de 2x2 et un stride égal à la longueur du filtre.\n",
    "\n",
    "        # La taille d'entrée est de 10x24x24, et après pooling, la taille de sortie est de 10x12x12.\n",
    "        x = F.relu(F.max_pool2d(self.conv1(x), 2))\n",
    "        # La deuxième couche convolutive est suivie d'une couche d'abandon.\n",
    "\n",
    "        # Après la couche d'abandon, suit une autre couche de max-pooling, qui possède un filtre\n",
    "        # convolutif de taille de 2x2 et un stride aussi égal à la longueur du filtre.\n",
    "\n",
    "        # La taille d'entrée est de 20x8x8, et après pooling, la taille de sortie est de 20x4x4.\n",
    "        x = F.relu(F.max_pool2d(self.conv2_drop(self.conv2(x)), 2))\n",
    "        # La carte de caractéristiques multidimensionnelle est transformée en un vecteur\n",
    "        # unidimensionnel, d'une taille de 20x4x4 = 320.\n",
    "        x = x.view(-1, 320)\n",
    "        # La première couche de fully connected, activée par la fonction d'activation ReLU.\n",
    "        x = F.relu(self.fc1(x))\n",
    "        # Pendant l'entraînement du modèle, certains nœuds de la sortie de la première couche de\n",
    "        # fully connected sont mis à zéro de manière aléatoire avec une probabilité p.\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        # La deuxième couche de fully connected sert également de couche de sortie.\n",
    "        x = self.fc2(x)\n",
    "        # Les probabilités logarithmiques de tous les nœuds de la couche de sortie sont calculées en\n",
    "        # appliquant une fonction softmax suivie d'un logarithme.\n",
    "        return F.log_softmax(x, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "01738796",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1           [-1, 10, 24, 24]             260\n",
      "            Conv2d-2             [-1, 20, 8, 8]           5,020\n",
      "         Dropout2d-3             [-1, 20, 8, 8]               0\n",
      "            Linear-4                   [-1, 50]          16,050\n",
      "            Linear-5                   [-1, 10]             510\n",
      "================================================================\n",
      "Total params: 21,840\n",
      "Trainable params: 21,840\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.00\n",
      "Forward/backward pass size (MB): 0.06\n",
      "Params size (MB): 0.08\n",
      "Estimated Total Size (MB): 0.15\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "model = Net()\n",
    "summary(model, next(iter(mnist_train))[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7933b052",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[35m  conv1.weight .............................................. 250  \u001b[0m\n",
      "\u001b[35m  conv1.bias ................................................. 10  \u001b[0m\n",
      "\u001b[35m  conv2.weight ............................................. 5000  \u001b[0m\n",
      "\u001b[35m  conv2.bias ................................................. 20  \u001b[0m\n",
      "\u001b[35m  fc1.weight .............................................. 16000  \u001b[0m\n",
      "\u001b[35m  fc1.bias ................................................... 50  \u001b[0m\n",
      "\u001b[35m  fc2.weight ................................................ 500  \u001b[0m\n",
      "\u001b[35m  fc2.bias ................................................... 10  \u001b[0m\n",
      "\u001b[35m _________________________________________________________________ \u001b[0m\n",
      "\u001b[35m  total parameters ........................................ 21840  \u001b[0m\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def count_parameters(model):\n",
    "    length = 67\n",
    "    names = [n for (n, p) in model.named_parameters() if p.requires_grad]\n",
    "    name = \"total parameters\"\n",
    "    names.append(name)\n",
    "    max_length = max(map(len, names))\n",
    "    formatted_names = [f\"{f'  {n} ':.<{max_length + 3}}\" for n in names]\n",
    "    params = [p.numel() for p in model.parameters() if p.requires_grad]\n",
    "    params.append(sum(params))\n",
    "    formatted_params = [f\"{f' {p}  ':.>{length - max_length - 3}}\" for p in params]\n",
    "\n",
    "    for n, p in zip(formatted_names[:-1], formatted_params[:-1]):\n",
    "        cprint((n + p), \"magenta\")\n",
    "    cprint(\" \" + \"_\" * (length - 2) + \" \", \"magenta\")\n",
    "    cprint(\n",
    "        (formatted_names[-1] + formatted_params[-1]),\n",
    "        \"magenta\",\n",
    "        end=\"\\n\\n\",\n",
    "    )\n",
    "\n",
    "    return names, params\n",
    "\n",
    "\n",
    "names, params = count_parameters(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19e4583c",
   "metadata": {},
   "source": [
    "## <a id='toc2_3_'></a>[Définition de la fonction d'inférence utilisée dans le test](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "13b360d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "08. torch.nn.functional.nll_loss(input, target, weight=None, size_average=None, ignore_index=-100,\n",
    "    reduce=None, reduction='mean')\n",
    "    - Compute the negative log likelihood loss.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def inference(network, test_loader):\n",
    "    # Mettre le module en mode évaluation.\n",
    "    network.eval()\n",
    "    network.to(device)\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = network(data)\n",
    "            # L'entropie est une mesure de l'incertitude, c'est-à-dire que, si un résultat est \n",
    "            # certain, l'entropie est faible.\n",
    "\n",
    "            # La perte d'entropie croisée, ou perte logarithmique, mesure les performances d'un \n",
    "            # modèle de classification dont le résultat est une valeur de probabilité comprise entre \n",
    "            # 0 et 1.\n",
    "\n",
    "            # La perte d'entropie croisée augmente à mesure que la probabilité prédite s'écarte de \n",
    "            # l'étiquette réelle.\n",
    "\n",
    "            # L’entropie croisée catégorielle sert au classement en plusieurs classes.\n",
    "\n",
    "            test_loss += F.cross_entropy(output, target, reduction=\"sum\").item()\n",
    "            # Si `keepdim` est `True`, alors le tenseur de sortie est de la même taille que celui \n",
    "            # d'entrée, sauf dans la (les) dimension(s) `dim` où il est de dimension 1.\n",
    "            pred = output.data.max(dim=1, keepdim=True)[1]\n",
    "            # Calcul de l'égalité par éléments.\n",
    "            correct += pred.eq(target.data.view_as(pred)).sum()\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    cprint(\n",
    "        \"Test set: Avg. loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\".format(\n",
    "            test_loss,\n",
    "            correct,\n",
    "            len(test_loader.dataset),\n",
    "            100.0 * correct / len(test_loader.dataset),\n",
    "        ),\n",
    "        \"magenta\",\n",
    "        attrs=[\"underline\"],\n",
    "        end=\"\\n\\n\",\n",
    "    )\n",
    "    return float(correct / len(test_loader.dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f8c156a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[4m\u001b[35mTest set: Avg. loss: 2.3080, Accuracy: 701/10000 (7%)\u001b[0m\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.07010000199079514"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_loader = DataLoader(mnist_test, batch_size=100, shuffle=False)\n",
    "\n",
    "inference(model, test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad727510",
   "metadata": {},
   "source": [
    "# <a id='toc3_'></a>[Définition des règles de l'apprentissage fédéré](#toc0_)\n",
    "\n",
    "## <a id='toc3_1_'></a>[Méthode de calcul de la moyenne des poids d'apprentissage fédéré](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "59d93477",
   "metadata": {},
   "outputs": [],
   "source": [
    "def FedAvg(models, weights=None):\n",
    "    new_model = models[0]\n",
    "    state_dicts = [model.state_dict() for model in models]\n",
    "    state_dict = new_model.state_dict()\n",
    "    for key in models[1].state_dict():\n",
    "        if state_dict[key].dim() != 0:\n",
    "            state_dict[key] = torch.from_numpy(\n",
    "                np.average(\n",
    "                    [state[key].cpu().numpy() for state in state_dicts],\n",
    "                    axis=0,\n",
    "                    weights=weights,\n",
    "                )\n",
    "            )\n",
    "        else:\n",
    "            state_dict[key] = torch.from_numpy(\n",
    "                np.average(\n",
    "                    [state[key].reshape(1).cpu().numpy() for state in state_dicts],\n",
    "                    axis=0,\n",
    "                    weights=weights,\n",
    "                )\n",
    "            )\n",
    "    new_model.load_state_dict(state_dict)\n",
    "    return new_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a3d20ab8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aggregator step \"start\" registered\n",
      "Collaborator step \"aggregated_model_validation\" registered\n",
      "Collaborator step \"train\" registered\n",
      "Collaborator step \"local_model_validation\" registered\n",
      "Aggregator step \"join\" registered\n",
      "Aggregator step \"end\" registered\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 0.01\n",
    "log_interval = 10\n",
    "momentum = 0.5\n",
    "\n",
    "class FederatedFlow(FLSpec):\n",
    "\n",
    "    def __init__(self, model=None, optimizer=None, rounds=3, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        # Importe un modèle personnalisé et ajoute le bon algorithme d’optimisation pour ce dernier.\n",
    "        if model is not None:\n",
    "            self.model = model\n",
    "            self.optimizer = optimizer\n",
    "        # Chargez le modèle `Net()` et configurez l'optimiseur pour qu'il s'applique uniquement à ce \n",
    "        # modèle.\n",
    "        else:\n",
    "            self.model = Net()\n",
    "            self.optimizer = optim.SGD(\n",
    "                self.model.parameters(), lr=learning_rate, momentum=momentum\n",
    "            )\n",
    "        self.rounds = rounds\n",
    "\n",
    "    # Un agrégateur est le nœud central de l'apprentissage fédéré.\n",
    "\n",
    "    # L'agrégateur commence par un modèle et un optimiseur transmis de manière facultative.\n",
    "\n",
    "    # L'agrégateur commence le flux avec la tâche de `start`, où la liste des collaborateurs est \n",
    "    # extraite de l'exécution (`self.collaborators = self.runtime.collaborators`) et est ensuite \n",
    "    # utilisée comme liste de participants pour exécuter la tâche énumérée dans `self.next`, \n",
    "    # `aggregated_model_validation`.\n",
    "    @aggregator\n",
    "    def start(self):\n",
    "        cprint(\"Performing initialization for model\", \"black\", attrs=[\"bold\"])\n",
    "        self.collaborators = self.runtime.collaborators\n",
    "        self.private = 10\n",
    "        self.current_round = 0\n",
    "        self.next(\n",
    "            self.aggregated_model_validation,\n",
    "            foreach=\"collaborators\",\n",
    "            exclude=[\"private\"],\n",
    "        )\n",
    "\n",
    "    # Le modèle, l'optimiseur et tout ce qui n'est pas explicitement exclu de la fonction suivante \n",
    "    # seront transmis de la fonction de `start` de l'agrégateur à la tâche \n",
    "    # `aggregated_model_validation` du collaborateur.\n",
    "\n",
    "    # L’endroit où les tâches sont exécutées est déterminé par le décorateur de placement qui \n",
    "    # précède chaque définition de tâche (`@aggregator` ou `@collaborator`).\n",
    "\n",
    "    # Une fois que chaque collaborateur (défini dans l’exécution) a terminé la tâche \n",
    "    # `aggregated_model_validation`, il transmet son état actuel à la tâche `train`, de `train` à \n",
    "    # `local_model_validation`, et enfin à `join` à l'agrégateur.\n",
    "\n",
    "    # C'est au niveau de `join` qu'une moyenne des poids des modèles est calculée et que le tour \n",
    "    # suivant peut commencer.\n",
    "    @collaborator\n",
    "    def aggregated_model_validation(self):\n",
    "        cprint(\n",
    "            f\"Performing aggregated model validation for collaborator {self.input}\",\n",
    "            \"red\",\n",
    "            attrs=[\"bold\"],\n",
    "        )\n",
    "        self.agg_validation_score = inference(self.model, self.test_loader)\n",
    "        cprint(\n",
    "            f\"{self.input} value of {self.agg_validation_score}\",\n",
    "            \"red\",\n",
    "            attrs=[\"underline\"],\n",
    "        )\n",
    "        self.next(self.train)\n",
    "\n",
    "    @collaborator\n",
    "    def train(self):\n",
    "        self.model.train()\n",
    "        self.optimizer = optim.SGD(\n",
    "            self.model.parameters(), lr=learning_rate, momentum=momentum\n",
    "        )\n",
    "        for batch_idx, (data, target) in enumerate(self.train_loader):\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            self.optimizer.zero_grad()\n",
    "            output = self.model(data)\n",
    "            loss = F.cross_entropy(output, target)\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "            if batch_idx % log_interval == 0:\n",
    "                cprint(\n",
    "                    \"Train Epoch: 1 [{}/{} ({:.0f}%)]\\tLoss: {:.6f}\".format(\n",
    "                        batch_idx * len(data),\n",
    "                        len(self.train_loader.dataset),\n",
    "                        100.0 * batch_idx / len(self.train_loader),\n",
    "                        loss.item(),\n",
    "                    ),\n",
    "                    \"yellow\",\n",
    "                )\n",
    "                self.loss = loss.item()\n",
    "                torch.save(self.model.state_dict(), \"model.pth\")\n",
    "                torch.save(self.optimizer.state_dict(), \"optimizer.pth\")\n",
    "        self.training_completed = True\n",
    "        self.next(self.local_model_validation)\n",
    "\n",
    "    @collaborator\n",
    "    def local_model_validation(self):\n",
    "        self.local_validation_score = inference(self.model, self.test_loader)\n",
    "        cprint(\n",
    "            f\"Doing local model validation for collaborator {self.input}: \\\n",
    "                {self.local_validation_score}\",\n",
    "            \"white\",\n",
    "        )\n",
    "        self.next(self.join, exclude=[\"training_completed\"])\n",
    "\n",
    "    @aggregator\n",
    "    def join(self, inputs):\n",
    "        self.average_loss = sum(input.loss for input in inputs) / len(inputs)\n",
    "        self.aggregated_model_accuracy = sum(\n",
    "            input.agg_validation_score for input in inputs\n",
    "        ) / len(inputs)\n",
    "        self.local_model_accuracy = sum(\n",
    "            input.local_validation_score for input in inputs\n",
    "        ) / len(inputs)\n",
    "        cprint(\n",
    "            f\"Average aggregated model validation values = \\\n",
    "                {self.aggregated_model_accuracy}\",\n",
    "            \"green\",\n",
    "        )\n",
    "        cprint(f\"Average training loss = {self.average_loss}\", \"green\")\n",
    "        cprint(\n",
    "            f\"Average local model validation values = \\\n",
    "            {self.local_model_accuracy}\",\n",
    "            \"green\",\n",
    "        )\n",
    "        self.model = FedAvg([input.model for input in inputs])\n",
    "        self.optimizer = [input.optimizer for input in inputs][0]\n",
    "        self.current_round += 1\n",
    "        if self.current_round < self.rounds:\n",
    "            self.next(\n",
    "                self.aggregated_model_validation,\n",
    "                foreach=\"collaborators\",\n",
    "                exclude=[\"private\"],\n",
    "            )\n",
    "        else:\n",
    "            self.next(self.end)\n",
    "\n",
    "    @aggregator\n",
    "    def end(self):\n",
    "        cprint(\"This is the end of the flow\", \"black\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2eabf4c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x125b9f990>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_seed = 1\n",
    "torch.manual_seed(random_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fc74f516",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Local runtime collaborators = ['Portland', 'Seattle', 'Chandler', 'Bangalore']\n"
     ]
    }
   ],
   "source": [
    "batch_size_train = 64\n",
    "\n",
    "# Configurer les participants.\n",
    "aggregator = Aggregator()\n",
    "aggregator.private_attributes = {}\n",
    "\n",
    "# Configurer des collaborateurs avec des attributs privés.\n",
    "collaborator_names = [\"Portland\", \"Seattle\", \"Chandler\", \"Bangalore\"]\n",
    "collaborators = [Collaborator(name=name) for name in collaborator_names]\n",
    "for idx, collaborator in enumerate(collaborators):\n",
    "    local_train = deepcopy(mnist_train)\n",
    "    local_test = deepcopy(mnist_test)\n",
    "    local_train.data = mnist_train.data[idx :: len(collaborators)]\n",
    "    local_train.targets = mnist_train.targets[idx :: len(collaborators)]\n",
    "    local_test.data = mnist_test.data[idx :: len(collaborators)]\n",
    "    local_test.targets = mnist_test.targets[idx :: len(collaborators)]\n",
    "    collaborator.private_attributes = {\n",
    "        \"train_loader\": DataLoader(\n",
    "            local_train, batch_size=batch_size_train, shuffle=True\n",
    "        ),\n",
    "        \"test_loader\": DataLoader(\n",
    "            local_test, batch_size=batch_size_train, shuffle=True\n",
    "        ),\n",
    "    }\n",
    "\n",
    "# La classe `LocalRuntime` est un environnement d'exécution qui exécute le flux sur la machine\n",
    "# locale.\n",
    "\n",
    "# Il est utilisé pour le développement et le débogage.\n",
    "\n",
    "# Il est également utilisé pour exécuter le flux sur un seul nœud.\n",
    "local_runtime = LocalRuntime(\n",
    "    aggregator=aggregator, collaborators=collaborators, backend=\"single_process\"\n",
    ")\n",
    "print(f\"Local runtime collaborators = {local_runtime.collaborators}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5494deda",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "if os.environ.get(\"USERNAME\") is None:\n",
    "    os.environ[\"USERNAME\"] = \"Hao\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8018e1e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "haozhang\n"
     ]
    }
   ],
   "source": [
    "import getpass\n",
    "print(getpass.getuser())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "edbaff16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created flow FederatedFlow\n",
      "\n",
      "Calling start\n",
      "\u001b[94m\u001b[1m\u001b[30mPerforming initialization for model\u001b[0m\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94mSaving data artifacts for start\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94mSaved data artifacts for start\u001b[0m\u001b[94m\n",
      "\u001b[0m\n",
      "Calling aggregated_model_validation\n",
      "\u001b[94m\u001b[1m\u001b[31mPerforming aggregated model validation for collaborator Portland\u001b[0m\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94m\u001b[4m\u001b[35mTest set: Avg. loss: 2.3264, Accuracy: 309/2500 (12%)\u001b[0m\u001b[0m\u001b[94m\n",
      "\n",
      "\u001b[0m\u001b[94m\u001b[4m\u001b[31mPortland value of 0.12359999865293503\u001b[0m\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94mSaving data artifacts for aggregated_model_validation\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94mSaved data artifacts for aggregated_model_validation\u001b[0m\u001b[94m\n",
      "\u001b[0m\n",
      "Calling train\n",
      "\u001b[94m\u001b[33mTrain Epoch: 1 [0/15000 (0%)]\tLoss: 2.369644\u001b[0m\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94m\u001b[33mTrain Epoch: 1 [640/15000 (4%)]\tLoss: 2.295182\u001b[0m\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94m\u001b[33mTrain Epoch: 1 [1280/15000 (9%)]\tLoss: 2.272259\u001b[0m\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94m\u001b[33mTrain Epoch: 1 [1920/15000 (13%)]\tLoss: 2.302402\u001b[0m\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94m\u001b[33mTrain Epoch: 1 [2560/15000 (17%)]\tLoss: 2.248393\u001b[0m\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94m\u001b[33mTrain Epoch: 1 [3200/15000 (21%)]\tLoss: 2.218558\u001b[0m\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94m\u001b[33mTrain Epoch: 1 [3840/15000 (26%)]\tLoss: 2.189838\u001b[0m\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94m\u001b[33mTrain Epoch: 1 [4480/15000 (30%)]\tLoss: 2.171542\u001b[0m\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94m\u001b[33mTrain Epoch: 1 [5120/15000 (34%)]\tLoss: 2.017724\u001b[0m\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94m\u001b[33mTrain Epoch: 1 [5760/15000 (38%)]\tLoss: 2.042556\u001b[0m\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94m\u001b[33mTrain Epoch: 1 [6400/15000 (43%)]\tLoss: 1.866701\u001b[0m\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94m\u001b[33mTrain Epoch: 1 [7040/15000 (47%)]\tLoss: 1.726192\u001b[0m\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94m\u001b[33mTrain Epoch: 1 [7680/15000 (51%)]\tLoss: 1.615178\u001b[0m\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94m\u001b[33mTrain Epoch: 1 [8320/15000 (55%)]\tLoss: 1.692942\u001b[0m\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94m\u001b[33mTrain Epoch: 1 [8960/15000 (60%)]\tLoss: 1.703749\u001b[0m\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94m\u001b[33mTrain Epoch: 1 [9600/15000 (64%)]\tLoss: 1.440259\u001b[0m\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94m\u001b[33mTrain Epoch: 1 [10240/15000 (68%)]\tLoss: 1.501244\u001b[0m\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94m\u001b[33mTrain Epoch: 1 [10880/15000 (72%)]\tLoss: 1.575412\u001b[0m\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94m\u001b[33mTrain Epoch: 1 [11520/15000 (77%)]\tLoss: 1.275227\u001b[0m\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94m\u001b[33mTrain Epoch: 1 [12160/15000 (81%)]\tLoss: 1.164355\u001b[0m\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94m\u001b[33mTrain Epoch: 1 [12800/15000 (85%)]\tLoss: 1.091366\u001b[0m\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94m\u001b[33mTrain Epoch: 1 [13440/15000 (89%)]\tLoss: 1.101477\u001b[0m\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94m\u001b[33mTrain Epoch: 1 [14080/15000 (94%)]\tLoss: 1.100811\u001b[0m\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94m\u001b[33mTrain Epoch: 1 [14720/15000 (98%)]\tLoss: 1.068817\u001b[0m\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94mSaving data artifacts for train\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94mSaved data artifacts for train\u001b[0m\u001b[94m\n",
      "\u001b[0m\n",
      "Calling local_model_validation\n",
      "\u001b[94m\u001b[4m\u001b[35mTest set: Avg. loss: 0.7191, Accuracy: 2060/2500 (82%)\u001b[0m\u001b[0m\u001b[94m\n",
      "\n",
      "\u001b[0m\u001b[94m\u001b[97mDoing local model validation for collaborator Portland:                 0.8240000009536743\u001b[0m\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94mSaving data artifacts for local_model_validation\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94mSaved data artifacts for local_model_validation\u001b[0m\u001b[94m\n",
      "\u001b[0mShould transfer from local_model_validation to join\n",
      "\n",
      "Calling aggregated_model_validation\n",
      "\u001b[94m\u001b[1m\u001b[31mPerforming aggregated model validation for collaborator Seattle\u001b[0m\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94m\u001b[4m\u001b[35mTest set: Avg. loss: 2.3319, Accuracy: 272/2500 (11%)\u001b[0m\u001b[0m\u001b[94m\n",
      "\n",
      "\u001b[0m\u001b[94m\u001b[4m\u001b[31mSeattle value of 0.1088000014424324\u001b[0m\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94mSaving data artifacts for aggregated_model_validation\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94mSaved data artifacts for aggregated_model_validation\u001b[0m\u001b[94m\n",
      "\u001b[0m\n",
      "Calling train\n",
      "\u001b[94m\u001b[33mTrain Epoch: 1 [0/15000 (0%)]\tLoss: 2.332791\u001b[0m\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94m\u001b[33mTrain Epoch: 1 [640/15000 (4%)]\tLoss: 2.322751\u001b[0m\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94m\u001b[33mTrain Epoch: 1 [1280/15000 (9%)]\tLoss: 2.269918\u001b[0m\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94m\u001b[33mTrain Epoch: 1 [1920/15000 (13%)]\tLoss: 2.290510\u001b[0m\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94m\u001b[33mTrain Epoch: 1 [2560/15000 (17%)]\tLoss: 2.268531\u001b[0m\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94m\u001b[33mTrain Epoch: 1 [3200/15000 (21%)]\tLoss: 2.226192\u001b[0m\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94m\u001b[33mTrain Epoch: 1 [3840/15000 (26%)]\tLoss: 2.176828\u001b[0m\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94m\u001b[33mTrain Epoch: 1 [4480/15000 (30%)]\tLoss: 2.183002\u001b[0m\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94m\u001b[33mTrain Epoch: 1 [5120/15000 (34%)]\tLoss: 2.105009\u001b[0m\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94m\u001b[33mTrain Epoch: 1 [5760/15000 (38%)]\tLoss: 1.981959\u001b[0m\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94m\u001b[33mTrain Epoch: 1 [6400/15000 (43%)]\tLoss: 1.866464\u001b[0m\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94m\u001b[33mTrain Epoch: 1 [7040/15000 (47%)]\tLoss: 1.679553\u001b[0m\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94m\u001b[33mTrain Epoch: 1 [7680/15000 (51%)]\tLoss: 1.669854\u001b[0m\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94m\u001b[33mTrain Epoch: 1 [8320/15000 (55%)]\tLoss: 1.591703\u001b[0m\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94m\u001b[33mTrain Epoch: 1 [8960/15000 (60%)]\tLoss: 1.148883\u001b[0m\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94m\u001b[33mTrain Epoch: 1 [9600/15000 (64%)]\tLoss: 1.365044\u001b[0m\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94m\u001b[33mTrain Epoch: 1 [10240/15000 (68%)]\tLoss: 1.498770\u001b[0m\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94m\u001b[33mTrain Epoch: 1 [10880/15000 (72%)]\tLoss: 1.078157\u001b[0m\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94m\u001b[33mTrain Epoch: 1 [11520/15000 (77%)]\tLoss: 1.106767\u001b[0m\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94m\u001b[33mTrain Epoch: 1 [12160/15000 (81%)]\tLoss: 0.994163\u001b[0m\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94m\u001b[33mTrain Epoch: 1 [12800/15000 (85%)]\tLoss: 1.065276\u001b[0m\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94m\u001b[33mTrain Epoch: 1 [13440/15000 (89%)]\tLoss: 0.807855\u001b[0m\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94m\u001b[33mTrain Epoch: 1 [14080/15000 (94%)]\tLoss: 0.854803\u001b[0m\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94m\u001b[33mTrain Epoch: 1 [14720/15000 (98%)]\tLoss: 0.860824\u001b[0m\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94mSaving data artifacts for train\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94mSaved data artifacts for train\u001b[0m\u001b[94m\n",
      "\u001b[0m\n",
      "Calling local_model_validation\n",
      "\u001b[94m\u001b[4m\u001b[35mTest set: Avg. loss: 0.6033, Accuracy: 2101/2500 (84%)\u001b[0m\u001b[0m\u001b[94m\n",
      "\n",
      "\u001b[0m\u001b[94m\u001b[97mDoing local model validation for collaborator Seattle:                 0.840399980545044\u001b[0m\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94mSaving data artifacts for local_model_validation\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94mSaved data artifacts for local_model_validation\u001b[0m\u001b[94m\n",
      "\u001b[0mShould transfer from local_model_validation to join\n",
      "\n",
      "Calling aggregated_model_validation\n",
      "\u001b[94m\u001b[1m\u001b[31mPerforming aggregated model validation for collaborator Chandler\u001b[0m\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94m\u001b[4m\u001b[35mTest set: Avg. loss: 2.3338, Accuracy: 284/2500 (11%)\u001b[0m\u001b[0m\u001b[94m\n",
      "\n",
      "\u001b[0m\u001b[94m\u001b[4m\u001b[31mChandler value of 0.1136000007390976\u001b[0m\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94mSaving data artifacts for aggregated_model_validation\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94mSaved data artifacts for aggregated_model_validation\u001b[0m\u001b[94m\n",
      "\u001b[0m\n",
      "Calling train\n",
      "\u001b[94m\u001b[33mTrain Epoch: 1 [0/15000 (0%)]\tLoss: 2.317358\u001b[0m\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94m\u001b[33mTrain Epoch: 1 [640/15000 (4%)]\tLoss: 2.288361\u001b[0m\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94m\u001b[33mTrain Epoch: 1 [1280/15000 (9%)]\tLoss: 2.279685\u001b[0m\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94m\u001b[33mTrain Epoch: 1 [1920/15000 (13%)]\tLoss: 2.282242\u001b[0m\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94m\u001b[33mTrain Epoch: 1 [2560/15000 (17%)]\tLoss: 2.264517\u001b[0m\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94m\u001b[33mTrain Epoch: 1 [3200/15000 (21%)]\tLoss: 2.254941\u001b[0m\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94m\u001b[33mTrain Epoch: 1 [3840/15000 (26%)]\tLoss: 2.207403\u001b[0m\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94m\u001b[33mTrain Epoch: 1 [4480/15000 (30%)]\tLoss: 2.181719\u001b[0m\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94m\u001b[33mTrain Epoch: 1 [5120/15000 (34%)]\tLoss: 2.113139\u001b[0m\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94m\u001b[33mTrain Epoch: 1 [5760/15000 (38%)]\tLoss: 1.985403\u001b[0m\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94m\u001b[33mTrain Epoch: 1 [6400/15000 (43%)]\tLoss: 1.949073\u001b[0m\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94m\u001b[33mTrain Epoch: 1 [7040/15000 (47%)]\tLoss: 1.974892\u001b[0m\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94m\u001b[33mTrain Epoch: 1 [7680/15000 (51%)]\tLoss: 1.725470\u001b[0m\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94m\u001b[33mTrain Epoch: 1 [8320/15000 (55%)]\tLoss: 1.819498\u001b[0m\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94m\u001b[33mTrain Epoch: 1 [8960/15000 (60%)]\tLoss: 1.714153\u001b[0m\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94m\u001b[33mTrain Epoch: 1 [9600/15000 (64%)]\tLoss: 1.559057\u001b[0m\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94m\u001b[33mTrain Epoch: 1 [10240/15000 (68%)]\tLoss: 1.507755\u001b[0m\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94m\u001b[33mTrain Epoch: 1 [10880/15000 (72%)]\tLoss: 1.509037\u001b[0m\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94m\u001b[33mTrain Epoch: 1 [11520/15000 (77%)]\tLoss: 1.268822\u001b[0m\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94m\u001b[33mTrain Epoch: 1 [12160/15000 (81%)]\tLoss: 1.483501\u001b[0m\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94m\u001b[33mTrain Epoch: 1 [12800/15000 (85%)]\tLoss: 1.387972\u001b[0m\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94m\u001b[33mTrain Epoch: 1 [13440/15000 (89%)]\tLoss: 1.187392\u001b[0m\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94m\u001b[33mTrain Epoch: 1 [14080/15000 (94%)]\tLoss: 1.111732\u001b[0m\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94m\u001b[33mTrain Epoch: 1 [14720/15000 (98%)]\tLoss: 1.228651\u001b[0m\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94mSaving data artifacts for train\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94mSaved data artifacts for train\u001b[0m\u001b[94m\n",
      "\u001b[0m\n",
      "Calling local_model_validation\n",
      "\u001b[94m\u001b[4m\u001b[35mTest set: Avg. loss: 0.7696, Accuracy: 1980/2500 (79%)\u001b[0m\u001b[0m\u001b[94m\n",
      "\n",
      "\u001b[0m\u001b[94m\u001b[97mDoing local model validation for collaborator Chandler:                 0.7919999957084656\u001b[0m\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94mSaving data artifacts for local_model_validation\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94mSaved data artifacts for local_model_validation\u001b[0m\u001b[94m\n",
      "\u001b[0mShould transfer from local_model_validation to join\n",
      "\n",
      "Calling aggregated_model_validation\n",
      "\u001b[94m\u001b[1m\u001b[31mPerforming aggregated model validation for collaborator Bangalore\u001b[0m\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94m\u001b[4m\u001b[35mTest set: Avg. loss: 2.3345, Accuracy: 272/2500 (11%)\u001b[0m\u001b[0m\u001b[94m\n",
      "\n",
      "\u001b[0m\u001b[94m\u001b[4m\u001b[31mBangalore value of 0.1088000014424324\u001b[0m\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94mSaving data artifacts for aggregated_model_validation\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94mSaved data artifacts for aggregated_model_validation\u001b[0m\u001b[94m\n",
      "\u001b[0m\n",
      "Calling train\n",
      "\u001b[94m\u001b[33mTrain Epoch: 1 [0/15000 (0%)]\tLoss: 2.368124\u001b[0m\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94m\u001b[33mTrain Epoch: 1 [640/15000 (4%)]\tLoss: 2.331737\u001b[0m\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94m\u001b[33mTrain Epoch: 1 [1280/15000 (9%)]\tLoss: 2.282820\u001b[0m\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94m\u001b[33mTrain Epoch: 1 [1920/15000 (13%)]\tLoss: 2.287500\u001b[0m\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94m\u001b[33mTrain Epoch: 1 [2560/15000 (17%)]\tLoss: 2.230427\u001b[0m\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94m\u001b[33mTrain Epoch: 1 [3200/15000 (21%)]\tLoss: 2.227893\u001b[0m\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94m\u001b[33mTrain Epoch: 1 [3840/15000 (26%)]\tLoss: 2.133003\u001b[0m\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94m\u001b[33mTrain Epoch: 1 [4480/15000 (30%)]\tLoss: 2.075834\u001b[0m\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94m\u001b[33mTrain Epoch: 1 [5120/15000 (34%)]\tLoss: 2.017371\u001b[0m\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94m\u001b[33mTrain Epoch: 1 [5760/15000 (38%)]\tLoss: 2.004196\u001b[0m\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94m\u001b[33mTrain Epoch: 1 [6400/15000 (43%)]\tLoss: 1.832824\u001b[0m\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94m\u001b[33mTrain Epoch: 1 [7040/15000 (47%)]\tLoss: 1.659397\u001b[0m\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94m\u001b[33mTrain Epoch: 1 [7680/15000 (51%)]\tLoss: 1.705898\u001b[0m\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94m\u001b[33mTrain Epoch: 1 [8320/15000 (55%)]\tLoss: 1.592872\u001b[0m\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94m\u001b[33mTrain Epoch: 1 [8960/15000 (60%)]\tLoss: 1.357648\u001b[0m\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94m\u001b[33mTrain Epoch: 1 [9600/15000 (64%)]\tLoss: 1.347577\u001b[0m\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94m\u001b[33mTrain Epoch: 1 [10240/15000 (68%)]\tLoss: 1.140102\u001b[0m\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94m\u001b[33mTrain Epoch: 1 [10880/15000 (72%)]\tLoss: 1.184234\u001b[0m\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94m\u001b[33mTrain Epoch: 1 [11520/15000 (77%)]\tLoss: 1.044160\u001b[0m\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94m\u001b[33mTrain Epoch: 1 [12160/15000 (81%)]\tLoss: 1.061091\u001b[0m\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94m\u001b[33mTrain Epoch: 1 [12800/15000 (85%)]\tLoss: 1.152420\u001b[0m\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94m\u001b[33mTrain Epoch: 1 [13440/15000 (89%)]\tLoss: 1.053633\u001b[0m\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94m\u001b[33mTrain Epoch: 1 [14080/15000 (94%)]\tLoss: 1.156082\u001b[0m\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94m\u001b[33mTrain Epoch: 1 [14720/15000 (98%)]\tLoss: 0.825849\u001b[0m\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94mSaving data artifacts for train\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94mSaved data artifacts for train\u001b[0m\u001b[94m\n",
      "\u001b[0m\n",
      "Calling local_model_validation\n",
      "\u001b[94m\u001b[4m\u001b[35mTest set: Avg. loss: 0.5950, Accuracy: 2069/2500 (83%)\u001b[0m\u001b[0m\u001b[94m\n",
      "\n",
      "\u001b[0m\u001b[94m\u001b[97mDoing local model validation for collaborator Bangalore:                 0.8276000022888184\u001b[0m\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94mSaving data artifacts for local_model_validation\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94mSaved data artifacts for local_model_validation\u001b[0m\u001b[94m\n",
      "\u001b[0mShould transfer from local_model_validation to join\n",
      "\n",
      "Calling join\n",
      "\u001b[94m\u001b[32mAverage aggregated model validation values =                 0.11370000056922436\u001b[0m\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94m\u001b[32mAverage training loss = 0.9960352182388306\u001b[0m\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94m\u001b[32mAverage local model validation values =             0.8209999948740005\u001b[0m\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94mSaving data artifacts for join\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94mSaved data artifacts for join\u001b[0m\u001b[94m\n",
      "\u001b[0m\n",
      "Calling aggregated_model_validation\n",
      "\u001b[94m\u001b[1m\u001b[31mPerforming aggregated model validation for collaborator Portland\u001b[0m\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94m\u001b[4m\u001b[35mTest set: Avg. loss: 0.6811, Accuracy: 2114/2500 (85%)\u001b[0m\u001b[0m\u001b[94m\n",
      "\n",
      "\u001b[0m\u001b[94m\u001b[4m\u001b[31mPortland value of 0.8456000089645386\u001b[0m\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94mSaving data artifacts for aggregated_model_validation\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94mSaved data artifacts for aggregated_model_validation\u001b[0m\u001b[94m\n",
      "\u001b[0m\n",
      "Calling train\n",
      "\u001b[94m\u001b[33mTrain Epoch: 1 [0/15000 (0%)]\tLoss: 1.153695\u001b[0m\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94m\u001b[33mTrain Epoch: 1 [640/15000 (4%)]\tLoss: 1.079856\u001b[0m\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94m\u001b[33mTrain Epoch: 1 [1280/15000 (9%)]\tLoss: 0.985835\u001b[0m\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94m\u001b[33mTrain Epoch: 1 [1920/15000 (13%)]\tLoss: 1.011191\u001b[0m\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94m\u001b[33mTrain Epoch: 1 [2560/15000 (17%)]\tLoss: 1.058357\u001b[0m\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94m\u001b[33mTrain Epoch: 1 [3200/15000 (21%)]\tLoss: 0.708941\u001b[0m\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94m\u001b[33mTrain Epoch: 1 [3840/15000 (26%)]\tLoss: 1.223528\u001b[0m\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94m\u001b[33mTrain Epoch: 1 [4480/15000 (30%)]\tLoss: 0.864369\u001b[0m\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94m\u001b[33mTrain Epoch: 1 [5120/15000 (34%)]\tLoss: 1.032842\u001b[0m\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94m\u001b[33mTrain Epoch: 1 [5760/15000 (38%)]\tLoss: 1.030515\u001b[0m\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94m\u001b[33mTrain Epoch: 1 [6400/15000 (43%)]\tLoss: 0.828151\u001b[0m\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94m\u001b[33mTrain Epoch: 1 [7040/15000 (47%)]\tLoss: 0.847214\u001b[0m\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94m\u001b[33mTrain Epoch: 1 [7680/15000 (51%)]\tLoss: 0.684994\u001b[0m\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94m\u001b[33mTrain Epoch: 1 [8320/15000 (55%)]\tLoss: 0.811192\u001b[0m\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94m\u001b[33mTrain Epoch: 1 [8960/15000 (60%)]\tLoss: 0.830859\u001b[0m\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94m\u001b[33mTrain Epoch: 1 [9600/15000 (64%)]\tLoss: 0.800170\u001b[0m\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94m\u001b[33mTrain Epoch: 1 [10240/15000 (68%)]\tLoss: 1.005376\u001b[0m\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94m\u001b[33mTrain Epoch: 1 [10880/15000 (72%)]\tLoss: 0.531628\u001b[0m\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94m\u001b[33mTrain Epoch: 1 [11520/15000 (77%)]\tLoss: 0.809211\u001b[0m\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94m\u001b[33mTrain Epoch: 1 [12160/15000 (81%)]\tLoss: 0.766890\u001b[0m\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94m\u001b[33mTrain Epoch: 1 [12800/15000 (85%)]\tLoss: 0.650979\u001b[0m\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94m\u001b[33mTrain Epoch: 1 [13440/15000 (89%)]\tLoss: 0.862463\u001b[0m\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94m\u001b[33mTrain Epoch: 1 [14080/15000 (94%)]\tLoss: 0.690397\u001b[0m\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94m\u001b[33mTrain Epoch: 1 [14720/15000 (98%)]\tLoss: 0.620498\u001b[0m\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94mSaving data artifacts for train\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94mSaved data artifacts for train\u001b[0m\u001b[94m\n",
      "\u001b[0m\n",
      "Calling local_model_validation\n",
      "\u001b[94m\u001b[4m\u001b[35mTest set: Avg. loss: 0.3488, Accuracy: 2255/2500 (90%)\u001b[0m\u001b[0m\u001b[94m\n",
      "\n",
      "\u001b[0m\u001b[94m\u001b[97mDoing local model validation for collaborator Portland:                 0.9020000100135803\u001b[0m\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94mSaving data artifacts for local_model_validation\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94mSaved data artifacts for local_model_validation\u001b[0m\u001b[94m\n",
      "\u001b[0mShould transfer from local_model_validation to join\n",
      "\n",
      "Calling aggregated_model_validation\n",
      "\u001b[94m\u001b[1m\u001b[31mPerforming aggregated model validation for collaborator Seattle\u001b[0m\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94m\u001b[4m\u001b[35mTest set: Avg. loss: 0.6850, Accuracy: 2127/2500 (85%)\u001b[0m\u001b[0m\u001b[94m\n",
      "\n",
      "\u001b[0m\u001b[94m\u001b[4m\u001b[31mSeattle value of 0.8507999777793884\u001b[0m\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94mSaving data artifacts for aggregated_model_validation\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94mSaved data artifacts for aggregated_model_validation\u001b[0m\u001b[94m\n",
      "\u001b[0m\n",
      "Calling train\n",
      "\u001b[94m\u001b[33mTrain Epoch: 1 [0/15000 (0%)]\tLoss: 0.967068\u001b[0m\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94m\u001b[33mTrain Epoch: 1 [640/15000 (4%)]\tLoss: 0.847724\u001b[0m\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94m\u001b[33mTrain Epoch: 1 [1280/15000 (9%)]\tLoss: 1.028218\u001b[0m\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94m\u001b[33mTrain Epoch: 1 [1920/15000 (13%)]\tLoss: 0.929041\u001b[0m\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94m\u001b[33mTrain Epoch: 1 [2560/15000 (17%)]\tLoss: 0.945038\u001b[0m\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94m\u001b[33mTrain Epoch: 1 [3200/15000 (21%)]\tLoss: 0.777765\u001b[0m\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94m\u001b[33mTrain Epoch: 1 [3840/15000 (26%)]\tLoss: 0.967960\u001b[0m\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94m\u001b[33mTrain Epoch: 1 [4480/15000 (30%)]\tLoss: 0.600011\u001b[0m\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94m\u001b[33mTrain Epoch: 1 [5120/15000 (34%)]\tLoss: 0.537120\u001b[0m\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94m\u001b[33mTrain Epoch: 1 [5760/15000 (38%)]\tLoss: 0.578831\u001b[0m\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94m\u001b[33mTrain Epoch: 1 [6400/15000 (43%)]\tLoss: 0.687161\u001b[0m\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94m\u001b[33mTrain Epoch: 1 [7040/15000 (47%)]\tLoss: 0.639167\u001b[0m\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94m\u001b[33mTrain Epoch: 1 [7680/15000 (51%)]\tLoss: 0.442239\u001b[0m\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94m\u001b[33mTrain Epoch: 1 [8320/15000 (55%)]\tLoss: 0.486966\u001b[0m\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94m\u001b[33mTrain Epoch: 1 [8960/15000 (60%)]\tLoss: 0.671592\u001b[0m\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94m\u001b[33mTrain Epoch: 1 [9600/15000 (64%)]\tLoss: 0.604600\u001b[0m\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94m\u001b[33mTrain Epoch: 1 [10240/15000 (68%)]\tLoss: 0.527363\u001b[0m\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94m\u001b[33mTrain Epoch: 1 [10880/15000 (72%)]\tLoss: 0.457734\u001b[0m\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94m\u001b[33mTrain Epoch: 1 [11520/15000 (77%)]\tLoss: 0.578533\u001b[0m\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94m\u001b[33mTrain Epoch: 1 [12160/15000 (81%)]\tLoss: 0.692348\u001b[0m\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94m\u001b[33mTrain Epoch: 1 [12800/15000 (85%)]\tLoss: 0.662212\u001b[0m\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94m\u001b[33mTrain Epoch: 1 [13440/15000 (89%)]\tLoss: 0.462714\u001b[0m\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94m\u001b[33mTrain Epoch: 1 [14080/15000 (94%)]\tLoss: 0.735435\u001b[0m\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94m\u001b[33mTrain Epoch: 1 [14720/15000 (98%)]\tLoss: 0.483812\u001b[0m\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94mSaving data artifacts for train\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94mSaved data artifacts for train\u001b[0m\u001b[94m\n",
      "\u001b[0m\n",
      "Calling local_model_validation\n",
      "\u001b[94m\u001b[4m\u001b[35mTest set: Avg. loss: 0.3609, Accuracy: 2229/2500 (89%)\u001b[0m\u001b[0m\u001b[94m\n",
      "\n",
      "\u001b[0m\u001b[94m\u001b[97mDoing local model validation for collaborator Seattle:                 0.8916000127792358\u001b[0m\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94mSaving data artifacts for local_model_validation\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94mSaved data artifacts for local_model_validation\u001b[0m\u001b[94m\n",
      "\u001b[0mShould transfer from local_model_validation to join\n",
      "\n",
      "Calling aggregated_model_validation\n",
      "\u001b[94m\u001b[1m\u001b[31mPerforming aggregated model validation for collaborator Chandler\u001b[0m\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94m\u001b[4m\u001b[35mTest set: Avg. loss: 0.6960, Accuracy: 2096/2500 (84%)\u001b[0m\u001b[0m\u001b[94m\n",
      "\n",
      "\u001b[0m\u001b[94m\u001b[4m\u001b[31mChandler value of 0.8384000062942505\u001b[0m\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94mSaving data artifacts for aggregated_model_validation\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94mSaved data artifacts for aggregated_model_validation\u001b[0m\u001b[94m\n",
      "\u001b[0m\n",
      "Calling train\n",
      "\u001b[94m\u001b[33mTrain Epoch: 1 [0/15000 (0%)]\tLoss: 1.057825\u001b[0m\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94m\u001b[33mTrain Epoch: 1 [640/15000 (4%)]\tLoss: 1.149323\u001b[0m\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94m\u001b[33mTrain Epoch: 1 [1280/15000 (9%)]\tLoss: 1.103794\u001b[0m\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94m\u001b[33mTrain Epoch: 1 [1920/15000 (13%)]\tLoss: 1.100161\u001b[0m\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94m\u001b[33mTrain Epoch: 1 [2560/15000 (17%)]\tLoss: 0.933288\u001b[0m\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94m\u001b[33mTrain Epoch: 1 [3200/15000 (21%)]\tLoss: 0.876338\u001b[0m\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94m\u001b[33mTrain Epoch: 1 [3840/15000 (26%)]\tLoss: 1.205927\u001b[0m\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94m\u001b[33mTrain Epoch: 1 [4480/15000 (30%)]\tLoss: 0.781079\u001b[0m\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94m\u001b[33mTrain Epoch: 1 [5120/15000 (34%)]\tLoss: 0.667103\u001b[0m\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94m\u001b[33mTrain Epoch: 1 [5760/15000 (38%)]\tLoss: 1.186395\u001b[0m\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94m\u001b[33mTrain Epoch: 1 [6400/15000 (43%)]\tLoss: 0.869434\u001b[0m\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94m\u001b[33mTrain Epoch: 1 [7040/15000 (47%)]\tLoss: 0.842406\u001b[0m\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94m\u001b[33mTrain Epoch: 1 [7680/15000 (51%)]\tLoss: 0.536103\u001b[0m\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94m\u001b[33mTrain Epoch: 1 [8320/15000 (55%)]\tLoss: 0.925663\u001b[0m\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94m\u001b[33mTrain Epoch: 1 [8960/15000 (60%)]\tLoss: 0.779027\u001b[0m\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94m\u001b[33mTrain Epoch: 1 [9600/15000 (64%)]\tLoss: 0.909951\u001b[0m\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94m\u001b[33mTrain Epoch: 1 [10240/15000 (68%)]\tLoss: 0.701488\u001b[0m\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94m\u001b[33mTrain Epoch: 1 [10880/15000 (72%)]\tLoss: 0.757648\u001b[0m\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94m\u001b[33mTrain Epoch: 1 [11520/15000 (77%)]\tLoss: 0.837237\u001b[0m\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94m\u001b[33mTrain Epoch: 1 [12160/15000 (81%)]\tLoss: 0.755259\u001b[0m\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94m\u001b[33mTrain Epoch: 1 [12800/15000 (85%)]\tLoss: 0.717903\u001b[0m\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94m\u001b[33mTrain Epoch: 1 [13440/15000 (89%)]\tLoss: 0.559139\u001b[0m\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94m\u001b[33mTrain Epoch: 1 [14080/15000 (94%)]\tLoss: 0.717785\u001b[0m\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94m\u001b[33mTrain Epoch: 1 [14720/15000 (98%)]\tLoss: 0.501152\u001b[0m\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94mSaving data artifacts for train\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94mSaved data artifacts for train\u001b[0m\u001b[94m\n",
      "\u001b[0m\n",
      "Calling local_model_validation\n",
      "\u001b[94m\u001b[4m\u001b[35mTest set: Avg. loss: 0.3657, Accuracy: 2243/2500 (90%)\u001b[0m\u001b[0m\u001b[94m\n",
      "\n",
      "\u001b[0m\u001b[94m\u001b[97mDoing local model validation for collaborator Chandler:                 0.8971999883651733\u001b[0m\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94mSaving data artifacts for local_model_validation\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94mSaved data artifacts for local_model_validation\u001b[0m\u001b[94m\n",
      "\u001b[0mShould transfer from local_model_validation to join\n",
      "\n",
      "Calling aggregated_model_validation\n",
      "\u001b[94m\u001b[1m\u001b[31mPerforming aggregated model validation for collaborator Bangalore\u001b[0m\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94m\u001b[4m\u001b[35mTest set: Avg. loss: 0.6781, Accuracy: 2128/2500 (85%)\u001b[0m\u001b[0m\u001b[94m\n",
      "\n",
      "\u001b[0m\u001b[94m\u001b[4m\u001b[31mBangalore value of 0.8511999845504761\u001b[0m\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94mSaving data artifacts for aggregated_model_validation\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94mSaved data artifacts for aggregated_model_validation\u001b[0m\u001b[94m\n",
      "\u001b[0m\n",
      "Calling train\n",
      "\u001b[94m\u001b[33mTrain Epoch: 1 [0/15000 (0%)]\tLoss: 0.915421\u001b[0m\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94m\u001b[33mTrain Epoch: 1 [640/15000 (4%)]\tLoss: 1.056651\u001b[0m\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94m\u001b[33mTrain Epoch: 1 [1280/15000 (9%)]\tLoss: 0.932735\u001b[0m\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94m\u001b[33mTrain Epoch: 1 [1920/15000 (13%)]\tLoss: 0.779836\u001b[0m\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94m\u001b[33mTrain Epoch: 1 [2560/15000 (17%)]\tLoss: 0.931371\u001b[0m\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94m\u001b[33mTrain Epoch: 1 [3200/15000 (21%)]\tLoss: 0.714225\u001b[0m\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94m\u001b[33mTrain Epoch: 1 [3840/15000 (26%)]\tLoss: 0.791259\u001b[0m\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94m\u001b[33mTrain Epoch: 1 [4480/15000 (30%)]\tLoss: 0.761956\u001b[0m\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94m\u001b[33mTrain Epoch: 1 [5120/15000 (34%)]\tLoss: 0.528267\u001b[0m\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94m\u001b[33mTrain Epoch: 1 [5760/15000 (38%)]\tLoss: 0.594430\u001b[0m\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94m\u001b[33mTrain Epoch: 1 [6400/15000 (43%)]\tLoss: 0.545077\u001b[0m\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94m\u001b[33mTrain Epoch: 1 [7040/15000 (47%)]\tLoss: 0.501405\u001b[0m\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94m\u001b[33mTrain Epoch: 1 [7680/15000 (51%)]\tLoss: 0.592633\u001b[0m\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94m\u001b[33mTrain Epoch: 1 [8320/15000 (55%)]\tLoss: 0.568790\u001b[0m\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94m\u001b[33mTrain Epoch: 1 [8960/15000 (60%)]\tLoss: 0.785154\u001b[0m\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94m\u001b[33mTrain Epoch: 1 [9600/15000 (64%)]\tLoss: 0.612770\u001b[0m\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94m\u001b[33mTrain Epoch: 1 [10240/15000 (68%)]\tLoss: 0.614112\u001b[0m\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94m\u001b[33mTrain Epoch: 1 [10880/15000 (72%)]\tLoss: 0.608250\u001b[0m\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94m\u001b[33mTrain Epoch: 1 [11520/15000 (77%)]\tLoss: 0.634294\u001b[0m\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94m\u001b[33mTrain Epoch: 1 [12160/15000 (81%)]\tLoss: 0.381248\u001b[0m\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94m\u001b[33mTrain Epoch: 1 [12800/15000 (85%)]\tLoss: 0.688973\u001b[0m\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94m\u001b[33mTrain Epoch: 1 [13440/15000 (89%)]\tLoss: 0.661141\u001b[0m\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94m\u001b[33mTrain Epoch: 1 [14080/15000 (94%)]\tLoss: 0.733536\u001b[0m\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94m\u001b[33mTrain Epoch: 1 [14720/15000 (98%)]\tLoss: 0.501793\u001b[0m\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94mSaving data artifacts for train\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94mSaved data artifacts for train\u001b[0m\u001b[94m\n",
      "\u001b[0m\n",
      "Calling local_model_validation\n",
      "\u001b[94m\u001b[4m\u001b[35mTest set: Avg. loss: 0.3500, Accuracy: 2244/2500 (90%)\u001b[0m\u001b[0m\u001b[94m\n",
      "\n",
      "\u001b[0m\u001b[94m\u001b[97mDoing local model validation for collaborator Bangalore:                 0.897599995136261\u001b[0m\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94mSaving data artifacts for local_model_validation\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94mSaved data artifacts for local_model_validation\u001b[0m\u001b[94m\n",
      "\u001b[0mShould transfer from local_model_validation to join\n",
      "\n",
      "Calling join\n",
      "\u001b[94m\u001b[32mAverage aggregated model validation values =                 0.8464999943971634\u001b[0m\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94m\u001b[32mAverage training loss = 0.5268138125538826\u001b[0m\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94m\u001b[32mAverage local model validation values =             0.8971000015735626\u001b[0m\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94mSaving data artifacts for join\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94mSaved data artifacts for join\u001b[0m\u001b[94m\n",
      "\u001b[0m\n",
      "Calling end\n",
      "\u001b[94m\u001b[30mThis is the end of the flow\u001b[0m\u001b[0m\u001b[94m\n",
      "\u001b[0mSaving data artifacts for end\n",
      "Saved data artifacts for end\n"
     ]
    }
   ],
   "source": [
    "model = None\n",
    "optimizer = None\n",
    "flflow = FederatedFlow(model, optimizer, rounds=2, checkpoint=True)\n",
    "flflow.runtime = local_runtime\n",
    "flflow.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "dc503676",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample of the final model weights: tensor([[[ 0.1227, -0.0843, -0.0651,  0.0559, -0.2075],\n",
      "         [ 0.1562, -0.0230,  0.0963,  0.0244, -0.0143],\n",
      "         [ 0.1063,  0.0541,  0.1069, -0.0386,  0.0210],\n",
      "         [ 0.0124,  0.0622,  0.0314,  0.2054,  0.0920],\n",
      "         [-0.0780, -0.1249, -0.0430, -0.0908, -0.0533]]], device='mps:0')\n",
      "\n",
      "Final aggregated model accuracy for 2 rounds of training:         0.8464999943971634\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "    f'Sample of the final model weights: {flflow.model.state_dict()[\"conv1.weight\"][0]}'\n",
    ")\n",
    "\n",
    "print(\n",
    "    f\"\\nFinal aggregated model accuracy for {flflow.rounds} rounds of training: \\\n",
    "        {flflow.aggregated_model_accuracy}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4502c647",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44773e14",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70bbc67e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7770af23",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8be0e43a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv-python3-11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
