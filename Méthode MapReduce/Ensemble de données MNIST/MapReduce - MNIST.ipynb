{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b000a2ac",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0581e131",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import lit\n",
    "\n",
    "from pyspark.ml.feature import VectorAssembler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f3e5183b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f644c155",
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import reduce"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d8bfea4",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "21828933",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from torchsummary import summary\n",
    "\n",
    "from torchvision import datasets, models, transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9e8c609",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "eecbb34b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from termcolor import cprint"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bfa2539",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5ad59f8a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/haozhang/GitHub/openfl_projet/venv-python3-11/lib/python3.11/site-packages/pyspark'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "findspark.find()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da545145",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "66487235",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 28, 28, 60000])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_path = \"/tmp/files/\"\n",
    "\n",
    "tensor_mnist = datasets.MNIST(\n",
    "    data_path, train=True, download=True, transform=transforms.ToTensor()\n",
    ")\n",
    "\n",
    "tensor_images = torch.stack([tensor_image for tensor_image, _ in tensor_mnist], dim=3)\n",
    "\n",
    "tensor_images.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3f6a8202",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.1307])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor_mean = tensor_images.view(1, -1).mean(dim=1)\n",
    "tensor_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "89f30bbf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.3081])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor_std = tensor_images.view(1, -1).std(dim=1)\n",
    "tensor_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "450a0a57",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "01. torchvision.transforms.Compose(transforms)\n",
    "    - Composes several transforms together.\n",
    "\n",
    "02. torchvision.transforms.Normalize(mean, std, inplace=False)\n",
    "    - Normalize a tensor image with mean and standard deviation.\n",
    "    - output[channel] = (input[channel] - mean[channel]) / std[channel]\n",
    "\"\"\"\n",
    "\n",
    "mnist_train = datasets.MNIST(\n",
    "    \"/tmp/files/\",\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=transforms.Compose(\n",
    "        [\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(tensor_mean, tensor_std),\n",
    "        ]\n",
    "    ),\n",
    ")\n",
    "\n",
    "mnist_test = datasets.MNIST(\n",
    "    \"/tmp/files/\",\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=transforms.Compose(\n",
    "        [\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(tensor_mean, tensor_std),\n",
    "        ]\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd2d0725",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "53afa000",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/04/10 21:22:12 WARN Utils: Your hostname, MacBookPro-2022.local resolves to a loopback address: 127.0.0.1; using 192.168.28.167 instead (on interface en0)\n",
      "25/04/10 21:22:12 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/04/10 21:22:12 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder.master('local[*]') \\\n",
    "    .appName(\"model_training\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4f08c850",
   "metadata": {},
   "outputs": [],
   "source": [
    "zero_train = (\n",
    "    spark.read.format(\"image\")\n",
    "    .load(\"/tmp/files/MNIST/raw/train-images-idx3-ubyte\")\n",
    "    .withColumn(\"label\", lit(0))\n",
    ")\n",
    "one_train = (\n",
    "    spark.read.format(\"image\")\n",
    "    .load(\"/tmp/files/MNIST/raw/train-images-idx3-ubyte\")\n",
    "    .withColumn(\"label\", lit(1))\n",
    ")\n",
    "two_train = (\n",
    "    spark.read.format(\"image\")\n",
    "    .load(\"/tmp/files/MNIST/raw/train-images-idx3-ubyte\")\n",
    "    .withColumn(\"label\", lit(2))\n",
    ")\n",
    "three_train = (\n",
    "    spark.read.format(\"image\")\n",
    "    .load(\"/tmp/files/MNIST/raw/train-images-idx3-ubyte\")\n",
    "    .withColumn(\"label\", lit(3))\n",
    ")\n",
    "four_train = (\n",
    "    spark.read.format(\"image\")\n",
    "    .load(\"/tmp/files/MNIST/raw/train-images-idx3-ubyte\")\n",
    "    .withColumn(\"label\", lit(4))\n",
    ")\n",
    "five_train = (\n",
    "    spark.read.format(\"image\")\n",
    "    .load(\"/tmp/files/MNIST/raw/train-images-idx3-ubyte\")\n",
    "    .withColumn(\"label\", lit(5))\n",
    ")\n",
    "six_train = (\n",
    "    spark.read.format(\"image\")\n",
    "    .load(\"/tmp/files/MNIST/raw/train-images-idx3-ubyte\")\n",
    "    .withColumn(\"label\", lit(6))\n",
    ")\n",
    "seven_train = (\n",
    "    spark.read.format(\"image\")\n",
    "    .load(\"/tmp/files/MNIST/raw/train-images-idx3-ubyte\")\n",
    "    .withColumn(\"label\", lit(7))\n",
    ")\n",
    "eight_train = (\n",
    "    spark.read.format(\"image\")\n",
    "    .load(\"/tmp/files/MNIST/raw/train-images-idx3-ubyte\")\n",
    "    .withColumn(\"label\", lit(8))\n",
    ")\n",
    "nine_train = (\n",
    "    spark.read.format(\"image\")\n",
    "    .load(\"/tmp/files/MNIST/raw/train-images-idx3-ubyte\")\n",
    "    .withColumn(\"label\", lit(9))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "09b5ab7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 0:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------------------------------------+-----+------+\n",
      "|origin                                             |width|height|\n",
      "+---------------------------------------------------+-----+------+\n",
      "|file:///tmp/files/MNIST/raw/train-images-idx3-ubyte|8    |3     |\n",
      "+---------------------------------------------------+-----+------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "zero_train.select(\"image.origin\", \"image.width\", \"image.height\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c7fc46f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+\n",
      "|               image|label|\n",
      "+--------------------+-----+\n",
      "|{file:///tmp/file...|    0|\n",
      "+--------------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "zero_train.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "389db99b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 2)\n"
     ]
    }
   ],
   "source": [
    "print((zero_train.count(), len(zero_train.columns)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0debfa95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+\n",
      "|               image|label|\n",
      "+--------------------+-----+\n",
      "|{file:///tmp/file...|    0|\n",
      "|{file:///tmp/file...|    1|\n",
      "|{file:///tmp/file...|    2|\n",
      "|{file:///tmp/file...|    3|\n",
      "|{file:///tmp/file...|    4|\n",
      "|{file:///tmp/file...|    5|\n",
      "|{file:///tmp/file...|    6|\n",
      "|{file:///tmp/file...|    7|\n",
      "|{file:///tmp/file...|    8|\n",
      "|{file:///tmp/file...|    9|\n",
      "+--------------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_list = [\n",
    "    zero_train,\n",
    "    one_train,\n",
    "    two_train,\n",
    "    three_train,\n",
    "    four_train,\n",
    "    five_train,\n",
    "    six_train,\n",
    "    seven_train,\n",
    "    eight_train,\n",
    "    nine_train,\n",
    "]\n",
    "\n",
    "# merge data frame\n",
    "df_train = reduce(lambda first, second: first.union(second), df_list)\n",
    "\n",
    "df_train.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fa2b5eb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 2)\n"
     ]
    }
   ],
   "source": [
    "print((df_train.count(), len(df_train.columns)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3fb34234",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200\n",
      "(10, 2)\n",
      "+--------------------+-----+\n",
      "|               image|label|\n",
      "+--------------------+-----+\n",
      "|{file:///tmp/file...|    5|\n",
      "|{file:///tmp/file...|    0|\n",
      "|{file:///tmp/file...|    2|\n",
      "|{file:///tmp/file...|    3|\n",
      "|{file:///tmp/file...|    4|\n",
      "|{file:///tmp/file...|    8|\n",
      "|{file:///tmp/file...|    6|\n",
      "|{file:///tmp/file...|    1|\n",
      "|{file:///tmp/file...|    7|\n",
      "|{file:///tmp/file...|    9|\n",
      "+--------------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_train = df_train.repartition(200)\n",
    "print(df_train.rdd.getNumPartitions())\n",
    "print((df_train.count(), len(df_train.columns)))\n",
    "df_train.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f306950f",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[19]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28mprint\u001b[39m((\u001b[43mtrain\u001b[49m.count(), \u001b[38;5;28mlen\u001b[39m(train.columns)))\n",
      "\u001b[31mNameError\u001b[39m: name 'train' is not defined"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/04/10 21:22:29 WARN GarbageCollectionMetrics: To enable non-built-in garbage collector(s) List(G1 Concurrent GC), users should configure it(them) to spark.eventLog.gcMetrics.youngGenerationGarbageCollectors or spark.eventLog.gcMetrics.oldGenerationGarbageCollectors\n"
     ]
    }
   ],
   "source": [
    "print((train.count(), len(train.columns)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d339df11",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4676e39a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_for_eval():\n",
    "    \"\"\"Gets the broadcasted model.\"\"\"\n",
    "    model = models.resnet50(weights=None)\n",
    "    model.load_state_dict(bc_model_state.value)\n",
    "    model.eval()\n",
    "    return model\n",
    "\n",
    "sc = spark.sparkContext\n",
    "model_state = models.resnet50(weights=None).state_dict()\n",
    "bc_model_state = sc.broadcast(model_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e36d9b66",
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.backends.mps.is_available():\n",
    "    cprint(\"MPS is available\", \"green\")\n",
    "    device = torch.device(\"mps:0\")\n",
    "elif torch.backends.cuda.is_available():\n",
    "    cprint(\"CUDA is available\", \"green\")\n",
    "    device = torch.device(\"cuda:0\")\n",
    "elif torch.backends.cudnn.is_built():\n",
    "    cprint(\"CUDNN is available\", \"green\")\n",
    "    device = torch.device(\"cuda:0\")\n",
    "else:\n",
    "    cprint(\"CUDA and MPS are not available\", \"red\")\n",
    "    cprint(\"Using CPU\", \"red\")\n",
    "    device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f7f1cf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_df = train.select(col(\"image\"), predict_batch_udf(col(\"label\")).alias(\"prediction\"))\n",
    "predictions_df \\\n",
    "    .write \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .parquet(\"hdfs://xxx/output/\")\n",
    "\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74cfce21",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_batch_udf(paths: pd.Series) -> pd.Series:\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize(224),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                             std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "    images = ImageDataset(paths, transform=transform)\n",
    "    loader = torch.utils.data.DataLoader(images, batch_size=500, num_workers=8)\n",
    "    model = get_model_for_eval()\n",
    "    model.to(device)\n",
    "    all_predictions = []\n",
    "    with torch.no_grad():\n",
    "        for batch in loader:\n",
    "            predictions = list(model(batch.to(device)).cpu().numpy())\n",
    "            for prediction in predictions:\n",
    "                all_predictions.append(prediction)\n",
    "    return pd.Series(all_predictions)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv-python3-11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
