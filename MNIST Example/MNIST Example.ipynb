{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a66a6430",
   "metadata": {},
   "source": [
    "**Table of contents**<a id='toc0_'></a>    \n",
    "- [Importation des bibliothèques](#toc1_)    \n",
    "  - [Importation des paquets ou modules de la bibliothèque OpenFL](#toc1_1_)    \n",
    "  - [Importation des paquets ou modules de la bibliothèque PyTorch](#toc1_2_)    \n",
    "  - [Importation d’autres paquets ou modules requis](#toc1_3_)    \n",
    "- [Définition du modèle d‘entraînement](#toc2_)    \n",
    "  - [Définition des chargeurs de données](#toc2_1_)    \n",
    "  - [Définition du modèle de réseau CNN](#toc2_2_)    \n",
    "  - [Définition de la fonction d'inférence utilisée dans le test](#toc2_3_)    \n",
    "- [Définition des règles de l'apprentissage fédéré](#toc3_)    \n",
    "  - [Méthode de calcul de la moyenne des poids d'apprentissage fédéré](#toc3_1_)    \n",
    "\n",
    "<!-- vscode-jupyter-toc-config\n",
    "\tnumbering=false\n",
    "\tanchor=true\n",
    "\tflat=false\n",
    "\tminLevel=1\n",
    "\tmaxLevel=6\n",
    "\t/vscode-jupyter-toc-config -->\n",
    "<!-- THIS CELL WILL BE REPLACED ON TOC UPDATE. DO NOT WRITE YOUR TEXT IN THIS CELL -->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e7fe1d1",
   "metadata": {},
   "source": [
    "# <a id='toc1_'></a>[Importation des bibliothèques](#toc0_)\n",
    "\n",
    "## <a id='toc1_1_'></a>[Importation des paquets ou modules de la bibliothèque OpenFL](#toc0_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4f63aa4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# La classe `FLSpec` définit la spécification du flux.\n",
    "\n",
    "# Les flux définis par l'utilisateur sont des sous-classes de cette classe.\n",
    "from openfl.experimental.workflow.interface import Aggregator, Collaborator, FLSpec\n",
    "# La fonction `aggregator/collaborator` est un décorateur de placement qui définit l'endroit où la \n",
    "# tâche sera assignée.\n",
    "from openfl.experimental.workflow.placement import aggregator, collaborator\n",
    "from openfl.experimental.workflow.runtime import LocalRuntime"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c852c64d",
   "metadata": {},
   "source": [
    "## <a id='toc1_2_'></a>[Importation des paquets ou modules de la bibliothèque PyTorch](#toc0_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2b89c419",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from torchsummary import summary\n",
    "\n",
    "import torchvision"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60958c33",
   "metadata": {},
   "source": [
    "## <a id='toc1_3_'></a>[Importation d’autres paquets ou modules requis](#toc0_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3d76255d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from termcolor import cprint"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22594881",
   "metadata": {},
   "source": [
    "# <a id='toc2_'></a>[Définition du modèle d‘entraînement](#toc0_)\n",
    "\n",
    "## <a id='toc2_1_'></a>[Définition des chargeurs de données](#toc0_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "67ffab47",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "01. torchvision.transforms.Compose(transforms)\n",
    "    - Composes several transforms together.\n",
    "\n",
    "02. torchvision.transforms.Normalize(mean, std, inplace=False)\n",
    "    - Normalize a tensor image with mean and standard deviation.\n",
    "    - output[channel] = (input[channel] - mean[channel]) / std[channel]\n",
    "\"\"\"\n",
    "\n",
    "mnist_train = torchvision.datasets.MNIST(\n",
    "    \"/tmp/files/\",\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=torchvision.transforms.Compose(\n",
    "        [\n",
    "            torchvision.transforms.ToTensor(),\n",
    "            # Les valeurs ` 0.1307` et `0.3081` utilisées pour la transformation `Normalize()`\n",
    "            # ci-dessous sont la moyenne globale et l’écart-type de l’ensemble de données MNIST.\n",
    "            torchvision.transforms.Normalize((0.1307,), (0.3081,)),\n",
    "        ]\n",
    "    ),\n",
    ")\n",
    "\n",
    "mnist_test = torchvision.datasets.MNIST(\n",
    "    \"/tmp/files/\",\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=torchvision.transforms.Compose(\n",
    "        [\n",
    "            torchvision.transforms.ToTensor(),\n",
    "            torchvision.transforms.Normalize((0.1307,), (0.3081,)),\n",
    "        ]\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60eaf1c0",
   "metadata": {},
   "source": [
    "## <a id='toc2_2_'></a>[Définition du modèle de réseau CNN](#toc0_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cba3007c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "03. torch.nn.Conv2d(in_channels, out_channels, kernel_size, stride=1, padding=0, dilation=1,\n",
    "    groups=1, bias=True, padding_mode='zeros', device=None, dtype=None)\n",
    "    - Applies a 2D convolution over an input signal composed of several input planes.\n",
    "\n",
    "04. torch.nn.Dropout2d(p=0.5, inplace=False)\n",
    "    - Randomly zero out entire channels.\n",
    "    - Each channel will be zeroed out independently on every forward call with probability p using\n",
    "    samples from a Bernoulli distribution.\n",
    "\n",
    "05. torch.nn.functional.max_pool2d(input, kernel_size, stride=None, padding=0,\n",
    "    dilation=1, ceil_mode=False, return_indices=False)\n",
    "    - Applies a 2D max pooling over an input signal composed of several input planes.\n",
    "\n",
    "06. torch.nn.functional.dropout(input, p=0.5, training=True, inplace=False)\n",
    "    - During training, randomly zeroes some elements of the input tensor with probability p.\n",
    "    - Uses samples from a Bernoulli distribution.\n",
    "\n",
    "07. torch.nn.functional.log_softmax(input, dim=None, _stacklevel=3, dtype=None)\n",
    "    - Apply a softmax followed by a logarithm.\n",
    "    - While mathematically equivalent to log(softmax(x)), doing these two operations separately is\n",
    "    slower and numerically unstable. This function uses an alternative formulation to compute the\n",
    "    output and gradient correctly.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        # La première couche convolutionnelle : le nombre de canaux d’entrée est de 1, c’est-à-dire\n",
    "        # une image en niveaux de gris, le nombre de canaux de sortie est de 10, la taille du filtre\n",
    "        # convolutif est de 5x5, le stride est de 1 et le padding est de 0.\n",
    "\n",
    "        # Par conséquent, après que l'image d'entrée (1x28x28) a été convoluée, la taille de la\n",
    "        # carte de caractéristiques de sortie est de 10x24x24.\n",
    "        self.conv1 = nn.Conv2d(1, 10, kernel_size=5)\n",
    "        # La deuxième couche convolutionnelle : le nombre de canaux d’entrée est de 10, le nombre de\n",
    "        # canaux de sortie est de 20, la taille du filtre convolutif est de 5x5, le stride est de 1\n",
    "        # et le padding est de 0.\n",
    "\n",
    "        # Par conséquent, après que l'image d'entrée (10x12x12) a été convoluée, la taille de la\n",
    "        # carte de caractéristiques de sortie est de 20x8x8.\n",
    "        self.conv2 = nn.Conv2d(10, 20, kernel_size=5)\n",
    "        # Une couche d'abandon.\n",
    "        self.conv2_drop = nn.Dropout2d()\n",
    "        # La première couche de fully connected : le nombre de canaux d’entrée est de 20, chaque\n",
    "        # canal a une taille de 4x4, soit un total de 20x4x4 = 320 nœuds, tandis que la sortie est\n",
    "        # fixée à 50 nœuds.\n",
    "        self.fc1 = nn.Linear(320, 50)\n",
    "        # La deuxième couche de fully connected : l'entrée a 50 nœuds et la sortie a 10 nœuds\n",
    "        # (correspondant à 10 catégories).\n",
    "        self.fc2 = nn.Linear(50, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # La première couche convolutive est suivie d'une couche de pooling de type max pooling avec\n",
    "        # un filtre convolutif de taille de 2x2 et un stride égal à la longueur du filtre.\n",
    "\n",
    "        # La taille d'entrée est de 10x24x24, et après pooling, la taille de sortie est de 10x12x12.\n",
    "        x = F.relu(F.max_pool2d(self.conv1(x), 2))\n",
    "        # La deuxième couche convolutive est suivie d'une couche d'abandon.\n",
    "\n",
    "        # Après la couche d'abandon, suit une autre couche de max-pooling, qui possède un filtre\n",
    "        # convolutif de taille de 2x2 et un stride aussi égal à la longueur du filtre.\n",
    "\n",
    "        # La taille d'entrée est de 20x8x8, et après pooling, la taille de sortie est de 20x4x4.\n",
    "        x = F.relu(F.max_pool2d(self.conv2_drop(self.conv2(x)), 2))\n",
    "        # La carte de caractéristiques multidimensionnelle est transformée en un vecteur\n",
    "        # unidimensionnel, d'une taille de 20x4x4 = 320.\n",
    "        x = x.view(-1, 320)\n",
    "        # La première couche de fully connected, activée par la fonction d'activation ReLU.\n",
    "        x = F.relu(self.fc1(x))\n",
    "        # Pendant l'entraînement du modèle, certains nœuds de la sortie de la première couche de\n",
    "        # fully connected sont mis à zéro de manière aléatoire avec une probabilité p.\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        # La deuxième couche de fully connected sert également de couche de sortie.\n",
    "        x = self.fc2(x)\n",
    "        # Les probabilités logarithmiques de tous les nœuds de la couche de sortie sont calculées en\n",
    "        # appliquant une fonction softmax suivie d'un logarithme.\n",
    "        return F.log_softmax(x, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "01738796",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1           [-1, 10, 24, 24]             260\n",
      "            Conv2d-2             [-1, 20, 8, 8]           5,020\n",
      "         Dropout2d-3             [-1, 20, 8, 8]               0\n",
      "            Linear-4                   [-1, 50]          16,050\n",
      "            Linear-5                   [-1, 10]             510\n",
      "================================================================\n",
      "Total params: 21,840\n",
      "Trainable params: 21,840\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.00\n",
      "Forward/backward pass size (MB): 0.06\n",
      "Params size (MB): 0.08\n",
      "Estimated Total Size (MB): 0.15\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "model = Net()\n",
    "summary(model, next(iter(mnist_train))[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7933b052",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[35m  conv1.weight .............................................. 250  \u001b[0m\n",
      "\u001b[35m  conv1.bias ................................................. 10  \u001b[0m\n",
      "\u001b[35m  conv2.weight ............................................. 5000  \u001b[0m\n",
      "\u001b[35m  conv2.bias ................................................. 20  \u001b[0m\n",
      "\u001b[35m  fc1.weight .............................................. 16000  \u001b[0m\n",
      "\u001b[35m  fc1.bias ................................................... 50  \u001b[0m\n",
      "\u001b[35m  fc2.weight ................................................ 500  \u001b[0m\n",
      "\u001b[35m  fc2.bias ................................................... 10  \u001b[0m\n",
      "\u001b[35m _________________________________________________________________ \u001b[0m\n",
      "\u001b[35m  total parameters ........................................ 21840  \u001b[0m\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def count_parameters(model):\n",
    "    length = 67\n",
    "    names = [n for (n, p) in model.named_parameters() if p.requires_grad]\n",
    "    name = \"total parameters\"\n",
    "    names.append(name)\n",
    "    max_length = max(map(len, names))\n",
    "    formatted_names = [f\"{f'  {n} ':.<{max_length + 3}}\" for n in names]\n",
    "    params = [p.numel() for p in model.parameters() if p.requires_grad]\n",
    "    params.append(sum(params))\n",
    "    formatted_params = [f\"{f' {p}  ':.>{length - max_length - 3}}\" for p in params]\n",
    "\n",
    "    for n, p in zip(formatted_names[:-1], formatted_params[:-1]):\n",
    "        cprint((n + p), \"magenta\")\n",
    "    cprint(\" \" + \"_\" * (length - 2) + \" \", \"magenta\")\n",
    "    cprint(\n",
    "        (formatted_names[-1] + formatted_params[-1]),\n",
    "        \"magenta\",\n",
    "        end=\"\\n\\n\",\n",
    "    )\n",
    "\n",
    "    return names, params\n",
    "\n",
    "\n",
    "names, params = count_parameters(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3675674",
   "metadata": {},
   "source": [
    "## <a id='toc2_3_'></a>[Définition de la fonction d'inférence utilisée dans le test](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "13b360d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "08. torch.nn.functional.nll_loss(input, target, weight=None, size_average=None, ignore_index=-100,\n",
    "    reduce=None, reduction='mean')\n",
    "    - Compute the negative log likelihood loss.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def inference(network, test_loader):\n",
    "    # Mettre le module en mode évaluation.\n",
    "    network.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            output = network(data)\n",
    "            # L'entropie est une mesure de l'incertitude, c'est-à-dire que, si un résultat est\n",
    "            # ertain, l'entropie est faible.\n",
    "\n",
    "            # La perte d'entropie croisée, ou perte logarithmique mesure les performances d'un\n",
    "            # modèle de classification dont le résultat est une valeur de probabilité comprise entre\n",
    "            # 0 et 1.\n",
    "\n",
    "            # La perte d'entropie croisée augmente à mesure que la probabilité prédite s'écarte de\n",
    "            # l'étiquette réelle.\n",
    "\n",
    "            # L’entropie croisée catégorielle sert au classement en plusieurs classes.\n",
    "\n",
    "            # Le log-vraisemblance négatif est également connu sous le nom d'entropie croisée\n",
    "            # catégorielle, car il s'agit en fait de deux interprétations différentes de la même\n",
    "            # formule.\n",
    "\n",
    "            # test_loss += F.cross_entropy(output, target, reduction=\"sum\").item()\n",
    "            test_loss += F.nll_loss(output, target, reduction=\"sum\").item()\n",
    "\n",
    "            # Si `keepdim` est `True`, le tenseur de sortie est de la même taille que celui \n",
    "            # d'entrée, sauf dans la (les) dimension(s) `dim` où il est de dimension 1.\n",
    "            pred = output.data.max(dim=1, keepdim=True)[1]\n",
    "            # Calcul de l'égalité par éléments.\n",
    "            correct += pred.eq(target.data.view_as(pred)).sum()\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    cprint(\n",
    "        \"Test set: Avg. loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\".format(\n",
    "            test_loss,\n",
    "            correct,\n",
    "            len(test_loader.dataset),\n",
    "            100.0 * correct / len(test_loader.dataset),\n",
    "        ),\n",
    "        \"magenta\",\n",
    "        attrs=[\"underline\"],\n",
    "        end=\"\\n\\n\",\n",
    "    )\n",
    "    accuracy = float(correct / len(test_loader.dataset))\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7f8c156a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[4m\u001b[35mTest set: Avg. loss: 2.3036, Accuracy: 514/10000 (5%)\u001b[0m\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.05139999836683273"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_loader = DataLoader(mnist_test, batch_size=500, shuffle=False)\n",
    "\n",
    "inference(model, test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bc71b0f",
   "metadata": {},
   "source": [
    "# <a id='toc3_'></a>[Définition des règles de l'apprentissage fédéré](#toc0_)\n",
    "\n",
    "## <a id='toc3_1_'></a>[Méthode de calcul de la moyenne des poids d'apprentissage fédéré](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "59d93477",
   "metadata": {},
   "outputs": [],
   "source": [
    "def FedAvg(models, weights=None):\n",
    "    new_model = models[0]\n",
    "    state_dicts = [model.state_dict() for model in models]\n",
    "    state_dict = new_model.state_dict()\n",
    "    for key in models[1].state_dict():\n",
    "        state_dict[key] = torch.from_numpy(\n",
    "            np.average(\n",
    "                [state[key].numpy() for state in state_dicts], axis=0, weights=weights\n",
    "            )\n",
    "        )\n",
    "    new_model.load_state_dict(state_dict)\n",
    "    return new_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "670a3019",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keys = [k for k in model.state_dict().keys()]\n",
    "names = [n for (n, p) in model.named_parameters() if p.requires_grad]\n",
    "keys == names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a3d20ab8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aggregator step \"start\" registered\n",
      "Collaborator step \"aggregated_model_validation\" registered\n",
      "Collaborator step \"train\" registered\n",
      "Collaborator step \"local_model_validation\" registered\n",
      "Aggregator step \"join\" registered\n",
      "Aggregator step \"end\" registered\n"
     ]
    }
   ],
   "source": [
    "# n_epochs = 3\n",
    "# batch_size_train = 64\n",
    "# batch_size_test = 1000\n",
    "learning_rate = 0.01\n",
    "log_interval = 10\n",
    "momentum = 0.5\n",
    "\n",
    "\n",
    "# random_seed = 1\n",
    "# torch.backends.cudnn.enabled = False\n",
    "# torch.manual_seed(random_seed)\n",
    "\n",
    "\n",
    "class FederatedFlow(FLSpec):\n",
    "\n",
    "    def __init__(self, model=None, optimizer=None, rounds=3, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        # Importe un modèle personnalisé et ajoute le bon algorithme d’optimisation pour ce dernier.\n",
    "        if model is not None:\n",
    "            self.model = model\n",
    "            self.optimizer = optimizer\n",
    "        # Chargez le modèle `Net()` et configurez l'optimiseur pour qu'il s'applique uniquement à ce\n",
    "        # modèle.\n",
    "        else:\n",
    "            self.model = Net()\n",
    "            self.optimizer = optim.SGD(\n",
    "                self.model.parameters(), lr=learning_rate, momentum=momentum\n",
    "            )\n",
    "        self.rounds = rounds\n",
    "\n",
    "    # Un agrégateur est le nœud central de l'apprentissage fédéré.\n",
    "\n",
    "    # L'agrégateur commence par un modèle et un optimiseur transmis de manière facultative.\n",
    "\n",
    "    # L'agrégateur commence le flux avec la tâche de `start`, où la liste des collaborateurs est\n",
    "    # extraite de l'exécution (`self.collaborators = self.runtime.collaborators`) et est ensuite\n",
    "    # utilisée comme liste de participants pour exécuterla tâche énumérée dans `self.next`,\n",
    "    # `aggregated_model_validation`.\n",
    "    @aggregator\n",
    "    def start(self):\n",
    "        cprint(f\"Performing initialization for model\", \"black\", attrs=[\"bold\"])\n",
    "        self.collaborators = self.runtime.collaborators\n",
    "        self.private = 10\n",
    "        self.current_round = 0\n",
    "        self.next(\n",
    "            self.aggregated_model_validation,\n",
    "            foreach=\"collaborators\",\n",
    "            exclude=[\"private\"],\n",
    "        )\n",
    "\n",
    "    # Le modèle, l'optimiseur et tout ce qui n'est pas explicitement exclu de la fonction suivante\n",
    "    # seront transmis de la fonction de `start` de l'agrégateur à la tâche\n",
    "    # `aggregated_model_validation` du collaborateur.\n",
    "\n",
    "    # L’endroit où les tâches sont exécutées est déterminé par le décorateur de placement qui\n",
    "    # précède chaque définition de tâche (`@aggregator` ou `@collaborator`).\n",
    "\n",
    "    # Une fois que chaque collaborateur (défini dans l’exécution) a terminé la tâche\n",
    "    # `aggregated_model_validation`, il transmet son état actuel à la tâche `train`, de `train` à\n",
    "    # `local_model_validation`, et enfin à `join` à l'agrégateur.\n",
    "\n",
    "    # C'est au niveau de `join` qu'une moyenne des poids des modèles est calculée et que le tour\n",
    "    # suivant peut commencer.\n",
    "    @collaborator\n",
    "    def aggregated_model_validation(self):\n",
    "        cprint(\n",
    "            f\"Performing aggregated model validation for collaborator {self.input}\",\n",
    "            \"red\",\n",
    "            attrs=[\"bold\"],\n",
    "        )\n",
    "        self.agg_validation_score = inference(self.model, self.test_loader)\n",
    "        cprint(\n",
    "            f\"{self.input} value of {self.agg_validation_score}\",\n",
    "            \"red\",\n",
    "            attrs=[\"underline\"],\n",
    "        )\n",
    "        self.next(self.train)\n",
    "\n",
    "    @collaborator\n",
    "    def train(self):\n",
    "        self.model.train()\n",
    "        self.optimizer = optim.SGD(\n",
    "            self.model.parameters(), lr=learning_rate, momentum=momentum\n",
    "        )\n",
    "        train_losses = []\n",
    "        for batch_idx, (data, target) in enumerate(self.train_loader):\n",
    "            self.optimizer.zero_grad()\n",
    "            output = self.model(data)\n",
    "            loss = F.nll_loss(output, target)\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "            if batch_idx % log_interval == 0:\n",
    "                cprint(\n",
    "                    \"Train Epoch: 1 [{}/{} ({:.0f}%)]\\tLoss: {:.6f}\".format(\n",
    "                        batch_idx * len(data),\n",
    "                        len(self.train_loader.dataset),\n",
    "                        100.0 * batch_idx / len(self.train_loader),\n",
    "                        loss.item(),\n",
    "                    ),\n",
    "                    \"yellow\",\n",
    "                )\n",
    "                self.loss = loss.item()\n",
    "                torch.save(self.model.state_dict(), \"model.pth\")\n",
    "                torch.save(self.optimizer.state_dict(), \"optimizer.pth\")\n",
    "        self.training_completed = True\n",
    "        self.next(self.local_model_validation)\n",
    "\n",
    "    @collaborator\n",
    "    def local_model_validation(self):\n",
    "        self.local_validation_score = inference(self.model, self.test_loader)\n",
    "        cprint(\n",
    "            f\"Doing local model validation for collaborator {self.input}: \\\n",
    "                {self.local_validation_score}\",\n",
    "            \"white\",\n",
    "        )\n",
    "        self.next(self.join, exclude=[\"training_completed\"])\n",
    "\n",
    "    @aggregator\n",
    "    def join(self, inputs):\n",
    "        self.average_loss = sum(input.loss for input in inputs) / len(inputs)\n",
    "        self.aggregated_model_accuracy = sum(\n",
    "            input.agg_validation_score for input in inputs\n",
    "        ) / len(inputs)\n",
    "        self.local_model_accuracy = sum(\n",
    "            input.local_validation_score for input in inputs\n",
    "        ) / len(inputs)\n",
    "        cprint(\n",
    "            f\"Average aggregated model validation values = \\\n",
    "                {self.aggregated_model_accuracy}\",\n",
    "            \"green\",\n",
    "        )\n",
    "        cprint(f\"Average training loss = {self.average_loss}\", \"green\")\n",
    "        cprint(\n",
    "            f\"Average local model validation values = \\\n",
    "            {self.local_model_accuracy}\",\n",
    "            \"green\",\n",
    "        )\n",
    "        self.model = FedAvg([input.model for input in inputs])\n",
    "        self.optimizer = [input.optimizer for input in inputs][0]\n",
    "        self.current_round += 1\n",
    "        if self.current_round < self.rounds:\n",
    "            self.next(\n",
    "                self.aggregated_model_validation,\n",
    "                foreach=\"collaborators\",\n",
    "                exclude=[\"private\"],\n",
    "            )\n",
    "        else:\n",
    "            self.next(self.end)\n",
    "\n",
    "    @aggregator\n",
    "    def end(self):\n",
    "        cprint(f\"This is the end of the flow\", \"black\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b96e3045",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Local runtime collaborators = ['Portland', 'Seattle', 'Chandler', 'Bangalore']\n"
     ]
    }
   ],
   "source": [
    "batch_size_train = 64\n",
    "\n",
    "# Setup participants\n",
    "aggregator = Aggregator()\n",
    "aggregator.private_attributes = {}\n",
    "\n",
    "# Setup collaborators with private attributes\n",
    "collaborator_names = ['Portland', 'Seattle', 'Chandler','Bangalore']\n",
    "collaborators = [Collaborator(name=name) for name in collaborator_names]\n",
    "for idx, collaborator in enumerate(collaborators):\n",
    "    local_train = deepcopy(mnist_train)\n",
    "    local_test = deepcopy(mnist_test)\n",
    "    local_train.data = mnist_train.data[idx::len(collaborators)]\n",
    "    local_train.targets = mnist_train.targets[idx::len(collaborators)]\n",
    "    local_test.data = mnist_test.data[idx::len(collaborators)]\n",
    "    local_test.targets = mnist_test.targets[idx::len(collaborators)]\n",
    "    collaborator.private_attributes = {\n",
    "            'train_loader': torch.utils.data.DataLoader(local_train,batch_size=batch_size_train, shuffle=True),\n",
    "            'test_loader': torch.utils.data.DataLoader(local_test,batch_size=batch_size_train, shuffle=True)\n",
    "    }\n",
    "\n",
    "local_runtime = LocalRuntime(aggregator=aggregator, collaborators=collaborators, backend='single_process')\n",
    "print(f'Local runtime collaborators = {local_runtime.collaborators}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5494deda",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "if os.environ.get(\"USERNAME\") is None:\n",
    "    os.environ[\"USERNAME\"] = \"Hao\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8018e1e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hao\n"
     ]
    }
   ],
   "source": [
    "import getpass\n",
    "print(getpass.getuser())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "edbaff16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created flow FederatedFlow\n",
      "\n",
      "Calling start\n",
      "\u001b[94m\u001b[1m\u001b[30mPerforming initialization for model\u001b[0m\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94mSaving data artifacts for start\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94mSaved data artifacts for start\u001b[0m\u001b[94m\n",
      "\u001b[0m\n",
      "Calling aggregated_model_validation\n",
      "\u001b[94m\u001b[1m\u001b[31mPerforming aggregated model validation for collaborator Portland\u001b[0m\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94m\u001b[4m\u001b[35mTest set: Avg. loss: 2.3172, Accuracy: 246/2500 (10%)\u001b[0m\u001b[0m\u001b[94m\n",
      "\n",
      "\u001b[0m\u001b[94m\u001b[4m\u001b[31mPortland value of 0.09839999675750732\u001b[0m\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94mSaving data artifacts for aggregated_model_validation\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94mSaved data artifacts for aggregated_model_validation\u001b[0m\u001b[94m\n",
      "\u001b[0m\n",
      "Calling train\n",
      "\u001b[94m\u001b[33mTrain Epoch: 1 [0/15000 (0%)]\tLoss: 2.401142\u001b[0m\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94m\u001b[33mTrain Epoch: 1 [640/15000 (4%)]\tLoss: 2.311244\u001b[0m\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94m\u001b[33mTrain Epoch: 1 [1280/15000 (9%)]\tLoss: 2.282785\u001b[0m\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94m\u001b[33mTrain Epoch: 1 [1920/15000 (13%)]\tLoss: 2.275842\u001b[0m\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94m\u001b[33mTrain Epoch: 1 [2560/15000 (17%)]\tLoss: 2.281283\u001b[0m\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94m\u001b[33mTrain Epoch: 1 [3200/15000 (21%)]\tLoss: 2.237387\u001b[0m\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94m\u001b[33mTrain Epoch: 1 [3840/15000 (26%)]\tLoss: 2.226326\u001b[0m\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94m\u001b[33mTrain Epoch: 1 [4480/15000 (30%)]\tLoss: 2.255422\u001b[0m\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94m\u001b[33mTrain Epoch: 1 [5120/15000 (34%)]\tLoss: 2.217204\u001b[0m\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94m\u001b[33mTrain Epoch: 1 [5760/15000 (38%)]\tLoss: 2.187421\u001b[0m\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94m\u001b[33mTrain Epoch: 1 [6400/15000 (43%)]\tLoss: 2.126931\u001b[0m\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94m\u001b[33mTrain Epoch: 1 [7040/15000 (47%)]\tLoss: 1.963203\u001b[0m\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94m\u001b[33mTrain Epoch: 1 [7680/15000 (51%)]\tLoss: 2.012138\u001b[0m\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94m\u001b[33mTrain Epoch: 1 [8320/15000 (55%)]\tLoss: 1.970195\u001b[0m\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94m\u001b[33mTrain Epoch: 1 [8960/15000 (60%)]\tLoss: 1.821576\u001b[0m\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94m\u001b[33mTrain Epoch: 1 [9600/15000 (64%)]\tLoss: 1.636566\u001b[0m\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94m\u001b[33mTrain Epoch: 1 [10240/15000 (68%)]\tLoss: 1.445055\u001b[0m\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94m\u001b[33mTrain Epoch: 1 [10880/15000 (72%)]\tLoss: 1.535492\u001b[0m\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94m\u001b[33mTrain Epoch: 1 [11520/15000 (77%)]\tLoss: 1.433664\u001b[0m\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94m\u001b[33mTrain Epoch: 1 [12160/15000 (81%)]\tLoss: 1.402928\u001b[0m\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94m\u001b[33mTrain Epoch: 1 [12800/15000 (85%)]\tLoss: 1.463498\u001b[0m\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94m\u001b[33mTrain Epoch: 1 [13440/15000 (89%)]\tLoss: 1.249097\u001b[0m\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94m\u001b[33mTrain Epoch: 1 [14080/15000 (94%)]\tLoss: 1.263335\u001b[0m\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94m\u001b[33mTrain Epoch: 1 [14720/15000 (98%)]\tLoss: 1.525760\u001b[0m\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94mSaving data artifacts for train\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94mSaved data artifacts for train\u001b[0m\u001b[94m\n",
      "\u001b[0m\n",
      "Calling local_model_validation\n",
      "\u001b[94m\u001b[4m\u001b[35mTest set: Avg. loss: 0.7749, Accuracy: 1957/2500 (78%)\u001b[0m\u001b[0m\u001b[94m\n",
      "\n",
      "\u001b[0m\u001b[94m\u001b[97mDoing local model validation for collaborator Portland:                 0.782800018787384\u001b[0m\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94mSaving data artifacts for local_model_validation\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94mSaved data artifacts for local_model_validation\u001b[0m\u001b[94m\n",
      "\u001b[0mShould transfer from local_model_validation to join\n",
      "\n",
      "Calling aggregated_model_validation\n",
      "\u001b[94m\u001b[1m\u001b[31mPerforming aggregated model validation for collaborator Seattle\u001b[0m\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94m\u001b[4m\u001b[35mTest set: Avg. loss: 2.3152, Accuracy: 242/2500 (10%)\u001b[0m\u001b[0m\u001b[94m\n",
      "\n",
      "\u001b[0m\u001b[94m\u001b[4m\u001b[31mSeattle value of 0.09679999947547913\u001b[0m\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94mSaving data artifacts for aggregated_model_validation\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94mSaved data artifacts for aggregated_model_validation\u001b[0m\u001b[94m\n",
      "\u001b[0m\n",
      "Calling train\n",
      "\u001b[94m\u001b[33mTrain Epoch: 1 [0/15000 (0%)]\tLoss: 2.345585\u001b[0m\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94m\u001b[33mTrain Epoch: 1 [640/15000 (4%)]\tLoss: 2.304630\u001b[0m\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94m\u001b[33mTrain Epoch: 1 [1280/15000 (9%)]\tLoss: 2.301589\u001b[0m\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94m\u001b[33mTrain Epoch: 1 [1920/15000 (13%)]\tLoss: 2.235111\u001b[0m\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94m\u001b[33mTrain Epoch: 1 [2560/15000 (17%)]\tLoss: 2.255780\u001b[0m\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94m\u001b[33mTrain Epoch: 1 [3200/15000 (21%)]\tLoss: 2.259033\u001b[0m\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94m\u001b[33mTrain Epoch: 1 [3840/15000 (26%)]\tLoss: 2.230223\u001b[0m\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94m\u001b[33mTrain Epoch: 1 [4480/15000 (30%)]\tLoss: 2.182924\u001b[0m\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94m\u001b[33mTrain Epoch: 1 [5120/15000 (34%)]\tLoss: 2.154750\u001b[0m\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94m\u001b[33mTrain Epoch: 1 [5760/15000 (38%)]\tLoss: 2.104423\u001b[0m\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94m\u001b[33mTrain Epoch: 1 [6400/15000 (43%)]\tLoss: 1.836762\u001b[0m\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94m\u001b[33mTrain Epoch: 1 [7040/15000 (47%)]\tLoss: 1.805441\u001b[0m\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94m\u001b[33mTrain Epoch: 1 [7680/15000 (51%)]\tLoss: 1.771757\u001b[0m\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94m\u001b[33mTrain Epoch: 1 [8320/15000 (55%)]\tLoss: 1.560795\u001b[0m\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94m\u001b[33mTrain Epoch: 1 [8960/15000 (60%)]\tLoss: 1.461920\u001b[0m\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94m\u001b[33mTrain Epoch: 1 [9600/15000 (64%)]\tLoss: 1.308358\u001b[0m\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94m\u001b[33mTrain Epoch: 1 [10240/15000 (68%)]\tLoss: 1.522552\u001b[0m\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94m\u001b[33mTrain Epoch: 1 [10880/15000 (72%)]\tLoss: 1.262072\u001b[0m\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94m\u001b[33mTrain Epoch: 1 [11520/15000 (77%)]\tLoss: 1.060540\u001b[0m\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94m\u001b[33mTrain Epoch: 1 [12160/15000 (81%)]\tLoss: 1.129105\u001b[0m\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94m\u001b[33mTrain Epoch: 1 [12800/15000 (85%)]\tLoss: 1.063725\u001b[0m\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94m\u001b[33mTrain Epoch: 1 [13440/15000 (89%)]\tLoss: 1.165532\u001b[0m\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94m\u001b[33mTrain Epoch: 1 [14080/15000 (94%)]\tLoss: 0.883201\u001b[0m\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94m\u001b[33mTrain Epoch: 1 [14720/15000 (98%)]\tLoss: 1.100240\u001b[0m\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94mSaving data artifacts for train\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94mSaved data artifacts for train\u001b[0m\u001b[94m\n",
      "\u001b[0m\n",
      "Calling local_model_validation\n",
      "\u001b[94m\u001b[4m\u001b[35mTest set: Avg. loss: 0.7419, Accuracy: 2054/2500 (82%)\u001b[0m\u001b[0m\u001b[94m\n",
      "\n",
      "\u001b[0m\u001b[94m\u001b[97mDoing local model validation for collaborator Seattle:                 0.8216000199317932\u001b[0m\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94mSaving data artifacts for local_model_validation\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94mSaved data artifacts for local_model_validation\u001b[0m\u001b[94m\n",
      "\u001b[0mShould transfer from local_model_validation to join\n",
      "\n",
      "Calling aggregated_model_validation\n",
      "\u001b[94m\u001b[1m\u001b[31mPerforming aggregated model validation for collaborator Chandler\u001b[0m\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94m\u001b[4m\u001b[35mTest set: Avg. loss: 2.3171, Accuracy: 211/2500 (8%)\u001b[0m\u001b[0m\u001b[94m\n",
      "\n",
      "\u001b[0m\u001b[94m\u001b[4m\u001b[31mChandler value of 0.0843999981880188\u001b[0m\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94mSaving data artifacts for aggregated_model_validation\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94mSaved data artifacts for aggregated_model_validation\u001b[0m\u001b[94m\n",
      "\u001b[0m\n",
      "Calling train\n",
      "\u001b[94m\u001b[33mTrain Epoch: 1 [0/15000 (0%)]\tLoss: 2.306736\u001b[0m\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94m\u001b[33mTrain Epoch: 1 [640/15000 (4%)]\tLoss: 2.312124\u001b[0m\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94m\u001b[33mTrain Epoch: 1 [1280/15000 (9%)]\tLoss: 2.313751\u001b[0m\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94m\u001b[33mTrain Epoch: 1 [1920/15000 (13%)]\tLoss: 2.304244\u001b[0m\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94m\u001b[33mTrain Epoch: 1 [2560/15000 (17%)]\tLoss: 2.244131\u001b[0m\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94m\u001b[33mTrain Epoch: 1 [3200/15000 (21%)]\tLoss: 2.293708\u001b[0m\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94m\u001b[33mTrain Epoch: 1 [3840/15000 (26%)]\tLoss: 2.260859\u001b[0m\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94m\u001b[33mTrain Epoch: 1 [4480/15000 (30%)]\tLoss: 2.214480\u001b[0m\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94m\u001b[33mTrain Epoch: 1 [5120/15000 (34%)]\tLoss: 2.154546\u001b[0m\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94m\u001b[33mTrain Epoch: 1 [5760/15000 (38%)]\tLoss: 2.137000\u001b[0m\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94m\u001b[33mTrain Epoch: 1 [6400/15000 (43%)]\tLoss: 2.188874\u001b[0m\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94m\u001b[33mTrain Epoch: 1 [7040/15000 (47%)]\tLoss: 2.041807\u001b[0m\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94m\u001b[33mTrain Epoch: 1 [7680/15000 (51%)]\tLoss: 1.922109\u001b[0m\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94m\u001b[33mTrain Epoch: 1 [8320/15000 (55%)]\tLoss: 1.851048\u001b[0m\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94m\u001b[33mTrain Epoch: 1 [8960/15000 (60%)]\tLoss: 1.792250\u001b[0m\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94m\u001b[33mTrain Epoch: 1 [9600/15000 (64%)]\tLoss: 1.551665\u001b[0m\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94m\u001b[33mTrain Epoch: 1 [10240/15000 (68%)]\tLoss: 1.742627\u001b[0m\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94m\u001b[33mTrain Epoch: 1 [10880/15000 (72%)]\tLoss: 1.542351\u001b[0m\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94m\u001b[33mTrain Epoch: 1 [11520/15000 (77%)]\tLoss: 1.610503\u001b[0m\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94m\u001b[33mTrain Epoch: 1 [12160/15000 (81%)]\tLoss: 1.442136\u001b[0m\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94m\u001b[33mTrain Epoch: 1 [12800/15000 (85%)]\tLoss: 1.578478\u001b[0m\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94m\u001b[33mTrain Epoch: 1 [13440/15000 (89%)]\tLoss: 1.197223\u001b[0m\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94m\u001b[33mTrain Epoch: 1 [14080/15000 (94%)]\tLoss: 1.282654\u001b[0m\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94m\u001b[33mTrain Epoch: 1 [14720/15000 (98%)]\tLoss: 0.999285\u001b[0m\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94mSaving data artifacts for train\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94mSaved data artifacts for train\u001b[0m\u001b[94m\n",
      "\u001b[0m\n",
      "Calling local_model_validation\n",
      "\u001b[94m\u001b[4m\u001b[35mTest set: Avg. loss: 0.7814, Accuracy: 1987/2500 (79%)\u001b[0m\u001b[0m\u001b[94m\n",
      "\n",
      "\u001b[0m\u001b[94m\u001b[97mDoing local model validation for collaborator Chandler:                 0.7947999835014343\u001b[0m\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94mSaving data artifacts for local_model_validation\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94mSaved data artifacts for local_model_validation\u001b[0m\u001b[94m\n",
      "\u001b[0mShould transfer from local_model_validation to join\n",
      "\n",
      "Calling aggregated_model_validation\n",
      "\u001b[94m\u001b[1m\u001b[31mPerforming aggregated model validation for collaborator Bangalore\u001b[0m\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94m\u001b[4m\u001b[35mTest set: Avg. loss: 2.3133, Accuracy: 259/2500 (10%)\u001b[0m\u001b[0m\u001b[94m\n",
      "\n",
      "\u001b[0m\u001b[94m\u001b[4m\u001b[31mBangalore value of 0.10360000282526016\u001b[0m\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94mSaving data artifacts for aggregated_model_validation\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94mSaved data artifacts for aggregated_model_validation\u001b[0m\u001b[94m\n",
      "\u001b[0m\n",
      "Calling train\n",
      "\u001b[94m\u001b[33mTrain Epoch: 1 [0/15000 (0%)]\tLoss: 2.364353\u001b[0m\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94m\u001b[33mTrain Epoch: 1 [640/15000 (4%)]\tLoss: 2.310035\u001b[0m\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94m\u001b[33mTrain Epoch: 1 [1280/15000 (9%)]\tLoss: 2.287412\u001b[0m\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94m\u001b[33mTrain Epoch: 1 [1920/15000 (13%)]\tLoss: 2.267777\u001b[0m\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94m\u001b[33mTrain Epoch: 1 [2560/15000 (17%)]\tLoss: 2.243787\u001b[0m\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94m\u001b[33mTrain Epoch: 1 [3200/15000 (21%)]\tLoss: 2.253870\u001b[0m\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94m\u001b[33mTrain Epoch: 1 [3840/15000 (26%)]\tLoss: 2.255312\u001b[0m\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94m\u001b[33mTrain Epoch: 1 [4480/15000 (30%)]\tLoss: 2.205601\u001b[0m\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94m\u001b[33mTrain Epoch: 1 [5120/15000 (34%)]\tLoss: 2.144409\u001b[0m\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94m\u001b[33mTrain Epoch: 1 [5760/15000 (38%)]\tLoss: 2.020834\u001b[0m\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94m\u001b[33mTrain Epoch: 1 [6400/15000 (43%)]\tLoss: 1.837576\u001b[0m\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94m\u001b[33mTrain Epoch: 1 [7040/15000 (47%)]\tLoss: 1.974945\u001b[0m\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94m\u001b[33mTrain Epoch: 1 [7680/15000 (51%)]\tLoss: 1.786360\u001b[0m\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94m\u001b[33mTrain Epoch: 1 [8320/15000 (55%)]\tLoss: 1.466990\u001b[0m\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94m\u001b[33mTrain Epoch: 1 [8960/15000 (60%)]\tLoss: 1.402942\u001b[0m\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94m\u001b[33mTrain Epoch: 1 [9600/15000 (64%)]\tLoss: 1.435252\u001b[0m\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94m\u001b[33mTrain Epoch: 1 [10240/15000 (68%)]\tLoss: 1.344001\u001b[0m\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94m\u001b[33mTrain Epoch: 1 [10880/15000 (72%)]\tLoss: 1.261289\u001b[0m\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94m\u001b[33mTrain Epoch: 1 [11520/15000 (77%)]\tLoss: 1.264109\u001b[0m\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94m\u001b[33mTrain Epoch: 1 [12160/15000 (81%)]\tLoss: 1.203656\u001b[0m\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94m\u001b[33mTrain Epoch: 1 [12800/15000 (85%)]\tLoss: 1.151849\u001b[0m\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94m\u001b[33mTrain Epoch: 1 [13440/15000 (89%)]\tLoss: 0.998630\u001b[0m\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94m\u001b[33mTrain Epoch: 1 [14080/15000 (94%)]\tLoss: 1.064448\u001b[0m\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94m\u001b[33mTrain Epoch: 1 [14720/15000 (98%)]\tLoss: 0.724097\u001b[0m\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94mSaving data artifacts for train\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94mSaved data artifacts for train\u001b[0m\u001b[94m\n",
      "\u001b[0m\n",
      "Calling local_model_validation\n",
      "\u001b[94m\u001b[4m\u001b[35mTest set: Avg. loss: 0.7015, Accuracy: 2057/2500 (82%)\u001b[0m\u001b[0m\u001b[94m\n",
      "\n",
      "\u001b[0m\u001b[94m\u001b[97mDoing local model validation for collaborator Bangalore:                 0.8227999806404114\u001b[0m\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94mSaving data artifacts for local_model_validation\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94mSaved data artifacts for local_model_validation\u001b[0m\u001b[94m\n",
      "\u001b[0mShould transfer from local_model_validation to join\n",
      "\n",
      "Calling join\n",
      "\u001b[94m\u001b[32mAverage aggregated model validation values =                 0.09579999931156635\u001b[0m\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94m\u001b[32mAverage training loss = 1.087345540523529\u001b[0m\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94m\u001b[32mAverage local model validation values =             0.8055000007152557\u001b[0m\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94mSaving data artifacts for join\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94mSaved data artifacts for join\u001b[0m\u001b[94m\n",
      "\u001b[0m\n",
      "Calling aggregated_model_validation\n",
      "\u001b[94m\u001b[1m\u001b[31mPerforming aggregated model validation for collaborator Portland\u001b[0m\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94m\u001b[4m\u001b[35mTest set: Avg. loss: 0.7486, Accuracy: 2087/2500 (83%)\u001b[0m\u001b[0m\u001b[94m\n",
      "\n",
      "\u001b[0m\u001b[94m\u001b[4m\u001b[31mPortland value of 0.8348000049591064\u001b[0m\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94mSaving data artifacts for aggregated_model_validation\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94mSaved data artifacts for aggregated_model_validation\u001b[0m\u001b[94m\n",
      "\u001b[0m\n",
      "Calling train\n",
      "\u001b[94m\u001b[33mTrain Epoch: 1 [0/15000 (0%)]\tLoss: 1.047214\u001b[0m\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94m\u001b[33mTrain Epoch: 1 [640/15000 (4%)]\tLoss: 1.146358\u001b[0m\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94m\u001b[33mTrain Epoch: 1 [1280/15000 (9%)]\tLoss: 1.264929\u001b[0m\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94m\u001b[33mTrain Epoch: 1 [1920/15000 (13%)]\tLoss: 0.893098\u001b[0m\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94m\u001b[33mTrain Epoch: 1 [2560/15000 (17%)]\tLoss: 0.995032\u001b[0m\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94m\u001b[33mTrain Epoch: 1 [3200/15000 (21%)]\tLoss: 0.981613\u001b[0m\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94m\u001b[33mTrain Epoch: 1 [3840/15000 (26%)]\tLoss: 0.949879\u001b[0m\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94m\u001b[33mTrain Epoch: 1 [4480/15000 (30%)]\tLoss: 1.040843\u001b[0m\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94m\u001b[33mTrain Epoch: 1 [5120/15000 (34%)]\tLoss: 0.962451\u001b[0m\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94m\u001b[33mTrain Epoch: 1 [5760/15000 (38%)]\tLoss: 0.907804\u001b[0m\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94m\u001b[33mTrain Epoch: 1 [6400/15000 (43%)]\tLoss: 0.840576\u001b[0m\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94m\u001b[33mTrain Epoch: 1 [7040/15000 (47%)]\tLoss: 0.650395\u001b[0m\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94m\u001b[33mTrain Epoch: 1 [7680/15000 (51%)]\tLoss: 0.835171\u001b[0m\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94m\u001b[33mTrain Epoch: 1 [8320/15000 (55%)]\tLoss: 0.877936\u001b[0m\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94m\u001b[33mTrain Epoch: 1 [8960/15000 (60%)]\tLoss: 0.853286\u001b[0m\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94m\u001b[33mTrain Epoch: 1 [9600/15000 (64%)]\tLoss: 0.746993\u001b[0m\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94m\u001b[33mTrain Epoch: 1 [10240/15000 (68%)]\tLoss: 0.654444\u001b[0m\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94m\u001b[33mTrain Epoch: 1 [10880/15000 (72%)]\tLoss: 1.028655\u001b[0m\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94m\u001b[33mTrain Epoch: 1 [11520/15000 (77%)]\tLoss: 0.960092\u001b[0m\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94m\u001b[33mTrain Epoch: 1 [12160/15000 (81%)]\tLoss: 0.903137\u001b[0m\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94m\u001b[33mTrain Epoch: 1 [12800/15000 (85%)]\tLoss: 0.844702\u001b[0m\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94m\u001b[33mTrain Epoch: 1 [13440/15000 (89%)]\tLoss: 0.765128\u001b[0m\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94m\u001b[33mTrain Epoch: 1 [14080/15000 (94%)]\tLoss: 0.575988\u001b[0m\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94m\u001b[33mTrain Epoch: 1 [14720/15000 (98%)]\tLoss: 0.617343\u001b[0m\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94mSaving data artifacts for train\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94mSaved data artifacts for train\u001b[0m\u001b[94m\n",
      "\u001b[0m\n",
      "Calling local_model_validation\n",
      "\u001b[94m\u001b[4m\u001b[35mTest set: Avg. loss: 0.3854, Accuracy: 2218/2500 (89%)\u001b[0m\u001b[0m\u001b[94m\n",
      "\n",
      "\u001b[0m\u001b[94m\u001b[97mDoing local model validation for collaborator Portland:                 0.8871999979019165\u001b[0m\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94mSaving data artifacts for local_model_validation\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94mSaved data artifacts for local_model_validation\u001b[0m\u001b[94m\n",
      "\u001b[0mShould transfer from local_model_validation to join\n",
      "\n",
      "Calling aggregated_model_validation\n",
      "\u001b[94m\u001b[1m\u001b[31mPerforming aggregated model validation for collaborator Seattle\u001b[0m\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94m\u001b[4m\u001b[35mTest set: Avg. loss: 0.7435, Accuracy: 2100/2500 (84%)\u001b[0m\u001b[0m\u001b[94m\n",
      "\n",
      "\u001b[0m\u001b[94m\u001b[4m\u001b[31mSeattle value of 0.8399999737739563\u001b[0m\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94mSaving data artifacts for aggregated_model_validation\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94mSaved data artifacts for aggregated_model_validation\u001b[0m\u001b[94m\n",
      "\u001b[0m\n",
      "Calling train\n",
      "\u001b[94m\u001b[33mTrain Epoch: 1 [0/15000 (0%)]\tLoss: 0.900641\u001b[0m\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94m\u001b[33mTrain Epoch: 1 [640/15000 (4%)]\tLoss: 0.963317\u001b[0m\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94m\u001b[33mTrain Epoch: 1 [1280/15000 (9%)]\tLoss: 0.818338\u001b[0m\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94m\u001b[33mTrain Epoch: 1 [1920/15000 (13%)]\tLoss: 1.053455\u001b[0m\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94m\u001b[33mTrain Epoch: 1 [2560/15000 (17%)]\tLoss: 0.920228\u001b[0m\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94m\u001b[33mTrain Epoch: 1 [3200/15000 (21%)]\tLoss: 0.917351\u001b[0m\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94m\u001b[33mTrain Epoch: 1 [3840/15000 (26%)]\tLoss: 0.829454\u001b[0m\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94m\u001b[33mTrain Epoch: 1 [4480/15000 (30%)]\tLoss: 0.660103\u001b[0m\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94m\u001b[33mTrain Epoch: 1 [5120/15000 (34%)]\tLoss: 0.756419\u001b[0m\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94m\u001b[33mTrain Epoch: 1 [5760/15000 (38%)]\tLoss: 0.964110\u001b[0m\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94m\u001b[33mTrain Epoch: 1 [6400/15000 (43%)]\tLoss: 0.795740\u001b[0m\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94m\u001b[33mTrain Epoch: 1 [7040/15000 (47%)]\tLoss: 0.714419\u001b[0m\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94m\u001b[33mTrain Epoch: 1 [7680/15000 (51%)]\tLoss: 0.868091\u001b[0m\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94m\u001b[33mTrain Epoch: 1 [8320/15000 (55%)]\tLoss: 0.538030\u001b[0m\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94m\u001b[33mTrain Epoch: 1 [8960/15000 (60%)]\tLoss: 0.750735\u001b[0m\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94m\u001b[33mTrain Epoch: 1 [9600/15000 (64%)]\tLoss: 0.530976\u001b[0m\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94m\u001b[33mTrain Epoch: 1 [10240/15000 (68%)]\tLoss: 0.618259\u001b[0m\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94m\u001b[33mTrain Epoch: 1 [10880/15000 (72%)]\tLoss: 0.702926\u001b[0m\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94m\u001b[33mTrain Epoch: 1 [11520/15000 (77%)]\tLoss: 0.545785\u001b[0m\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94m\u001b[33mTrain Epoch: 1 [12160/15000 (81%)]\tLoss: 0.469259\u001b[0m\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94m\u001b[33mTrain Epoch: 1 [12800/15000 (85%)]\tLoss: 0.744365\u001b[0m\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94m\u001b[33mTrain Epoch: 1 [13440/15000 (89%)]\tLoss: 0.501044\u001b[0m\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94m\u001b[33mTrain Epoch: 1 [14080/15000 (94%)]\tLoss: 0.611986\u001b[0m\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94m\u001b[33mTrain Epoch: 1 [14720/15000 (98%)]\tLoss: 0.540778\u001b[0m\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94mSaving data artifacts for train\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94mSaved data artifacts for train\u001b[0m\u001b[94m\n",
      "\u001b[0m\n",
      "Calling local_model_validation\n",
      "\u001b[94m\u001b[4m\u001b[35mTest set: Avg. loss: 0.4039, Accuracy: 2226/2500 (89%)\u001b[0m\u001b[0m\u001b[94m\n",
      "\n",
      "\u001b[0m\u001b[94m\u001b[97mDoing local model validation for collaborator Seattle:                 0.8903999924659729\u001b[0m\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94mSaving data artifacts for local_model_validation\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94mSaved data artifacts for local_model_validation\u001b[0m\u001b[94m\n",
      "\u001b[0mShould transfer from local_model_validation to join\n",
      "\n",
      "Calling aggregated_model_validation\n",
      "\u001b[94m\u001b[1m\u001b[31mPerforming aggregated model validation for collaborator Chandler\u001b[0m\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94m\u001b[4m\u001b[35mTest set: Avg. loss: 0.7596, Accuracy: 2071/2500 (83%)\u001b[0m\u001b[0m\u001b[94m\n",
      "\n",
      "\u001b[0m\u001b[94m\u001b[4m\u001b[31mChandler value of 0.8284000158309937\u001b[0m\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94mSaving data artifacts for aggregated_model_validation\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94mSaved data artifacts for aggregated_model_validation\u001b[0m\u001b[94m\n",
      "\u001b[0m\n",
      "Calling train\n",
      "\u001b[94m\u001b[33mTrain Epoch: 1 [0/15000 (0%)]\tLoss: 1.079716\u001b[0m\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94m\u001b[33mTrain Epoch: 1 [640/15000 (4%)]\tLoss: 1.413034\u001b[0m\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94m\u001b[33mTrain Epoch: 1 [1280/15000 (9%)]\tLoss: 1.138919\u001b[0m\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94m\u001b[33mTrain Epoch: 1 [1920/15000 (13%)]\tLoss: 1.137443\u001b[0m\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94m\u001b[33mTrain Epoch: 1 [2560/15000 (17%)]\tLoss: 1.255704\u001b[0m\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94m\u001b[33mTrain Epoch: 1 [3200/15000 (21%)]\tLoss: 1.109210\u001b[0m\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94m\u001b[33mTrain Epoch: 1 [3840/15000 (26%)]\tLoss: 1.115990\u001b[0m\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94m\u001b[33mTrain Epoch: 1 [4480/15000 (30%)]\tLoss: 0.829298\u001b[0m\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94m\u001b[33mTrain Epoch: 1 [5120/15000 (34%)]\tLoss: 0.847504\u001b[0m\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94m\u001b[33mTrain Epoch: 1 [5760/15000 (38%)]\tLoss: 0.735219\u001b[0m\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94m\u001b[33mTrain Epoch: 1 [6400/15000 (43%)]\tLoss: 0.657065\u001b[0m\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94m\u001b[33mTrain Epoch: 1 [7040/15000 (47%)]\tLoss: 0.809990\u001b[0m\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94m\u001b[33mTrain Epoch: 1 [7680/15000 (51%)]\tLoss: 0.867862\u001b[0m\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94m\u001b[33mTrain Epoch: 1 [8320/15000 (55%)]\tLoss: 0.978880\u001b[0m\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94m\u001b[33mTrain Epoch: 1 [8960/15000 (60%)]\tLoss: 0.675604\u001b[0m\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94m\u001b[33mTrain Epoch: 1 [9600/15000 (64%)]\tLoss: 0.890725\u001b[0m\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94m\u001b[33mTrain Epoch: 1 [10240/15000 (68%)]\tLoss: 0.970718\u001b[0m\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94m\u001b[33mTrain Epoch: 1 [10880/15000 (72%)]\tLoss: 0.573187\u001b[0m\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94m\u001b[33mTrain Epoch: 1 [11520/15000 (77%)]\tLoss: 0.860904\u001b[0m\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94m\u001b[33mTrain Epoch: 1 [12160/15000 (81%)]\tLoss: 0.795940\u001b[0m\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94m\u001b[33mTrain Epoch: 1 [12800/15000 (85%)]\tLoss: 0.835162\u001b[0m\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94m\u001b[33mTrain Epoch: 1 [13440/15000 (89%)]\tLoss: 0.714318\u001b[0m\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94m\u001b[33mTrain Epoch: 1 [14080/15000 (94%)]\tLoss: 0.724438\u001b[0m\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94m\u001b[33mTrain Epoch: 1 [14720/15000 (98%)]\tLoss: 0.758060\u001b[0m\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94mSaving data artifacts for train\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94mSaved data artifacts for train\u001b[0m\u001b[94m\n",
      "\u001b[0m\n",
      "Calling local_model_validation\n",
      "\u001b[94m\u001b[4m\u001b[35mTest set: Avg. loss: 0.4224, Accuracy: 2191/2500 (88%)\u001b[0m\u001b[0m\u001b[94m\n",
      "\n",
      "\u001b[0m\u001b[94m\u001b[97mDoing local model validation for collaborator Chandler:                 0.8763999938964844\u001b[0m\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94mSaving data artifacts for local_model_validation\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94mSaved data artifacts for local_model_validation\u001b[0m\u001b[94m\n",
      "\u001b[0mShould transfer from local_model_validation to join\n",
      "\n",
      "Calling aggregated_model_validation\n",
      "\u001b[94m\u001b[1m\u001b[31mPerforming aggregated model validation for collaborator Bangalore\u001b[0m\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94m\u001b[4m\u001b[35mTest set: Avg. loss: 0.7410, Accuracy: 2115/2500 (85%)\u001b[0m\u001b[0m\u001b[94m\n",
      "\n",
      "\u001b[0m\u001b[94m\u001b[4m\u001b[31mBangalore value of 0.8460000157356262\u001b[0m\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94mSaving data artifacts for aggregated_model_validation\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94mSaved data artifacts for aggregated_model_validation\u001b[0m\u001b[94m\n",
      "\u001b[0m\n",
      "Calling train\n",
      "\u001b[94m\u001b[33mTrain Epoch: 1 [0/15000 (0%)]\tLoss: 1.041321\u001b[0m\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94m\u001b[33mTrain Epoch: 1 [640/15000 (4%)]\tLoss: 1.102225\u001b[0m\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94m\u001b[33mTrain Epoch: 1 [1280/15000 (9%)]\tLoss: 1.061086\u001b[0m\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94m\u001b[33mTrain Epoch: 1 [1920/15000 (13%)]\tLoss: 0.862942\u001b[0m\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94m\u001b[33mTrain Epoch: 1 [2560/15000 (17%)]\tLoss: 0.858932\u001b[0m\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94m\u001b[33mTrain Epoch: 1 [3200/15000 (21%)]\tLoss: 0.916544\u001b[0m\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94m\u001b[33mTrain Epoch: 1 [3840/15000 (26%)]\tLoss: 0.821962\u001b[0m\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94m\u001b[33mTrain Epoch: 1 [4480/15000 (30%)]\tLoss: 0.919983\u001b[0m\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94m\u001b[33mTrain Epoch: 1 [5120/15000 (34%)]\tLoss: 0.830016\u001b[0m\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94m\u001b[33mTrain Epoch: 1 [5760/15000 (38%)]\tLoss: 0.876083\u001b[0m\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94m\u001b[33mTrain Epoch: 1 [6400/15000 (43%)]\tLoss: 0.921952\u001b[0m\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94m\u001b[33mTrain Epoch: 1 [7040/15000 (47%)]\tLoss: 0.664301\u001b[0m\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94m\u001b[33mTrain Epoch: 1 [7680/15000 (51%)]\tLoss: 0.631396\u001b[0m\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94m\u001b[33mTrain Epoch: 1 [8320/15000 (55%)]\tLoss: 0.585002\u001b[0m\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94m\u001b[33mTrain Epoch: 1 [8960/15000 (60%)]\tLoss: 0.464063\u001b[0m\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94m\u001b[33mTrain Epoch: 1 [9600/15000 (64%)]\tLoss: 0.774935\u001b[0m\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94m\u001b[33mTrain Epoch: 1 [10240/15000 (68%)]\tLoss: 0.455800\u001b[0m\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94m\u001b[33mTrain Epoch: 1 [10880/15000 (72%)]\tLoss: 0.965320\u001b[0m\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94m\u001b[33mTrain Epoch: 1 [11520/15000 (77%)]\tLoss: 0.661236\u001b[0m\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94m\u001b[33mTrain Epoch: 1 [12160/15000 (81%)]\tLoss: 0.662028\u001b[0m\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94m\u001b[33mTrain Epoch: 1 [12800/15000 (85%)]\tLoss: 0.653065\u001b[0m\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94m\u001b[33mTrain Epoch: 1 [13440/15000 (89%)]\tLoss: 0.682812\u001b[0m\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94m\u001b[33mTrain Epoch: 1 [14080/15000 (94%)]\tLoss: 0.655035\u001b[0m\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94m\u001b[33mTrain Epoch: 1 [14720/15000 (98%)]\tLoss: 0.339491\u001b[0m\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94mSaving data artifacts for train\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94mSaved data artifacts for train\u001b[0m\u001b[94m\n",
      "\u001b[0m\n",
      "Calling local_model_validation\n",
      "\u001b[94m\u001b[4m\u001b[35mTest set: Avg. loss: 0.3746, Accuracy: 2258/2500 (90%)\u001b[0m\u001b[0m\u001b[94m\n",
      "\n",
      "\u001b[0m\u001b[94m\u001b[97mDoing local model validation for collaborator Bangalore:                 0.9031999707221985\u001b[0m\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94mSaving data artifacts for local_model_validation\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94mSaved data artifacts for local_model_validation\u001b[0m\u001b[94m\n",
      "\u001b[0mShould transfer from local_model_validation to join\n",
      "\n",
      "Calling join\n",
      "\u001b[94m\u001b[32mAverage aggregated model validation values =                 0.8373000025749207\u001b[0m\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94m\u001b[32mAverage training loss = 0.5639179199934006\u001b[0m\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94m\u001b[32mAverage local model validation values =             0.8892999887466431\u001b[0m\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94mSaving data artifacts for join\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94mSaved data artifacts for join\u001b[0m\u001b[94m\n",
      "\u001b[0m\n",
      "Calling end\n",
      "\u001b[94m\u001b[30mThis is the end of the flow\u001b[0m\u001b[0m\u001b[94m\n",
      "\u001b[0mSaving data artifacts for end\n",
      "Saved data artifacts for end\n"
     ]
    }
   ],
   "source": [
    "model = None\n",
    "best_model = None\n",
    "optimizer = None\n",
    "flflow = FederatedFlow(model, optimizer, rounds=2, checkpoint=True)\n",
    "flflow.runtime = local_runtime\n",
    "flflow.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "dc503676",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample of the final model weights: tensor([[[ 0.1624, -0.0078, -0.0106,  0.1956, -0.1898],\n",
      "         [-0.0887,  0.0761, -0.1713, -0.0497, -0.1441],\n",
      "         [-0.1015, -0.1179, -0.2096,  0.0269, -0.0182],\n",
      "         [ 0.1536, -0.0342,  0.1112,  0.0909, -0.1489],\n",
      "         [-0.1765, -0.1411, -0.1471, -0.1045,  0.0615]]])\n",
      "\n",
      "Final aggregated model accuracy for 2 rounds of training: 0.8373000025749207\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "    f'Sample of the final model weights: {flflow.model.state_dict()[\"conv1.weight\"][0]}'\n",
    ")\n",
    "\n",
    "print(\n",
    "    f\"\\nFinal aggregated model accuracy for {flflow.rounds} rounds of training: {flflow.aggregated_model_accuracy}\"\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
