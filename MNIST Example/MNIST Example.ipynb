{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a66a6430",
   "metadata": {},
   "source": [
    "**Table of contents**<a id='toc0_'></a>    \n",
    "- [Importation des bibliothèques](#toc1_)    \n",
    "  - [Importation des paquets ou modules de la bibliothèque OpenFL](#toc1_1_)    \n",
    "  - [Importation des paquets ou modules de la bibliothèque PyTorch](#toc1_2_)    \n",
    "  - [Importation d’autres paquets ou modules requis](#toc1_3_)    \n",
    "- [Définition du modèle d‘entraînement](#toc2_)    \n",
    "  - [Définition des chargeurs de données](#toc2_1_)    \n",
    "  - [Définition du modèle de réseau CNN](#toc2_2_)    \n",
    "  - [Définition de la fonction d'inférence utilisée dans le test](#toc2_3_)    \n",
    "- [Définition des règles de l'apprentissage fédéré](#toc3_)    \n",
    "  - [Méthode de calcul de la moyenne des poids d'apprentissage fédéré](#toc3_1_)    \n",
    "\n",
    "<!-- vscode-jupyter-toc-config\n",
    "\tnumbering=false\n",
    "\tanchor=true\n",
    "\tflat=false\n",
    "\tminLevel=1\n",
    "\tmaxLevel=6\n",
    "\t/vscode-jupyter-toc-config -->\n",
    "<!-- THIS CELL WILL BE REPLACED ON TOC UPDATE. DO NOT WRITE YOUR TEXT IN THIS CELL -->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e7fe1d1",
   "metadata": {},
   "source": [
    "# <a id='toc1_'></a>[Importation des bibliothèques](#toc0_)\n",
    "\n",
    "## <a id='toc1_1_'></a>[Importation des paquets ou modules de la bibliothèque OpenFL](#toc0_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4f63aa4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# La classe `FLSpec` définit la spécification du flux.\n",
    "\n",
    "# Les flux définis par l'utilisateur sont des sous-classes de cette classe.\n",
    "from openfl.experimental.workflow.interface import Aggregator, Collaborator, FLSpec\n",
    "# La fonction `aggregator/collaborator` est un décorateur de placement qui définit l'endroit où la \n",
    "# tâche sera assignée.\n",
    "from openfl.experimental.workflow.placement import aggregator, collaborator\n",
    "from openfl.experimental.workflow.runtime import LocalRuntime"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c852c64d",
   "metadata": {},
   "source": [
    "## <a id='toc1_2_'></a>[Importation des paquets ou modules de la bibliothèque PyTorch](#toc0_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2b89c419",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from torchsummary import summary\n",
    "\n",
    "import torchvision"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60958c33",
   "metadata": {},
   "source": [
    "## <a id='toc1_3_'></a>[Importation d’autres paquets ou modules requis](#toc0_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3d76255d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from termcolor import cprint"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22594881",
   "metadata": {},
   "source": [
    "# <a id='toc2_'></a>[Définition du modèle d‘entraînement](#toc0_)\n",
    "\n",
    "## <a id='toc2_1_'></a>[Définition des chargeurs de données](#toc0_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "67ffab47",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "01. torchvision.transforms.Compose(transforms)\n",
    "    - Composes several transforms together.\n",
    "\n",
    "02. torchvision.transforms.Normalize(mean, std, inplace=False)\n",
    "    - Normalize a tensor image with mean and standard deviation.\n",
    "    - output[channel] = (input[channel] - mean[channel]) / std[channel]\n",
    "\"\"\"\n",
    "\n",
    "cifar10_train = torchvision.datasets.MNIST(\n",
    "    \"/tmp/files/\",\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=torchvision.transforms.Compose(\n",
    "        [\n",
    "            torchvision.transforms.ToTensor(),\n",
    "            # Les valeurs ` 0.1307` et `0.3081` utilisées pour la transformation `Normalize()`\n",
    "            # ci-dessous sont la moyenne globale et l’écart-type de l’ensemble de données MNIST.\n",
    "            torchvision.transforms.Normalize((0.1307,), (0.3081,)),\n",
    "        ]\n",
    "    ),\n",
    ")\n",
    "\n",
    "cifar10_test = torchvision.datasets.MNIST(\n",
    "    \"/tmp/files/\",\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=torchvision.transforms.Compose(\n",
    "        [\n",
    "            torchvision.transforms.ToTensor(),\n",
    "            torchvision.transforms.Normalize((0.1307,), (0.3081,)),\n",
    "        ]\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60eaf1c0",
   "metadata": {},
   "source": [
    "## <a id='toc2_2_'></a>[Définition du modèle de réseau CNN](#toc0_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cba3007c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "03. torch.nn.Conv2d(in_channels, out_channels, kernel_size, stride=1, padding=0, dilation=1,\n",
    "    groups=1, bias=True, padding_mode='zeros', device=None, dtype=None)\n",
    "    - Applies a 2D convolution over an input signal composed of several input planes.\n",
    "\n",
    "04. torch.nn.Dropout2d(p=0.5, inplace=False)\n",
    "    - Randomly zero out entire channels.\n",
    "    - Each channel will be zeroed out independently on every forward call with probability p using\n",
    "    samples from a Bernoulli distribution.\n",
    "\n",
    "05. torch.nn.functional.max_pool2d(input, kernel_size, stride=None, padding=0,\n",
    "    dilation=1, ceil_mode=False, return_indices=False)\n",
    "    - Applies a 2D max pooling over an input signal composed of several input planes.\n",
    "\n",
    "06. torch.nn.functional.dropout(input, p=0.5, training=True, inplace=False)\n",
    "    - During training, randomly zeroes some elements of the input tensor with probability p.\n",
    "    - Uses samples from a Bernoulli distribution.\n",
    "\n",
    "07. torch.nn.functional.log_softmax(input, dim=None, _stacklevel=3, dtype=None)\n",
    "    - Apply a softmax followed by a logarithm.\n",
    "    - While mathematically equivalent to log(softmax(x)), doing these two operations separately is\n",
    "    slower and numerically unstable. This function uses an alternative formulation to compute the\n",
    "    output and gradient correctly.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        # La première couche convolutionnelle : le nombre de canaux d’entrée est de 1, c’est-à-dire\n",
    "        # une image en niveaux de gris, le nombre de canaux de sortie est de 10, la taille du filtre\n",
    "        # convolutif est de 5x5, le stride est de 1 et le padding est de 0.\n",
    "\n",
    "        # Par conséquent, après que l'image d'entrée (1x28x28) a été convoluée, la taille de la\n",
    "        # carte de caractéristiques de sortie est de 10x24x24.\n",
    "        self.conv1 = nn.Conv2d(1, 10, kernel_size=5)\n",
    "        # La deuxième couche convolutionnelle : le nombre de canaux d’entrée est de 10, le nombre de\n",
    "        # canaux de sortie est de 20, la taille du filtre convolutif est de 5x5, le stride est de 1\n",
    "        # et le padding est de 0.\n",
    "\n",
    "        # Par conséquent, après que l'image d'entrée (10x12x12) a été convoluée, la taille de la\n",
    "        # carte de caractéristiques de sortie est de 20x8x8.\n",
    "        self.conv2 = nn.Conv2d(10, 20, kernel_size=5)\n",
    "        # Une couche d'abandon.\n",
    "        self.conv2_drop = nn.Dropout2d()\n",
    "        # La première couche de fully connected : le nombre de canaux d’entrée est de 20, chaque\n",
    "        # canal a une taille de 4x4, soit un total de 20x4x4 = 320 nœuds, tandis que la sortie est\n",
    "        # fixée à 50 nœuds.\n",
    "        self.fc1 = nn.Linear(320, 50)\n",
    "        # La deuxième couche de fully connected : l'entrée a 50 nœuds et la sortie a 10 nœuds\n",
    "        # (correspondant à 10 catégories).\n",
    "        self.fc2 = nn.Linear(50, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # La première couche convolutive est suivie d'une couche de pooling de type max pooling avec\n",
    "        # un filtre convolutif de taille de 2x2 et un stride égal à la longueur du filtre.\n",
    "\n",
    "        # La taille d'entrée est de 10x24x24, et après pooling, la taille de sortie est de 10x12x12.\n",
    "        x = F.relu(F.max_pool2d(self.conv1(x), 2))\n",
    "        # La deuxième couche convolutive est suivie d'une couche d'abandon.\n",
    "\n",
    "        # Après la couche d'abandon, suit une autre couche de max-pooling, qui possède un filtre\n",
    "        # convolutif de taille de 2x2 et un stride aussi égal à la longueur du filtre.\n",
    "\n",
    "        # La taille d'entrée est de 20x8x8, et après pooling, la taille de sortie est de 20x4x4.\n",
    "        x = F.relu(F.max_pool2d(self.conv2_drop(self.conv2(x)), 2))\n",
    "        # La carte de caractéristiques multidimensionnelle est transformée en un vecteur\n",
    "        # unidimensionnel, d'une taille de 20x4x4 = 320.\n",
    "        x = x.view(-1, 320)\n",
    "        # La première couche de fully connected, activée par la fonction d'activation ReLU.\n",
    "        x = F.relu(self.fc1(x))\n",
    "        # Pendant l'entraînement du modèle, certains nœuds de la sortie de la première couche de\n",
    "        # fully connected sont mis à zéro de manière aléatoire avec une probabilité p.\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        # La deuxième couche de fully connected sert également de couche de sortie.\n",
    "        x = self.fc2(x)\n",
    "        # Les probabilités logarithmiques de tous les nœuds de la couche de sortie sont calculées en\n",
    "        # appliquant une fonction softmax suivie d'un logarithme.\n",
    "        return F.log_softmax(x, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "01738796",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1           [-1, 10, 24, 24]             260\n",
      "            Conv2d-2             [-1, 20, 8, 8]           5,020\n",
      "         Dropout2d-3             [-1, 20, 8, 8]               0\n",
      "            Linear-4                   [-1, 50]          16,050\n",
      "            Linear-5                   [-1, 10]             510\n",
      "================================================================\n",
      "Total params: 21,840\n",
      "Trainable params: 21,840\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.00\n",
      "Forward/backward pass size (MB): 0.06\n",
      "Params size (MB): 0.08\n",
      "Estimated Total Size (MB): 0.15\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "model = Net()\n",
    "summary(model, next(iter(cifar10_train))[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7933b052",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[35m  conv1.weight .............................................. 250  \u001b[0m\n",
      "\u001b[35m  conv1.bias ................................................. 10  \u001b[0m\n",
      "\u001b[35m  conv2.weight ............................................. 5000  \u001b[0m\n",
      "\u001b[35m  conv2.bias ................................................. 20  \u001b[0m\n",
      "\u001b[35m  fc1.weight .............................................. 16000  \u001b[0m\n",
      "\u001b[35m  fc1.bias ................................................... 50  \u001b[0m\n",
      "\u001b[35m  fc2.weight ................................................ 500  \u001b[0m\n",
      "\u001b[35m  fc2.bias ................................................... 10  \u001b[0m\n",
      "\u001b[35m _________________________________________________________________ \u001b[0m\n",
      "\u001b[35m  total parameters ........................................ 21840  \u001b[0m\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def count_parameters(model):\n",
    "    length = 67\n",
    "    names = [n for (n, p) in model.named_parameters() if p.requires_grad]\n",
    "    name = \"total parameters\"\n",
    "    names.append(name)\n",
    "    max_length = max(map(len, names))\n",
    "    formatted_names = [f\"{f'  {n} ':.<{max_length + 3}}\" for n in names]\n",
    "    params = [p.numel() for p in model.parameters() if p.requires_grad]\n",
    "    params.append(sum(params))\n",
    "    formatted_params = [f\"{f' {p}  ':.>{length - max_length - 3}}\" for p in params]\n",
    "\n",
    "    for n, p in zip(formatted_names[:-1], formatted_params[:-1]):\n",
    "        cprint((n + p), \"magenta\")\n",
    "    cprint(\" \" + \"_\" * (length - 2) + \" \", \"magenta\")\n",
    "    cprint(\n",
    "        (formatted_names[-1] + formatted_params[-1]),\n",
    "        \"magenta\",\n",
    "        end=\"\\n\\n\",\n",
    "    )\n",
    "\n",
    "    return names, params\n",
    "\n",
    "\n",
    "names, params = count_parameters(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3675674",
   "metadata": {},
   "source": [
    "## <a id='toc2_3_'></a>[Définition de la fonction d'inférence utilisée dans le test](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "13b360d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "08. torch.nn.functional.nll_loss(input, target, weight=None, size_average=None, ignore_index=-100,\n",
    "    reduce=None, reduction='mean')\n",
    "    - Compute the negative log likelihood loss.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def inference(network, test_loader):\n",
    "    # Mettre le module en mode évaluation.\n",
    "    network.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            output = network(data)\n",
    "            # L'entropie est une mesure de l'incertitude, c'est-à-dire que, si un résultat est\n",
    "            # ertain, l'entropie est faible.\n",
    "\n",
    "            # La perte d'entropie croisée, ou perte logarithmique mesure les performances d'un\n",
    "            # modèle de classification dont le résultat est une valeur de probabilité comprise entre\n",
    "            # 0 et 1.\n",
    "\n",
    "            # La perte d'entropie croisée augmente à mesure que la probabilité prédite s'écarte de\n",
    "            # l'étiquette réelle.\n",
    "\n",
    "            # L’entropie croisée catégorielle sert au classement en plusieurs classes.\n",
    "\n",
    "            # Le log-vraisemblance négatif est également connu sous le nom d'entropie croisée\n",
    "            # catégorielle, car il s'agit en fait de deux interprétations différentes de la même\n",
    "            # formule.\n",
    "\n",
    "            # test_loss += F.cross_entropy(output, target, reduction=\"sum\").item()\n",
    "            test_loss += F.nll_loss(output, target, reduction=\"sum\").item()\n",
    "\n",
    "            # Si `keepdim` est `True`, le tenseur de sortie est de la même taille que celui \n",
    "            # d'entrée, sauf dans la (les) dimension(s) `dim` où il est de dimension 1.\n",
    "            pred = output.data.max(dim=1, keepdim=True)[1]\n",
    "            # Calcul de l'égalité par éléments.\n",
    "            correct += pred.eq(target.data.view_as(pred)).sum()\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    cprint(\n",
    "        \"Test set: Avg. loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\".format(\n",
    "            test_loss,\n",
    "            correct,\n",
    "            len(test_loader.dataset),\n",
    "            100.0 * correct / len(test_loader.dataset),\n",
    "        ),\n",
    "        \"magenta\",\n",
    "        attrs=[\"underline\"],\n",
    "        end=\"\\n\\n\",\n",
    "    )\n",
    "    accuracy = float(correct / len(test_loader.dataset))\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7f8c156a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[4m\u001b[35mTest set: Avg. loss: 2.3147, Accuracy: 1086/10000 (11%)\u001b[0m\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.10859999805688858"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_loader = DataLoader(cifar10_test, batch_size=500, shuffle=False)\n",
    "\n",
    "inference(model, test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bc71b0f",
   "metadata": {},
   "source": [
    "# <a id='toc3_'></a>[Définition des règles de l'apprentissage fédéré](#toc0_)\n",
    "\n",
    "## <a id='toc3_1_'></a>[Méthode de calcul de la moyenne des poids d'apprentissage fédéré](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "59d93477",
   "metadata": {},
   "outputs": [],
   "source": [
    "def FedAvg(models, weights=None):\n",
    "    new_model = models[0]\n",
    "    state_dicts = [model.state_dict() for model in models]\n",
    "    state_dict = new_model.state_dict()\n",
    "    for key in models[1].state_dict():\n",
    "        state_dict[key] = torch.from_numpy(\n",
    "            np.average(\n",
    "                [state[key].numpy() for state in state_dicts], axis=0, weights=weights\n",
    "            )\n",
    "        )\n",
    "    new_model.load_state_dict(state_dict)\n",
    "    return new_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "670a3019",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keys = [k for k in model.state_dict().keys()]\n",
    "names = [n for (n, p) in model.named_parameters() if p.requires_grad]\n",
    "keys == names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a3d20ab8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aggregator step \"start\" registered\n",
      "Collaborator step \"aggregated_model_validation\" registered\n",
      "Collaborator step \"train\" registered\n",
      "Collaborator step \"local_model_validation\" registered\n",
      "Aggregator step \"join\" registered\n",
      "Aggregator step \"end\" registered\n"
     ]
    }
   ],
   "source": [
    "# n_epochs = 3\n",
    "# batch_size_train = 64\n",
    "# batch_size_test = 1000\n",
    "learning_rate = 0.01\n",
    "log_interval = 10\n",
    "momentum = 0.5\n",
    "\n",
    "\n",
    "# random_seed = 1\n",
    "# torch.backends.cudnn.enabled = False\n",
    "# torch.manual_seed(random_seed)\n",
    "\n",
    "\n",
    "class FederatedFlow(FLSpec):\n",
    "\n",
    "    def __init__(self, model=None, optimizer=None, rounds=3, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        # Importe un modèle personnalisé et ajoute le bon algorithme d’optimisation pour ce dernier.\n",
    "        if model is not None:\n",
    "            self.model = model\n",
    "            self.optimizer = optimizer\n",
    "        # Chargez le modèle `Net()` et configurez l'optimiseur pour qu'il s'applique uniquement à ce\n",
    "        # modèle.\n",
    "        else:\n",
    "            self.model = Net()\n",
    "            self.optimizer = optim.SGD(\n",
    "                self.model.parameters(), lr=learning_rate, momentum=momentum\n",
    "            )\n",
    "        self.rounds = rounds\n",
    "\n",
    "    # Un agrégateur est le nœud central de l'apprentissage fédéré.\n",
    "\n",
    "    # L'agrégateur commence par un modèle et un optimiseur transmis de manière facultative.\n",
    "\n",
    "    # L'agrégateur commence le flux avec la tâche de `start`, où la liste des collaborateurs est\n",
    "    # extraite de l'exécution (`self.collaborators = self.runtime.collaborators`) et est ensuite\n",
    "    # utilisée comme liste de participants pour exécuterla tâche énumérée dans `self.next`,\n",
    "    # `aggregated_model_validation`.\n",
    "    @aggregator\n",
    "    def start(self):\n",
    "        cprint(f\"Performing initialization for model\", \"black\", attrs=[\"bold\"])\n",
    "        self.collaborators = self.runtime.collaborators\n",
    "        self.private = 10\n",
    "        self.current_round = 0\n",
    "        self.next(\n",
    "            self.aggregated_model_validation,\n",
    "            foreach=\"collaborators\",\n",
    "            exclude=[\"private\"],\n",
    "        )\n",
    "\n",
    "    # Le modèle, l'optimiseur et tout ce qui n'est pas explicitement exclu de la fonction suivante\n",
    "    # seront transmis de la fonction de `start` de l'agrégateur à la tâche\n",
    "    # `aggregated_model_validation` du collaborateur.\n",
    "\n",
    "    # L’endroit où les tâches sont exécutées est déterminé par le décorateur de placement qui\n",
    "    # précède chaque définition de tâche (`@aggregator` ou `@collaborator`).\n",
    "\n",
    "    # Une fois que chaque collaborateur (défini dans l’exécution) a terminé la tâche\n",
    "    # `aggregated_model_validation`, il transmet son état actuel à la tâche `train`, de `train` à\n",
    "    # `local_model_validation`, et enfin à `join` à l'agrégateur.\n",
    "\n",
    "    # C'est au niveau de `join` qu'une moyenne des poids des modèles est calculée et que le tour\n",
    "    # suivant peut commencer.\n",
    "    @collaborator\n",
    "    def aggregated_model_validation(self):\n",
    "        cprint(\n",
    "            f\"Performing aggregated model validation for collaborator {self.input}\",\n",
    "            \"red\",\n",
    "            attrs=[\"bold\"],\n",
    "        )\n",
    "        self.agg_validation_score = inference(self.model, self.test_loader)\n",
    "        cprint(\n",
    "            f\"{self.input} value of {self.agg_validation_score}\",\n",
    "            \"red\",\n",
    "            attrs=[\"underline\"],\n",
    "        )\n",
    "        self.next(self.train)\n",
    "\n",
    "    @collaborator\n",
    "    def train(self):\n",
    "        self.model.train()\n",
    "        self.optimizer = optim.SGD(\n",
    "            self.model.parameters(), lr=learning_rate, momentum=momentum\n",
    "        )\n",
    "        for batch_idx, (data, target) in enumerate(self.train_loader):\n",
    "            self.optimizer.zero_grad()\n",
    "            output = self.model(data)\n",
    "            loss = F.nll_loss(output, target)\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "            if batch_idx % log_interval == 0:\n",
    "                cprint(\n",
    "                    \"Train Epoch: 1 [{}/{} ({:.0f}%)]\\tLoss: {:.6f}\".format(\n",
    "                        batch_idx * len(data),\n",
    "                        len(self.train_loader.dataset),\n",
    "                        100.0 * batch_idx / len(self.train_loader),\n",
    "                        loss.item(),\n",
    "                    ),\n",
    "                    \"yellow\",\n",
    "                )\n",
    "                self.loss = loss.item()\n",
    "                torch.save(self.model.state_dict(), \"model.pth\")\n",
    "                torch.save(self.optimizer.state_dict(), \"optimizer.pth\")\n",
    "        self.training_completed = True\n",
    "        self.next(self.local_model_validation)\n",
    "\n",
    "    @collaborator\n",
    "    def local_model_validation(self):\n",
    "        self.local_validation_score = inference(self.model, self.test_loader)\n",
    "        cprint(\n",
    "            f\"Doing local model validation for collaborator {self.input}: \\\n",
    "                {self.local_validation_score}\",\n",
    "            \"white\",\n",
    "        )\n",
    "        self.next(self.join, exclude=[\"training_completed\"])\n",
    "\n",
    "    @aggregator\n",
    "    def join(self, inputs):\n",
    "        self.average_loss = sum(input.loss for input in inputs) / len(inputs)\n",
    "        self.aggregated_model_accuracy = sum(\n",
    "            input.agg_validation_score for input in inputs\n",
    "        ) / len(inputs)\n",
    "        self.local_model_accuracy = sum(\n",
    "            input.local_validation_score for input in inputs\n",
    "        ) / len(inputs)\n",
    "        cprint(\n",
    "            f\"Average aggregated model validation values = \\\n",
    "                {self.aggregated_model_accuracy}\",\n",
    "            \"green\",\n",
    "        )\n",
    "        cprint(f\"Average training loss = {self.average_loss}\", \"green\")\n",
    "        cprint(\n",
    "            f\"Average local model validation values = \\\n",
    "            {self.local_model_accuracy}\",\n",
    "            \"green\",\n",
    "        )\n",
    "        self.model = FedAvg([input.model for input in inputs])\n",
    "        self.optimizer = [input.optimizer for input in inputs][0]\n",
    "        self.current_round += 1\n",
    "        if self.current_round < self.rounds:\n",
    "            self.next(\n",
    "                self.aggregated_model_validation,\n",
    "                foreach=\"collaborators\",\n",
    "                exclude=[\"private\"],\n",
    "            )\n",
    "        else:\n",
    "            self.next(self.end)\n",
    "\n",
    "    @aggregator\n",
    "    def end(self):\n",
    "        cprint(f\"This is the end of the flow\", \"black\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b96e3045",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Local runtime collaborators = ['Portland', 'Seattle', 'Chandler', 'Bangalore']\n"
     ]
    }
   ],
   "source": [
    "batch_size_train = 64\n",
    "\n",
    "# Setup participants\n",
    "aggregator = Aggregator()\n",
    "aggregator.private_attributes = {}\n",
    "\n",
    "# Setup collaborators with private attributes\n",
    "collaborator_names = ['Portland', 'Seattle', 'Chandler','Bangalore']\n",
    "collaborators = [Collaborator(name=name) for name in collaborator_names]\n",
    "for idx, collaborator in enumerate(collaborators):\n",
    "    local_train = deepcopy(cifar10_train)\n",
    "    local_test = deepcopy(cifar10_test)\n",
    "    local_train.data = cifar10_train.data[idx::len(collaborators)]\n",
    "    local_train.targets = cifar10_train.targets[idx::len(collaborators)]\n",
    "    local_test.data = cifar10_test.data[idx::len(collaborators)]\n",
    "    local_test.targets = cifar10_test.targets[idx::len(collaborators)]\n",
    "    collaborator.private_attributes = {\n",
    "            'train_loader': DataLoader(local_train,batch_size=batch_size_train, shuffle=True),\n",
    "            'test_loader': DataLoader(local_test,batch_size=batch_size_train, shuffle=True)\n",
    "    }\n",
    "\n",
    "local_runtime = LocalRuntime(aggregator=aggregator, collaborators=collaborators, backend='single_process')\n",
    "print(f'Local runtime collaborators = {local_runtime.collaborators}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5494deda",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "if os.environ.get(\"USERNAME\") is None:\n",
    "    os.environ[\"USERNAME\"] = \"Hao\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8018e1e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hao\n"
     ]
    }
   ],
   "source": [
    "import getpass\n",
    "print(getpass.getuser())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "edbaff16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created flow FederatedFlow\n",
      "\n",
      "Calling start\n",
      "\u001b[94m\u001b[1m\u001b[30mPerforming initialization for model\u001b[0m\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94mSaving data artifacts for start\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94mSaved data artifacts for start\u001b[0m\u001b[94m\n",
      "\u001b[0m\n",
      "Calling aggregated_model_validation\n",
      "\u001b[94m\u001b[1m\u001b[31mPerforming aggregated model validation for collaborator Portland\u001b[0m\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94m\u001b[4m\u001b[35mTest set: Avg. loss: 2.3133, Accuracy: 262/2500 (10%)\u001b[0m\u001b[0m\u001b[94m\n",
      "\n",
      "\u001b[0m\u001b[94m\u001b[4m\u001b[31mPortland value of 0.10480000078678131\u001b[0m\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94mSaving data artifacts for aggregated_model_validation\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94mSaved data artifacts for aggregated_model_validation\u001b[0m\u001b[94m\n",
      "\u001b[0m\n",
      "Calling train\n",
      "\u001b[94m\u001b[33mTrain Epoch: 1 [0/15000 (0%)]\tLoss: 2.388203\u001b[0m\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94m\u001b[33mTrain Epoch: 1 [640/15000 (4%)]\tLoss: 2.292961\u001b[0m\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94m\u001b[33mTrain Epoch: 1 [1280/15000 (9%)]\tLoss: 2.267097\u001b[0m\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94m\u001b[33mTrain Epoch: 1 [1920/15000 (13%)]\tLoss: 2.276434\u001b[0m\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94m\u001b[33mTrain Epoch: 1 [2560/15000 (17%)]\tLoss: 2.277918\u001b[0m\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94m\u001b[33mTrain Epoch: 1 [3200/15000 (21%)]\tLoss: 2.237162\u001b[0m\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94m\u001b[33mTrain Epoch: 1 [3840/15000 (26%)]\tLoss: 2.200501\u001b[0m\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94m\u001b[33mTrain Epoch: 1 [4480/15000 (30%)]\tLoss: 2.157209\u001b[0m\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94m\u001b[33mTrain Epoch: 1 [5120/15000 (34%)]\tLoss: 2.168993\u001b[0m\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94m\u001b[33mTrain Epoch: 1 [5760/15000 (38%)]\tLoss: 2.005601\u001b[0m\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94m\u001b[33mTrain Epoch: 1 [6400/15000 (43%)]\tLoss: 2.081197\u001b[0m\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94m\u001b[33mTrain Epoch: 1 [7040/15000 (47%)]\tLoss: 2.010782\u001b[0m\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94m\u001b[33mTrain Epoch: 1 [7680/15000 (51%)]\tLoss: 1.948575\u001b[0m\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94m\u001b[33mTrain Epoch: 1 [8320/15000 (55%)]\tLoss: 1.805282\u001b[0m\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94m\u001b[33mTrain Epoch: 1 [8960/15000 (60%)]\tLoss: 1.747229\u001b[0m\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94m\u001b[33mTrain Epoch: 1 [9600/15000 (64%)]\tLoss: 1.534110\u001b[0m\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94m\u001b[33mTrain Epoch: 1 [10240/15000 (68%)]\tLoss: 1.366669\u001b[0m\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94m\u001b[33mTrain Epoch: 1 [10880/15000 (72%)]\tLoss: 1.325932\u001b[0m\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94m\u001b[33mTrain Epoch: 1 [11520/15000 (77%)]\tLoss: 1.275600\u001b[0m\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94m\u001b[33mTrain Epoch: 1 [12160/15000 (81%)]\tLoss: 1.164613\u001b[0m\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94m\u001b[33mTrain Epoch: 1 [12800/15000 (85%)]\tLoss: 1.096380\u001b[0m\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94m\u001b[33mTrain Epoch: 1 [13440/15000 (89%)]\tLoss: 1.217817\u001b[0m\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94m\u001b[33mTrain Epoch: 1 [14080/15000 (94%)]\tLoss: 1.177906\u001b[0m\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94m\u001b[33mTrain Epoch: 1 [14720/15000 (98%)]\tLoss: 1.092584\u001b[0m\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94mSaving data artifacts for train\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94mSaved data artifacts for train\u001b[0m\u001b[94m\n",
      "\u001b[0m\n",
      "Calling local_model_validation\n",
      "\u001b[94m\u001b[4m\u001b[35mTest set: Avg. loss: 0.7683, Accuracy: 2026/2500 (81%)\u001b[0m\u001b[0m\u001b[94m\n",
      "\n",
      "\u001b[0m\u001b[94m\u001b[97mDoing local model validation for collaborator Portland:                 0.8104000091552734\u001b[0m\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94mSaving data artifacts for local_model_validation\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94mSaved data artifacts for local_model_validation\u001b[0m\u001b[94m\n",
      "\u001b[0mShould transfer from local_model_validation to join\n",
      "\n",
      "Calling aggregated_model_validation\n",
      "\u001b[94m\u001b[1m\u001b[31mPerforming aggregated model validation for collaborator Seattle\u001b[0m\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94m\u001b[4m\u001b[35mTest set: Avg. loss: 2.3134, Accuracy: 236/2500 (9%)\u001b[0m\u001b[0m\u001b[94m\n",
      "\n",
      "\u001b[0m\u001b[94m\u001b[4m\u001b[31mSeattle value of 0.09440000355243683\u001b[0m\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94mSaving data artifacts for aggregated_model_validation\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94mSaved data artifacts for aggregated_model_validation\u001b[0m\u001b[94m\n",
      "\u001b[0m\n",
      "Calling train\n",
      "\u001b[94m\u001b[33mTrain Epoch: 1 [0/15000 (0%)]\tLoss: 2.341307\u001b[0m\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94m\u001b[33mTrain Epoch: 1 [640/15000 (4%)]\tLoss: 2.286708\u001b[0m\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94m\u001b[33mTrain Epoch: 1 [1280/15000 (9%)]\tLoss: 2.291805\u001b[0m\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94m\u001b[33mTrain Epoch: 1 [1920/15000 (13%)]\tLoss: 2.274368\u001b[0m\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94m\u001b[33mTrain Epoch: 1 [2560/15000 (17%)]\tLoss: 2.264927\u001b[0m\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94m\u001b[33mTrain Epoch: 1 [3200/15000 (21%)]\tLoss: 2.221889\u001b[0m\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94m\u001b[33mTrain Epoch: 1 [3840/15000 (26%)]\tLoss: 2.199017\u001b[0m\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94m\u001b[33mTrain Epoch: 1 [4480/15000 (30%)]\tLoss: 2.145065\u001b[0m\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94m\u001b[33mTrain Epoch: 1 [5120/15000 (34%)]\tLoss: 2.042996\u001b[0m\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94m\u001b[33mTrain Epoch: 1 [5760/15000 (38%)]\tLoss: 2.047870\u001b[0m\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94m\u001b[33mTrain Epoch: 1 [6400/15000 (43%)]\tLoss: 1.861350\u001b[0m\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94m\u001b[33mTrain Epoch: 1 [7040/15000 (47%)]\tLoss: 1.826393\u001b[0m\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94m\u001b[33mTrain Epoch: 1 [7680/15000 (51%)]\tLoss: 1.864169\u001b[0m\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94m\u001b[33mTrain Epoch: 1 [8320/15000 (55%)]\tLoss: 1.632296\u001b[0m\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94m\u001b[33mTrain Epoch: 1 [8960/15000 (60%)]\tLoss: 1.437944\u001b[0m\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94m\u001b[33mTrain Epoch: 1 [9600/15000 (64%)]\tLoss: 1.351970\u001b[0m\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94m\u001b[33mTrain Epoch: 1 [10240/15000 (68%)]\tLoss: 1.280639\u001b[0m\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94m\u001b[33mTrain Epoch: 1 [10880/15000 (72%)]\tLoss: 1.296837\u001b[0m\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94m\u001b[33mTrain Epoch: 1 [11520/15000 (77%)]\tLoss: 1.294080\u001b[0m\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94m\u001b[33mTrain Epoch: 1 [12160/15000 (81%)]\tLoss: 1.102941\u001b[0m\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94m\u001b[33mTrain Epoch: 1 [12800/15000 (85%)]\tLoss: 1.058780\u001b[0m\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94m\u001b[33mTrain Epoch: 1 [13440/15000 (89%)]\tLoss: 0.979812\u001b[0m\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94m\u001b[33mTrain Epoch: 1 [14080/15000 (94%)]\tLoss: 0.979474\u001b[0m\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94m\u001b[33mTrain Epoch: 1 [14720/15000 (98%)]\tLoss: 0.937825\u001b[0m\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94mSaving data artifacts for train\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94mSaved data artifacts for train\u001b[0m\u001b[94m\n",
      "\u001b[0m\n",
      "Calling local_model_validation\n",
      "\u001b[94m\u001b[4m\u001b[35mTest set: Avg. loss: 0.6040, Accuracy: 2052/2500 (82%)\u001b[0m\u001b[0m\u001b[94m\n",
      "\n",
      "\u001b[0m\u001b[94m\u001b[97mDoing local model validation for collaborator Seattle:                 0.8208000063896179\u001b[0m\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94mSaving data artifacts for local_model_validation\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94mSaved data artifacts for local_model_validation\u001b[0m\u001b[94m\n",
      "\u001b[0mShould transfer from local_model_validation to join\n",
      "\n",
      "Calling aggregated_model_validation\n",
      "\u001b[94m\u001b[1m\u001b[31mPerforming aggregated model validation for collaborator Chandler\u001b[0m\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94m\u001b[4m\u001b[35mTest set: Avg. loss: 2.3193, Accuracy: 230/2500 (9%)\u001b[0m\u001b[0m\u001b[94m\n",
      "\n",
      "\u001b[0m\u001b[94m\u001b[4m\u001b[31mChandler value of 0.09200000017881393\u001b[0m\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94mSaving data artifacts for aggregated_model_validation\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94mSaved data artifacts for aggregated_model_validation\u001b[0m\u001b[94m\n",
      "\u001b[0m\n",
      "Calling train\n",
      "\u001b[94m\u001b[33mTrain Epoch: 1 [0/15000 (0%)]\tLoss: 2.301071\u001b[0m\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94m\u001b[33mTrain Epoch: 1 [640/15000 (4%)]\tLoss: 2.289689\u001b[0m\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94m\u001b[33mTrain Epoch: 1 [1280/15000 (9%)]\tLoss: 2.282243\u001b[0m\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94m\u001b[33mTrain Epoch: 1 [1920/15000 (13%)]\tLoss: 2.265293\u001b[0m\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94m\u001b[33mTrain Epoch: 1 [2560/15000 (17%)]\tLoss: 2.273041\u001b[0m\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94m\u001b[33mTrain Epoch: 1 [3200/15000 (21%)]\tLoss: 2.218132\u001b[0m\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94m\u001b[33mTrain Epoch: 1 [3840/15000 (26%)]\tLoss: 2.246407\u001b[0m\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94m\u001b[33mTrain Epoch: 1 [4480/15000 (30%)]\tLoss: 2.228994\u001b[0m\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94m\u001b[33mTrain Epoch: 1 [5120/15000 (34%)]\tLoss: 2.137632\u001b[0m\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94m\u001b[33mTrain Epoch: 1 [5760/15000 (38%)]\tLoss: 2.032423\u001b[0m\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94m\u001b[33mTrain Epoch: 1 [6400/15000 (43%)]\tLoss: 1.948404\u001b[0m\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94m\u001b[33mTrain Epoch: 1 [7040/15000 (47%)]\tLoss: 2.126966\u001b[0m\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94m\u001b[33mTrain Epoch: 1 [7680/15000 (51%)]\tLoss: 1.785967\u001b[0m\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94m\u001b[33mTrain Epoch: 1 [8320/15000 (55%)]\tLoss: 1.627003\u001b[0m\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94m\u001b[33mTrain Epoch: 1 [8960/15000 (60%)]\tLoss: 1.565799\u001b[0m\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94m\u001b[33mTrain Epoch: 1 [9600/15000 (64%)]\tLoss: 1.500626\u001b[0m\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94m\u001b[33mTrain Epoch: 1 [10240/15000 (68%)]\tLoss: 1.401833\u001b[0m\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94m\u001b[33mTrain Epoch: 1 [10880/15000 (72%)]\tLoss: 1.420252\u001b[0m\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94m\u001b[33mTrain Epoch: 1 [11520/15000 (77%)]\tLoss: 1.384821\u001b[0m\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94m\u001b[33mTrain Epoch: 1 [12160/15000 (81%)]\tLoss: 1.381745\u001b[0m\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94m\u001b[33mTrain Epoch: 1 [12800/15000 (85%)]\tLoss: 1.270891\u001b[0m\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94m\u001b[33mTrain Epoch: 1 [13440/15000 (89%)]\tLoss: 1.026448\u001b[0m\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94m\u001b[33mTrain Epoch: 1 [14080/15000 (94%)]\tLoss: 0.919651\u001b[0m\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94m\u001b[33mTrain Epoch: 1 [14720/15000 (98%)]\tLoss: 1.308957\u001b[0m\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94mSaving data artifacts for train\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94mSaved data artifacts for train\u001b[0m\u001b[94m\n",
      "\u001b[0m\n",
      "Calling local_model_validation\n",
      "\u001b[94m\u001b[4m\u001b[35mTest set: Avg. loss: 0.7587, Accuracy: 1940/2500 (78%)\u001b[0m\u001b[0m\u001b[94m\n",
      "\n",
      "\u001b[0m\u001b[94m\u001b[97mDoing local model validation for collaborator Chandler:                 0.7760000228881836\u001b[0m\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94mSaving data artifacts for local_model_validation\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94mSaved data artifacts for local_model_validation\u001b[0m\u001b[94m\n",
      "\u001b[0mShould transfer from local_model_validation to join\n",
      "\n",
      "Calling aggregated_model_validation\n",
      "\u001b[94m\u001b[1m\u001b[31mPerforming aggregated model validation for collaborator Bangalore\u001b[0m\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94m\u001b[4m\u001b[35mTest set: Avg. loss: 2.3124, Accuracy: 225/2500 (9%)\u001b[0m\u001b[0m\u001b[94m\n",
      "\n",
      "\u001b[0m\u001b[94m\u001b[4m\u001b[31mBangalore value of 0.09000000357627869\u001b[0m\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94mSaving data artifacts for aggregated_model_validation\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94mSaved data artifacts for aggregated_model_validation\u001b[0m\u001b[94m\n",
      "\u001b[0m\n",
      "Calling train\n",
      "\u001b[94m\u001b[33mTrain Epoch: 1 [0/15000 (0%)]\tLoss: 2.320732\u001b[0m\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94m\u001b[33mTrain Epoch: 1 [640/15000 (4%)]\tLoss: 2.326915\u001b[0m\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94m\u001b[33mTrain Epoch: 1 [1280/15000 (9%)]\tLoss: 2.292917\u001b[0m\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94m\u001b[33mTrain Epoch: 1 [1920/15000 (13%)]\tLoss: 2.278403\u001b[0m\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94m\u001b[33mTrain Epoch: 1 [2560/15000 (17%)]\tLoss: 2.265009\u001b[0m\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94m\u001b[33mTrain Epoch: 1 [3200/15000 (21%)]\tLoss: 2.228839\u001b[0m\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94m\u001b[33mTrain Epoch: 1 [3840/15000 (26%)]\tLoss: 2.185376\u001b[0m\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94m\u001b[33mTrain Epoch: 1 [4480/15000 (30%)]\tLoss: 2.275832\u001b[0m\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94m\u001b[33mTrain Epoch: 1 [5120/15000 (34%)]\tLoss: 2.118941\u001b[0m\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94m\u001b[33mTrain Epoch: 1 [5760/15000 (38%)]\tLoss: 2.111933\u001b[0m\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94m\u001b[33mTrain Epoch: 1 [6400/15000 (43%)]\tLoss: 1.892352\u001b[0m\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94m\u001b[33mTrain Epoch: 1 [7040/15000 (47%)]\tLoss: 1.869729\u001b[0m\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94m\u001b[33mTrain Epoch: 1 [7680/15000 (51%)]\tLoss: 1.787822\u001b[0m\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94m\u001b[33mTrain Epoch: 1 [8320/15000 (55%)]\tLoss: 1.700193\u001b[0m\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94m\u001b[33mTrain Epoch: 1 [8960/15000 (60%)]\tLoss: 1.547075\u001b[0m\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94m\u001b[33mTrain Epoch: 1 [9600/15000 (64%)]\tLoss: 1.342015\u001b[0m\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94m\u001b[33mTrain Epoch: 1 [10240/15000 (68%)]\tLoss: 1.429067\u001b[0m\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94m\u001b[33mTrain Epoch: 1 [10880/15000 (72%)]\tLoss: 1.169153\u001b[0m\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94m\u001b[33mTrain Epoch: 1 [11520/15000 (77%)]\tLoss: 1.317978\u001b[0m\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94m\u001b[33mTrain Epoch: 1 [12160/15000 (81%)]\tLoss: 0.887466\u001b[0m\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94m\u001b[33mTrain Epoch: 1 [12800/15000 (85%)]\tLoss: 0.932854\u001b[0m\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94m\u001b[33mTrain Epoch: 1 [13440/15000 (89%)]\tLoss: 1.049332\u001b[0m\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94m\u001b[33mTrain Epoch: 1 [14080/15000 (94%)]\tLoss: 0.858403\u001b[0m\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94m\u001b[33mTrain Epoch: 1 [14720/15000 (98%)]\tLoss: 1.032934\u001b[0m\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94mSaving data artifacts for train\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94mSaved data artifacts for train\u001b[0m\u001b[94m\n",
      "\u001b[0m\n",
      "Calling local_model_validation\n",
      "\u001b[94m\u001b[4m\u001b[35mTest set: Avg. loss: 0.6315, Accuracy: 2103/2500 (84%)\u001b[0m\u001b[0m\u001b[94m\n",
      "\n",
      "\u001b[0m\u001b[94m\u001b[97mDoing local model validation for collaborator Bangalore:                 0.8411999940872192\u001b[0m\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94mSaving data artifacts for local_model_validation\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94mSaved data artifacts for local_model_validation\u001b[0m\u001b[94m\n",
      "\u001b[0mShould transfer from local_model_validation to join\n",
      "\n",
      "Calling join\n",
      "\u001b[94m\u001b[32mAverage aggregated model validation values =                 0.09530000202357769\u001b[0m\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94m\u001b[32mAverage training loss = 1.093075156211853\u001b[0m\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94m\u001b[32mAverage local model validation values =             0.8121000081300735\u001b[0m\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94mSaving data artifacts for join\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94mSaved data artifacts for join\u001b[0m\u001b[94m\n",
      "\u001b[0m\n",
      "Calling aggregated_model_validation\n",
      "\u001b[94m\u001b[1m\u001b[31mPerforming aggregated model validation for collaborator Portland\u001b[0m\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94m\u001b[4m\u001b[35mTest set: Avg. loss: 0.6687, Accuracy: 2059/2500 (82%)\u001b[0m\u001b[0m\u001b[94m\n",
      "\n",
      "\u001b[0m\u001b[94m\u001b[4m\u001b[31mPortland value of 0.8235999941825867\u001b[0m\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94mSaving data artifacts for aggregated_model_validation\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94mSaved data artifacts for aggregated_model_validation\u001b[0m\u001b[94m\n",
      "\u001b[0m\n",
      "Calling train\n",
      "\u001b[94m\u001b[33mTrain Epoch: 1 [0/15000 (0%)]\tLoss: 1.117194\u001b[0m\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94m\u001b[33mTrain Epoch: 1 [640/15000 (4%)]\tLoss: 0.959902\u001b[0m\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94m\u001b[33mTrain Epoch: 1 [1280/15000 (9%)]\tLoss: 1.162763\u001b[0m\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94m\u001b[33mTrain Epoch: 1 [1920/15000 (13%)]\tLoss: 0.985242\u001b[0m\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94m\u001b[33mTrain Epoch: 1 [2560/15000 (17%)]\tLoss: 0.925464\u001b[0m\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94m\u001b[33mTrain Epoch: 1 [3200/15000 (21%)]\tLoss: 1.106375\u001b[0m\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94m\u001b[33mTrain Epoch: 1 [3840/15000 (26%)]\tLoss: 0.976756\u001b[0m\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94m\u001b[33mTrain Epoch: 1 [4480/15000 (30%)]\tLoss: 0.581847\u001b[0m\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94m\u001b[33mTrain Epoch: 1 [5120/15000 (34%)]\tLoss: 0.652471\u001b[0m\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94m\u001b[33mTrain Epoch: 1 [5760/15000 (38%)]\tLoss: 0.746739\u001b[0m\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94m\u001b[33mTrain Epoch: 1 [6400/15000 (43%)]\tLoss: 0.905862\u001b[0m\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94m\u001b[33mTrain Epoch: 1 [7040/15000 (47%)]\tLoss: 0.982412\u001b[0m\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94m\u001b[33mTrain Epoch: 1 [7680/15000 (51%)]\tLoss: 1.031451\u001b[0m\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94m\u001b[33mTrain Epoch: 1 [8320/15000 (55%)]\tLoss: 0.864737\u001b[0m\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94m\u001b[33mTrain Epoch: 1 [8960/15000 (60%)]\tLoss: 0.720579\u001b[0m\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94m\u001b[33mTrain Epoch: 1 [9600/15000 (64%)]\tLoss: 0.879089\u001b[0m\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94m\u001b[33mTrain Epoch: 1 [10240/15000 (68%)]\tLoss: 0.833290\u001b[0m\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94m\u001b[33mTrain Epoch: 1 [10880/15000 (72%)]\tLoss: 0.992734\u001b[0m\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94m\u001b[33mTrain Epoch: 1 [11520/15000 (77%)]\tLoss: 0.845350\u001b[0m\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94m\u001b[33mTrain Epoch: 1 [12160/15000 (81%)]\tLoss: 1.010064\u001b[0m\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94m\u001b[33mTrain Epoch: 1 [12800/15000 (85%)]\tLoss: 0.612797\u001b[0m\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94m\u001b[33mTrain Epoch: 1 [13440/15000 (89%)]\tLoss: 0.965815\u001b[0m\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94m\u001b[33mTrain Epoch: 1 [14080/15000 (94%)]\tLoss: 0.671353\u001b[0m\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94m\u001b[33mTrain Epoch: 1 [14720/15000 (98%)]\tLoss: 0.782204\u001b[0m\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94mSaving data artifacts for train\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94mSaved data artifacts for train\u001b[0m\u001b[94m\n",
      "\u001b[0m\n",
      "Calling local_model_validation\n",
      "\u001b[94m\u001b[4m\u001b[35mTest set: Avg. loss: 0.3774, Accuracy: 2233/2500 (89%)\u001b[0m\u001b[0m\u001b[94m\n",
      "\n",
      "\u001b[0m\u001b[94m\u001b[97mDoing local model validation for collaborator Portland:                 0.8931999802589417\u001b[0m\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94mSaving data artifacts for local_model_validation\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94mSaved data artifacts for local_model_validation\u001b[0m\u001b[94m\n",
      "\u001b[0mShould transfer from local_model_validation to join\n",
      "\n",
      "Calling aggregated_model_validation\n",
      "\u001b[94m\u001b[1m\u001b[31mPerforming aggregated model validation for collaborator Seattle\u001b[0m\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94m\u001b[4m\u001b[35mTest set: Avg. loss: 0.6649, Accuracy: 2082/2500 (83%)\u001b[0m\u001b[0m\u001b[94m\n",
      "\n",
      "\u001b[0m\u001b[94m\u001b[4m\u001b[31mSeattle value of 0.8327999711036682\u001b[0m\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94mSaving data artifacts for aggregated_model_validation\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94mSaved data artifacts for aggregated_model_validation\u001b[0m\u001b[94m\n",
      "\u001b[0m\n",
      "Calling train\n",
      "\u001b[94m\u001b[33mTrain Epoch: 1 [0/15000 (0%)]\tLoss: 0.837370\u001b[0m\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94m\u001b[33mTrain Epoch: 1 [640/15000 (4%)]\tLoss: 1.123712\u001b[0m\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94m\u001b[33mTrain Epoch: 1 [1280/15000 (9%)]\tLoss: 0.992106\u001b[0m\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94m\u001b[33mTrain Epoch: 1 [1920/15000 (13%)]\tLoss: 0.860248\u001b[0m\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94m\u001b[33mTrain Epoch: 1 [2560/15000 (17%)]\tLoss: 1.017820\u001b[0m\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94m\u001b[33mTrain Epoch: 1 [3200/15000 (21%)]\tLoss: 0.777438\u001b[0m\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94m\u001b[33mTrain Epoch: 1 [3840/15000 (26%)]\tLoss: 0.945590\u001b[0m\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94m\u001b[33mTrain Epoch: 1 [4480/15000 (30%)]\tLoss: 0.726948\u001b[0m\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94m\u001b[33mTrain Epoch: 1 [5120/15000 (34%)]\tLoss: 0.516470\u001b[0m\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94m\u001b[33mTrain Epoch: 1 [5760/15000 (38%)]\tLoss: 0.683781\u001b[0m\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94m\u001b[33mTrain Epoch: 1 [6400/15000 (43%)]\tLoss: 0.837656\u001b[0m\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94m\u001b[33mTrain Epoch: 1 [7040/15000 (47%)]\tLoss: 0.769630\u001b[0m\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94m\u001b[33mTrain Epoch: 1 [7680/15000 (51%)]\tLoss: 0.727186\u001b[0m\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94m\u001b[33mTrain Epoch: 1 [8320/15000 (55%)]\tLoss: 0.654110\u001b[0m\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94m\u001b[33mTrain Epoch: 1 [8960/15000 (60%)]\tLoss: 0.585913\u001b[0m\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94m\u001b[33mTrain Epoch: 1 [9600/15000 (64%)]\tLoss: 0.654798\u001b[0m\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94m\u001b[33mTrain Epoch: 1 [10240/15000 (68%)]\tLoss: 0.532826\u001b[0m\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94m\u001b[33mTrain Epoch: 1 [10880/15000 (72%)]\tLoss: 0.569897\u001b[0m\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94m\u001b[33mTrain Epoch: 1 [11520/15000 (77%)]\tLoss: 0.459862\u001b[0m\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94m\u001b[33mTrain Epoch: 1 [12160/15000 (81%)]\tLoss: 0.580754\u001b[0m\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94m\u001b[33mTrain Epoch: 1 [12800/15000 (85%)]\tLoss: 0.499258\u001b[0m\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94m\u001b[33mTrain Epoch: 1 [13440/15000 (89%)]\tLoss: 0.706480\u001b[0m\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94m\u001b[33mTrain Epoch: 1 [14080/15000 (94%)]\tLoss: 0.767768\u001b[0m\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94m\u001b[33mTrain Epoch: 1 [14720/15000 (98%)]\tLoss: 0.572736\u001b[0m\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94mSaving data artifacts for train\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94mSaved data artifacts for train\u001b[0m\u001b[94m\n",
      "\u001b[0m\n",
      "Calling local_model_validation\n",
      "\u001b[94m\u001b[4m\u001b[35mTest set: Avg. loss: 0.3535, Accuracy: 2249/2500 (90%)\u001b[0m\u001b[0m\u001b[94m\n",
      "\n",
      "\u001b[0m\u001b[94m\u001b[97mDoing local model validation for collaborator Seattle:                 0.8996000289916992\u001b[0m\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94mSaving data artifacts for local_model_validation\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94mSaved data artifacts for local_model_validation\u001b[0m\u001b[94m\n",
      "\u001b[0mShould transfer from local_model_validation to join\n",
      "\n",
      "Calling aggregated_model_validation\n",
      "\u001b[94m\u001b[1m\u001b[31mPerforming aggregated model validation for collaborator Chandler\u001b[0m\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94m\u001b[4m\u001b[35mTest set: Avg. loss: 0.6786, Accuracy: 2071/2500 (83%)\u001b[0m\u001b[0m\u001b[94m\n",
      "\n",
      "\u001b[0m\u001b[94m\u001b[4m\u001b[31mChandler value of 0.8284000158309937\u001b[0m\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94mSaving data artifacts for aggregated_model_validation\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94mSaved data artifacts for aggregated_model_validation\u001b[0m\u001b[94m\n",
      "\u001b[0m\n",
      "Calling train\n",
      "\u001b[94m\u001b[33mTrain Epoch: 1 [0/15000 (0%)]\tLoss: 0.978471\u001b[0m\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94m\u001b[33mTrain Epoch: 1 [640/15000 (4%)]\tLoss: 1.302421\u001b[0m\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94m\u001b[33mTrain Epoch: 1 [1280/15000 (9%)]\tLoss: 0.975873\u001b[0m\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94m\u001b[33mTrain Epoch: 1 [1920/15000 (13%)]\tLoss: 1.094498\u001b[0m\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94m\u001b[33mTrain Epoch: 1 [2560/15000 (17%)]\tLoss: 0.906383\u001b[0m\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94m\u001b[33mTrain Epoch: 1 [3200/15000 (21%)]\tLoss: 0.963470\u001b[0m\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94m\u001b[33mTrain Epoch: 1 [3840/15000 (26%)]\tLoss: 0.681538\u001b[0m\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94m\u001b[33mTrain Epoch: 1 [4480/15000 (30%)]\tLoss: 0.710819\u001b[0m\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94m\u001b[33mTrain Epoch: 1 [5120/15000 (34%)]\tLoss: 1.072829\u001b[0m\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94m\u001b[33mTrain Epoch: 1 [5760/15000 (38%)]\tLoss: 1.063428\u001b[0m\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94m\u001b[33mTrain Epoch: 1 [6400/15000 (43%)]\tLoss: 1.040274\u001b[0m\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94m\u001b[33mTrain Epoch: 1 [7040/15000 (47%)]\tLoss: 0.830312\u001b[0m\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94m\u001b[33mTrain Epoch: 1 [7680/15000 (51%)]\tLoss: 1.013636\u001b[0m\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94m\u001b[33mTrain Epoch: 1 [8320/15000 (55%)]\tLoss: 0.864681\u001b[0m\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94m\u001b[33mTrain Epoch: 1 [8960/15000 (60%)]\tLoss: 0.763160\u001b[0m\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94m\u001b[33mTrain Epoch: 1 [9600/15000 (64%)]\tLoss: 0.872470\u001b[0m\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94m\u001b[33mTrain Epoch: 1 [10240/15000 (68%)]\tLoss: 0.623707\u001b[0m\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94m\u001b[33mTrain Epoch: 1 [10880/15000 (72%)]\tLoss: 0.762739\u001b[0m\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94m\u001b[33mTrain Epoch: 1 [11520/15000 (77%)]\tLoss: 0.772892\u001b[0m\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94m\u001b[33mTrain Epoch: 1 [12160/15000 (81%)]\tLoss: 0.600553\u001b[0m\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94m\u001b[33mTrain Epoch: 1 [12800/15000 (85%)]\tLoss: 0.786087\u001b[0m\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94m\u001b[33mTrain Epoch: 1 [13440/15000 (89%)]\tLoss: 0.815889\u001b[0m\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94m\u001b[33mTrain Epoch: 1 [14080/15000 (94%)]\tLoss: 0.455025\u001b[0m\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94m\u001b[33mTrain Epoch: 1 [14720/15000 (98%)]\tLoss: 0.728822\u001b[0m\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94mSaving data artifacts for train\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94mSaved data artifacts for train\u001b[0m\u001b[94m\n",
      "\u001b[0m\n",
      "Calling local_model_validation\n",
      "\u001b[94m\u001b[4m\u001b[35mTest set: Avg. loss: 0.3959, Accuracy: 2207/2500 (88%)\u001b[0m\u001b[0m\u001b[94m\n",
      "\n",
      "\u001b[0m\u001b[94m\u001b[97mDoing local model validation for collaborator Chandler:                 0.8827999830245972\u001b[0m\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94mSaving data artifacts for local_model_validation\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94mSaved data artifacts for local_model_validation\u001b[0m\u001b[94m\n",
      "\u001b[0mShould transfer from local_model_validation to join\n",
      "\n",
      "Calling aggregated_model_validation\n",
      "\u001b[94m\u001b[1m\u001b[31mPerforming aggregated model validation for collaborator Bangalore\u001b[0m\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94m\u001b[4m\u001b[35mTest set: Avg. loss: 0.6516, Accuracy: 2069/2500 (83%)\u001b[0m\u001b[0m\u001b[94m\n",
      "\n",
      "\u001b[0m\u001b[94m\u001b[4m\u001b[31mBangalore value of 0.8276000022888184\u001b[0m\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94mSaving data artifacts for aggregated_model_validation\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94mSaved data artifacts for aggregated_model_validation\u001b[0m\u001b[94m\n",
      "\u001b[0m\n",
      "Calling train\n",
      "\u001b[94m\u001b[33mTrain Epoch: 1 [0/15000 (0%)]\tLoss: 0.958682\u001b[0m\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94m\u001b[33mTrain Epoch: 1 [640/15000 (4%)]\tLoss: 0.827428\u001b[0m\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94m\u001b[33mTrain Epoch: 1 [1280/15000 (9%)]\tLoss: 1.210269\u001b[0m\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94m\u001b[33mTrain Epoch: 1 [1920/15000 (13%)]\tLoss: 0.913601\u001b[0m\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94m\u001b[33mTrain Epoch: 1 [2560/15000 (17%)]\tLoss: 0.784394\u001b[0m\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94m\u001b[33mTrain Epoch: 1 [3200/15000 (21%)]\tLoss: 0.763703\u001b[0m\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94m\u001b[33mTrain Epoch: 1 [3840/15000 (26%)]\tLoss: 0.865526\u001b[0m\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94m\u001b[33mTrain Epoch: 1 [4480/15000 (30%)]\tLoss: 0.674384\u001b[0m\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94m\u001b[33mTrain Epoch: 1 [5120/15000 (34%)]\tLoss: 0.601442\u001b[0m\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94m\u001b[33mTrain Epoch: 1 [5760/15000 (38%)]\tLoss: 1.131632\u001b[0m\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94m\u001b[33mTrain Epoch: 1 [6400/15000 (43%)]\tLoss: 0.663594\u001b[0m\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94m\u001b[33mTrain Epoch: 1 [7040/15000 (47%)]\tLoss: 0.751783\u001b[0m\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94m\u001b[33mTrain Epoch: 1 [7680/15000 (51%)]\tLoss: 0.617137\u001b[0m\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94m\u001b[33mTrain Epoch: 1 [8320/15000 (55%)]\tLoss: 0.520232\u001b[0m\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94m\u001b[33mTrain Epoch: 1 [8960/15000 (60%)]\tLoss: 0.541307\u001b[0m\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94m\u001b[33mTrain Epoch: 1 [9600/15000 (64%)]\tLoss: 0.708963\u001b[0m\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94m\u001b[33mTrain Epoch: 1 [10240/15000 (68%)]\tLoss: 0.693652\u001b[0m\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94m\u001b[33mTrain Epoch: 1 [10880/15000 (72%)]\tLoss: 0.963873\u001b[0m\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94m\u001b[33mTrain Epoch: 1 [11520/15000 (77%)]\tLoss: 0.814014\u001b[0m\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94m\u001b[33mTrain Epoch: 1 [12160/15000 (81%)]\tLoss: 0.601377\u001b[0m\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94m\u001b[33mTrain Epoch: 1 [12800/15000 (85%)]\tLoss: 0.557420\u001b[0m\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94m\u001b[33mTrain Epoch: 1 [13440/15000 (89%)]\tLoss: 0.597357\u001b[0m\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94m\u001b[33mTrain Epoch: 1 [14080/15000 (94%)]\tLoss: 0.509475\u001b[0m\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94m\u001b[33mTrain Epoch: 1 [14720/15000 (98%)]\tLoss: 0.667526\u001b[0m\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94mSaving data artifacts for train\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94mSaved data artifacts for train\u001b[0m\u001b[94m\n",
      "\u001b[0m\n",
      "Calling local_model_validation\n",
      "\u001b[94m\u001b[4m\u001b[35mTest set: Avg. loss: 0.3583, Accuracy: 2226/2500 (89%)\u001b[0m\u001b[0m\u001b[94m\n",
      "\n",
      "\u001b[0m\u001b[94m\u001b[97mDoing local model validation for collaborator Bangalore:                 0.8903999924659729\u001b[0m\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94mSaving data artifacts for local_model_validation\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94mSaved data artifacts for local_model_validation\u001b[0m\u001b[94m\n",
      "\u001b[0mShould transfer from local_model_validation to join\n",
      "\n",
      "Calling join\n",
      "\u001b[94m\u001b[32mAverage aggregated model validation values =                 0.8280999958515167\u001b[0m\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94m\u001b[32mAverage training loss = 0.6878219842910767\u001b[0m\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94m\u001b[32mAverage local model validation values =             0.8914999961853027\u001b[0m\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94mSaving data artifacts for join\u001b[0m\u001b[94m\n",
      "\u001b[0m\u001b[94mSaved data artifacts for join\u001b[0m\u001b[94m\n",
      "\u001b[0m\n",
      "Calling end\n",
      "\u001b[94m\u001b[30mThis is the end of the flow\u001b[0m\u001b[0m\u001b[94m\n",
      "\u001b[0mSaving data artifacts for end\n",
      "Saved data artifacts for end\n"
     ]
    }
   ],
   "source": [
    "model = None\n",
    "best_model = None\n",
    "optimizer = None\n",
    "flflow = FederatedFlow(model, optimizer, rounds=2, checkpoint=True)\n",
    "flflow.runtime = local_runtime\n",
    "flflow.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "dc503676",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample of the final model weights: tensor([[[-0.0246,  0.0669,  0.2033,  0.3637,  0.1195],\n",
      "         [-0.1677,  0.0307,  0.3475,  0.3762,  0.3048],\n",
      "         [ 0.0829,  0.1147,  0.1561,  0.2371, -0.0464],\n",
      "         [-0.0757,  0.0008,  0.2846,  0.1775, -0.0356],\n",
      "         [ 0.1617,  0.0923,  0.0763,  0.1791,  0.0776]]])\n",
      "\n",
      "Final aggregated model accuracy for 2 rounds of training: 0.8280999958515167\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "    f'Sample of the final model weights: {flflow.model.state_dict()[\"conv1.weight\"][0]}'\n",
    ")\n",
    "\n",
    "print(\n",
    "    f\"\\nFinal aggregated model accuracy for {flflow.rounds} rounds of training: {flflow.aggregated_model_accuracy}\"\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
