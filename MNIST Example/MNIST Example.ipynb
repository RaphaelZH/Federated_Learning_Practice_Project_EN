{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a66a6430",
   "metadata": {},
   "source": [
    "**Table of contents**<a id='toc0_'></a>    \n",
    "- [Importation des bibliothèques](#toc1_)    \n",
    "  - [Importation des paquets ou modules de la bibliothèque OpenFL](#toc1_1_)    \n",
    "  - [Importation des paquets ou modules de la bibliothèque PyTorch](#toc1_2_)    \n",
    "  - [Importation d’autres paquets ou modules requis](#toc1_3_)    \n",
    "- [Définition du modèle d‘entraînement](#toc2_)    \n",
    "  - [Définition des chargeurs de données](#toc2_1_)    \n",
    "  - [Définition du modèle de réseau CNN](#toc2_2_)    \n",
    "  - [Définition de la fonction d'inférence utilisée dans le test](#toc2_3_)    \n",
    "- [Définition des règles de l'apprentissage fédéré](#toc3_)    \n",
    "  - [Méthode de calcul de la moyenne des poids d'apprentissage fédéré](#toc3_1_)    \n",
    "\n",
    "<!-- vscode-jupyter-toc-config\n",
    "\tnumbering=false\n",
    "\tanchor=true\n",
    "\tflat=false\n",
    "\tminLevel=1\n",
    "\tmaxLevel=6\n",
    "\t/vscode-jupyter-toc-config -->\n",
    "<!-- THIS CELL WILL BE REPLACED ON TOC UPDATE. DO NOT WRITE YOUR TEXT IN THIS CELL -->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e7fe1d1",
   "metadata": {},
   "source": [
    "# <a id='toc1_'></a>[Importation des bibliothèques](#toc0_)\n",
    "\n",
    "## <a id='toc1_1_'></a>[Importation des paquets ou modules de la bibliothèque OpenFL](#toc0_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4f63aa4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openfl.experimental.workflow.interface import Aggregator, Collaborator, FLSpec\n",
    "from openfl.experimental.workflow.placement import aggregator, collaborator\n",
    "from openfl.experimental.workflow.runtime import LocalRuntime"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c852c64d",
   "metadata": {},
   "source": [
    "## <a id='toc1_2_'></a>[Importation des paquets ou modules de la bibliothèque PyTorch](#toc0_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2b89c419",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from torchsummary import summary\n",
    "\n",
    "import torchvision"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60958c33",
   "metadata": {},
   "source": [
    "## <a id='toc1_3_'></a>[Importation d’autres paquets ou modules requis](#toc0_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3d76255d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from termcolor import cprint"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22594881",
   "metadata": {},
   "source": [
    "# <a id='toc2_'></a>[Définition du modèle d‘entraînement](#toc0_)\n",
    "\n",
    "## <a id='toc2_1_'></a>[Définition des chargeurs de données](#toc0_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "67ffab47",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "01. torchvision.transforms.Compose(transforms)\n",
    "    - Composes several transforms together.\n",
    "\n",
    "02. torchvision.transforms.Normalize(mean, std, inplace=False)\n",
    "    - Normalize a tensor image with mean and standard deviation.\n",
    "    - output[channel] = (input[channel] - mean[channel]) / std[channel]\n",
    "\"\"\"\n",
    "\n",
    "mnist_train = torchvision.datasets.MNIST(\n",
    "    \"/tmp/files/\",\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=torchvision.transforms.Compose(\n",
    "        [\n",
    "            torchvision.transforms.ToTensor(),\n",
    "            # Les valeurs ` 0.1307` et `0.3081` utilisées pour la transformation `Normalize()`\n",
    "            # ci-dessous sont la moyenne globale et l’écart-type de l’ensemble de données MNIST.\n",
    "            torchvision.transforms.Normalize((0.1307,), (0.3081,)),\n",
    "        ]\n",
    "    ),\n",
    ")\n",
    "\n",
    "mnist_test = torchvision.datasets.MNIST(\n",
    "    \"/tmp/files/\",\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=torchvision.transforms.Compose(\n",
    "        [\n",
    "            torchvision.transforms.ToTensor(),\n",
    "            torchvision.transforms.Normalize((0.1307,), (0.3081,)),\n",
    "        ]\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60eaf1c0",
   "metadata": {},
   "source": [
    "## <a id='toc2_2_'></a>[Définition du modèle de réseau CNN](#toc0_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cba3007c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "03. torch.nn.Conv2d(in_channels, out_channels, kernel_size, stride=1, padding=0, dilation=1,\n",
    "    groups=1, bias=True, padding_mode='zeros', device=None, dtype=None)\n",
    "    - Applies a 2D convolution over an input signal composed of several input planes.\n",
    "\n",
    "04. torch.nn.Dropout2d(p=0.5, inplace=False)\n",
    "    - Randomly zero out entire channels.\n",
    "    - Each channel will be zeroed out independently on every forward call with probability p using\n",
    "    samples from a Bernoulli distribution.\n",
    "\n",
    "05. torch.nn.functional.max_pool2d(input, kernel_size, stride=None, padding=0,\n",
    "    dilation=1, ceil_mode=False, return_indices=False)\n",
    "    - Applies a 2D max pooling over an input signal composed of several input planes.\n",
    "\n",
    "06. torch.nn.functional.dropout(input, p=0.5, training=True, inplace=False)\n",
    "    - During training, randomly zeroes some elements of the input tensor with probability p.\n",
    "    - Uses samples from a Bernoulli distribution.\n",
    "\n",
    "07. torch.nn.functional.log_softmax(input, dim=None, _stacklevel=3, dtype=None)\n",
    "    - Apply a softmax followed by a logarithm.\n",
    "    - While mathematically equivalent to log(softmax(x)), doing these two operations separately is\n",
    "    slower and numerically unstable. This function uses an alternative formulation to compute the\n",
    "    output and gradient correctly.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        # La première couche convolutionnelle : le nombre de canaux d’entrée est de 1, c’est-à-dire\n",
    "        # une image en niveaux de gris, le nombre de canaux de sortie est de 10, la taille du filtre\n",
    "        # convolutif est de 5x5, le stride est de 1 et le padding est de 0.\n",
    "\n",
    "        # Par conséquent, après que l'image d'entrée (1x28x28) a été convoluée, la taille de la\n",
    "        # carte de caractéristiques de sortie est de 10x24x24.\n",
    "        self.conv1 = nn.Conv2d(1, 10, kernel_size=5)\n",
    "        # La deuxième couche convolutionnelle : le nombre de canaux d’entrée est de 10, le nombre de\n",
    "        # canaux de sortie est de 20, la taille du filtre convolutif est de 5x5, le stride est de 1\n",
    "        # et le padding est de 0.\n",
    "\n",
    "        # Par conséquent, après que l'image d'entrée (10x12x12) a été convoluée, la taille de la\n",
    "        # carte de caractéristiques de sortie est de 20x8x8.\n",
    "        self.conv2 = nn.Conv2d(10, 20, kernel_size=5)\n",
    "        # Une couche d'abandon.\n",
    "        self.conv2_drop = nn.Dropout2d()\n",
    "        # La première couche de fully connected : le nombre de canaux d’entrée est de 20, chaque\n",
    "        # canal a une taille de 4x4, soit un total de 20x4x4 = 320 nœuds, tandis que la sortie est\n",
    "        # fixée à 50 nœuds.\n",
    "        self.fc1 = nn.Linear(320, 50)\n",
    "        # La deuxième couche de fully connected : l'entrée a 50 nœuds et la sortie a 10 nœuds\n",
    "        # (correspondant à 10 catégories).\n",
    "        self.fc2 = nn.Linear(50, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # La première couche convolutive est suivie d'une couche de pooling de type max pooling avec\n",
    "        # un filtre convolutif de taille de 2x2 et un stride égal à la longueur du filtre.\n",
    "\n",
    "        # La taille d'entrée est de 10x24x24, et après pooling, la taille de sortie est de 10x12x12.\n",
    "        x = F.relu(F.max_pool2d(self.conv1(x), 2))\n",
    "        # La deuxième couche convolutive est suivie d'une couche d'abandon.\n",
    "\n",
    "        # Après la couche d'abandon, suit une autre couche de max-pooling, qui possède un filtre\n",
    "        # convolutif de taille de 2x2 et un stride aussi égal à la longueur du filtre.\n",
    "\n",
    "        # La taille d'entrée est de 20x8x8, et après pooling, la taille de sortie est de 20x4x4.\n",
    "        x = F.relu(F.max_pool2d(self.conv2_drop(self.conv2(x)), 2))\n",
    "        # La carte de caractéristiques multidimensionnelle est transformée en un vecteur\n",
    "        # unidimensionnel, d'une taille de 20x4x4 = 320.\n",
    "        x = x.view(-1, 320)\n",
    "        # La première couche de fully connected, activée par la fonction d'activation ReLU.\n",
    "        x = F.relu(self.fc1(x))\n",
    "        # Pendant l'entraînement du modèle, certains nœuds de la sortie de la première couche de\n",
    "        # fully connected sont mis à zéro de manière aléatoire avec une probabilité p.\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        # La deuxième couche de fully connected sert également de couche de sortie.\n",
    "        x = self.fc2(x)\n",
    "        # Les probabilités logarithmiques de tous les nœuds de la couche de sortie sont calculées en\n",
    "        # appliquant une fonction softmax suivie d'un logarithme.\n",
    "        return F.log_softmax(x, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "01738796",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1           [-1, 10, 24, 24]             260\n",
      "            Conv2d-2             [-1, 20, 8, 8]           5,020\n",
      "         Dropout2d-3             [-1, 20, 8, 8]               0\n",
      "            Linear-4                   [-1, 50]          16,050\n",
      "            Linear-5                   [-1, 10]             510\n",
      "================================================================\n",
      "Total params: 21,840\n",
      "Trainable params: 21,840\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.00\n",
      "Forward/backward pass size (MB): 0.06\n",
      "Params size (MB): 0.08\n",
      "Estimated Total Size (MB): 0.15\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "model = Net()\n",
    "summary(model, next(iter(mnist_train))[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7933b052",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[35m  conv1.weight .............................................. 250  \u001b[0m\n",
      "\u001b[35m  conv1.bias ................................................. 10  \u001b[0m\n",
      "\u001b[35m  conv2.weight ............................................. 5000  \u001b[0m\n",
      "\u001b[35m  conv2.bias ................................................. 20  \u001b[0m\n",
      "\u001b[35m  fc1.weight .............................................. 16000  \u001b[0m\n",
      "\u001b[35m  fc1.bias ................................................... 50  \u001b[0m\n",
      "\u001b[35m  fc2.weight ................................................ 500  \u001b[0m\n",
      "\u001b[35m  fc2.bias ................................................... 10  \u001b[0m\n",
      "\u001b[35m _________________________________________________________________ \u001b[0m\n",
      "\u001b[35m  total parameters ........................................ 21840  \u001b[0m\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def count_parameters(model):\n",
    "    length = 67\n",
    "    names = [n for (n, p) in model.named_parameters() if p.requires_grad]\n",
    "    name = \"total parameters\"\n",
    "    names.append(name)\n",
    "    max_length = max(map(len, names))\n",
    "    formatted_names = [f\"{f'  {n} ':.<{max_length + 3}}\" for n in names]\n",
    "    params = [p.numel() for p in model.parameters() if p.requires_grad]\n",
    "    params.append(sum(params))\n",
    "    formatted_params = [f\"{f' {p}  ':.>{length - max_length - 3}}\" for p in params]\n",
    "\n",
    "    for n, p in zip(formatted_names[:-1], formatted_params[:-1]):\n",
    "        cprint((n + p), \"magenta\")\n",
    "    cprint(\" \" + \"_\" * (length - 2) + \" \", \"magenta\")\n",
    "    cprint(\n",
    "        (formatted_names[-1] + formatted_params[-1]),\n",
    "        \"magenta\",\n",
    "        end=\"\\n\\n\",\n",
    "    )\n",
    "\n",
    "    return names, params\n",
    "\n",
    "\n",
    "names, params = count_parameters(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3675674",
   "metadata": {},
   "source": [
    "## <a id='toc2_3_'></a>[Définition de la fonction d'inférence utilisée dans le test](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "13b360d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "08. torch.nn.functional.nll_loss(input, target, weight=None, size_average=None, ignore_index=-100,\n",
    "    reduce=None, reduction='mean')\n",
    "    - Compute the negative log likelihood loss.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def inference(network, test_loader):\n",
    "    # Mettre le module en mode évaluation.\n",
    "    network.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            output = network(data)\n",
    "            # L'entropie est une mesure de l'incertitude, c'est-à-dire que, si un résultat est\n",
    "            # ertain, l'entropie est faible.\n",
    "\n",
    "            # La perte d'entropie croisée, ou perte logarithmique mesure les performances d'un\n",
    "            # modèle de classification dont le résultat est une valeur de probabilité comprise entre\n",
    "            # 0 et 1.\n",
    "\n",
    "            # La perte d'entropie croisée augmente à mesure que la probabilité prédite s'écarte de\n",
    "            # l'étiquette réelle.\n",
    "\n",
    "            # L’entropie croisée catégorielle sert au classement en plusieurs classes.\n",
    "\n",
    "            # Le log-vraisemblance négatif est également connu sous le nom d'entropie croisée\n",
    "            # catégorielle, car il s'agit en fait de deux interprétations différentes de la même\n",
    "            # formule.\n",
    "\n",
    "            # test_loss += F.cross_entropy(output, target, reduction=\"sum\").item()\n",
    "            test_loss += F.nll_loss(output, target, reduction=\"sum\").item()\n",
    "\n",
    "            # Si `keepdim` est `True`, le tenseur de sortie est de la même taille que celui\n",
    "            # d'entrée, sauf dans la (les) dimension(s) `dim` où il est de taille 1.\n",
    "            pred = output.data.max(dim=1, keepdim=True)[1]\n",
    "            # Calcul de l'égalité par éléments.\n",
    "            correct += pred.eq(target.data.view_as(pred)).sum()\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    cprint(\n",
    "        \"Test set: Avg. loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\".format(\n",
    "            test_loss,\n",
    "            correct,\n",
    "            len(test_loader.dataset),\n",
    "            100.0 * correct / len(test_loader.dataset),\n",
    "        ),\n",
    "        \"magenta\",\n",
    "        attrs=[\"bold\"],\n",
    "        end=\"\\n\\n\",\n",
    "    )\n",
    "    accuracy = float(correct / len(test_loader.dataset))\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7f8c156a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[35mTest set: Avg. loss: 2.3068, Accuracy: 814/10000 (8%)\u001b[0m\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.08139999955892563"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_loader = DataLoader(mnist_test, batch_size=500, shuffle=False)\n",
    "\n",
    "inference(model, test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bc71b0f",
   "metadata": {},
   "source": [
    "# <a id='toc3_'></a>[Définition des règles de l'apprentissage fédéré](#toc0_)\n",
    "\n",
    "## <a id='toc3_1_'></a>[Méthode de calcul de la moyenne des poids d'apprentissage fédéré](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "59d93477",
   "metadata": {},
   "outputs": [],
   "source": [
    "def FedAvg(models, weights=None):\n",
    "    new_model = models[0]\n",
    "    state_dicts = [model.state_dict() for model in models]\n",
    "    state_dict = new_model.state_dict()\n",
    "    for key in models[1].state_dict():\n",
    "        state_dict[key] = torch.from_numpy(\n",
    "            np.average(\n",
    "                [state[key].numpy() for state in state_dicts], axis=0, weights=weights\n",
    "            )\n",
    "        )\n",
    "    new_model.load_state_dict(state_dict)\n",
    "    return new_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "670a3019",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keys = [k for k in model.state_dict().keys()]\n",
    "names = [n for (n, p) in model.named_parameters() if p.requires_grad]\n",
    "keys == names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3d20ab8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FederatedFlow(FLSpec):\n",
    "\n",
    "    def __init__(self, model=None, optimizer=None, rounds=3, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        if model is not None:\n",
    "            self.model = model\n",
    "            self.optimizer = optimizer\n",
    "        else:\n",
    "            self.model = Net()\n",
    "            self.optimizer = optim.SGD(\n",
    "                self.model.parameters(), lr=learning_rate, momentum=momentum\n",
    "            )\n",
    "        self.rounds = rounds\n",
    "\n",
    "    @aggregator\n",
    "    def start(self):\n",
    "        print(f\"Performing initialization for model\")\n",
    "        self.collaborators = self.runtime.collaborators\n",
    "        self.private = 10\n",
    "        self.current_round = 0\n",
    "        self.next(\n",
    "            self.aggregated_model_validation,\n",
    "            foreach=\"collaborators\",\n",
    "            exclude=[\"private\"],\n",
    "        )\n",
    "\n",
    "    @collaborator\n",
    "    def aggregated_model_validation(self):\n",
    "        print(f\"Performing aggregated model validation for collaborator {self.input}\")\n",
    "        self.agg_validation_score = inference(self.model, self.test_loader)\n",
    "        print(f\"{self.input} value of {self.agg_validation_score}\")\n",
    "        self.next(self.train)\n",
    "\n",
    "    @collaborator\n",
    "    def train(self):\n",
    "        self.model.train()\n",
    "        self.optimizer = optim.SGD(\n",
    "            self.model.parameters(), lr=learning_rate, momentum=momentum\n",
    "        )\n",
    "        train_losses = []\n",
    "        for batch_idx, (data, target) in enumerate(self.train_loader):\n",
    "            self.optimizer.zero_grad()\n",
    "            output = self.model(data)\n",
    "            loss = F.nll_loss(output, target)\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "            if batch_idx % log_interval == 0:\n",
    "                print(\n",
    "                    \"Train Epoch: 1 [{}/{} ({:.0f}%)]\\tLoss: {:.6f}\".format(\n",
    "                        batch_idx * len(data),\n",
    "                        len(self.train_loader.dataset),\n",
    "                        100.0 * batch_idx / len(self.train_loader),\n",
    "                        loss.item(),\n",
    "                    )\n",
    "                )\n",
    "                self.loss = loss.item()\n",
    "                torch.save(self.model.state_dict(), \"model.pth\")\n",
    "                torch.save(self.optimizer.state_dict(), \"optimizer.pth\")\n",
    "        self.training_completed = True\n",
    "        self.next(self.local_model_validation)\n",
    "\n",
    "    @collaborator\n",
    "    def local_model_validation(self):\n",
    "        self.local_validation_score = inference(self.model, self.test_loader)\n",
    "        print(\n",
    "            f\"Doing local model validation for collaborator {self.input}: {self.local_validation_score}\"\n",
    "        )\n",
    "        self.next(self.join, exclude=[\"training_completed\"])\n",
    "\n",
    "    @aggregator\n",
    "    def join(self, inputs):\n",
    "        self.average_loss = sum(input.loss for input in inputs) / len(inputs)\n",
    "        self.aggregated_model_accuracy = sum(\n",
    "            input.agg_validation_score for input in inputs\n",
    "        ) / len(inputs)\n",
    "        self.local_model_accuracy = sum(\n",
    "            input.local_validation_score for input in inputs\n",
    "        ) / len(inputs)\n",
    "        print(\n",
    "            f\"Average aggregated model validation values = {self.aggregated_model_accuracy}\"\n",
    "        )\n",
    "        print(f\"Average training loss = {self.average_loss}\")\n",
    "        print(f\"Average local model validation values = {self.local_model_accuracy}\")\n",
    "        self.model = FedAvg([input.model for input in inputs])\n",
    "        self.optimizer = [input.optimizer for input in inputs][0]\n",
    "        self.current_round += 1\n",
    "        if self.current_round < self.rounds:\n",
    "            self.next(\n",
    "                self.aggregated_model_validation,\n",
    "                foreach=\"collaborators\",\n",
    "                exclude=[\"private\"],\n",
    "            )\n",
    "        else:\n",
    "            self.next(self.end)\n",
    "\n",
    "    @aggregator\n",
    "    def end(self):\n",
    "        print(f\"This is the end of the flow\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b96e3045",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
